{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient 소실, 폭주"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출력층으로 오차 gradient를 전파하면서 진행되고, 알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의 gradient(기울기)를 계산하면 경사 하강법 단계에서 gradient를 사용해 각 파라미터를 수정함.\n",
    "* 그러나, 알고리즘이 하위 층으로 진행될수록 gradient가 점점 작아지게 되고, 이러한 가중치를 변경되지 않은 채로 둔다면 좋은 솔루션으로 수렴되지 않게 됨. 이를 그레이디언트 소실(vanishing gradient)라고 함.\n",
    "* 또한, 그레이디언트가 점점 커져 여러 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘이 발산(diverse)하는데 이를 그레이디언트 폭주(exploding gradient)라고 함.\n",
    "  * 불안정한 gradient는 층마다 학습 속도를 달라지게 만들기 때문에 심층 신경망 훈련을 어렵게 함.\n",
    "  * sigmoid 활성화 함수와, 정규분포를 기반으로 한 가중치 초기화 방법으로 인해 각 층에서 출력의 분산이 입력의 분산보다 크다는 게 밝혀져, 신경망의 위쪽으로 갈수록 분산이 계속 커져 결국에는 가장 높은 층에서 활성화 함수가 0이나 1로 수렴해버림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gradient 문제를 해결하기 위해서는 각 층의 출력에 대한 분산이 입력에 대한 분산과 같아야 하고, 역방향에서 층을 통과하기 전과 후의 gradient 분산이 동일해야 한다는 주장이 많음.\n",
    "  * 층의 입력과 출력 개수는 각각 층의 fan-in, fan-out이라고 함.\n",
    "* 이를 위해, 각 층의 연결 가중치를 아래의 방식대로 무작위로 초기화하는 데, 이를 Xavier 초기화(또는 Glorot 초기화)라고 함.\n",
    "$$ \\text{평균이 0이고 분산이}\\;\\sigma^2=\\frac{1}{fan_{\\text{avg}}}\\;인\\;정규분포$$\n",
    "$$또는\\;r=\\sqrt{\\frac{3}{fan_{\\text{avg}}}}일\\;때, -r과\\;+r사이의\\;균등분포$$\n",
    "$$(fan_{\\text{avg}}=(\\text{fan-in}+\\text{fan-out})/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * $fan_{\\text{avg}}$를 $fan_{\\text{in}}$으로 바꾸면 LuCun 초기화.\n",
    "* 위의 초기화 방식을 사용하면 훈련 속도가 상당히 높아짐.\n",
    "* 각 활성화 함수에 대한 초기화 전략은 위의 식에서 분산의 스케일링이나 fan_avg, fan_in중 어떤 것을 쓰느냐만 다름. 특별히 ReLU활성화 함수에 대한 초기화 방법은 He 초기화라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|초기화 전략|활성화 함수|$\\sigma^2$(정규분포)|\n",
    "|----------|----------|--------------------|\n",
    "|Glorot    |tanh, softmax, sigmoid, 활성화 함수 없음|1/$fan_{\\text{avg}}$|\n",
    "|He        |ReLU      |2/$fan_{\\text{in}}$|\n",
    "|LuCun     |SELU      |1/$fan_{\\text{in}}$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras에서는 기본적으로 균등분포의 Glorot 초기화를 사용.\n",
    "* 다른 초기화를 사용하려면 층을 만들 때 <code>kernel_initalizer</code> 하이퍼파라미터를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1c924cc04f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# He 초기화 사용\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "\n",
    "# fan_in 대신 fan_avg기반의 균등분포 He 초기화(VarianceScaling 사용)\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>VarianceScaling</code>의 매개변수 기본값 : scale=1.0, mode='fan_in', distribution='truncated_normal'.\n",
    ">   * distribution='truncated_normal' : 절단 정규분포. $\\sigma^2=1.3*scale/mode$ \n",
    ">   * distribution='untruncated_normal' : 정규분포. $\\sigma^2=scale/mode$\n",
    "> * kernel_initializer의 기본값은 'glorot_uniform'으로, <code>VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')</code>과 동일.\n",
    ">   * 'he_normal'은 <code>VarianceScaling(scale=2., mode='fan_in', distribution='truncated_normal')</code>\n",
    ">   * 'lecun_normal'은 VarianceScaling의 기본값 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 활성화 함수를 잘못 선택하면 gradient 소실이나 폭주로 이어질 수 있음.\n",
    "* ReLU는 특정 양수값에 수렴하지 않는다는 장점이 있지만 ReLU는 완벽하지 않음.\n",
    "  * 훈련하는 동안 일부 뉴런이 0이외의 값을 출력하지 않게 되는 죽은 ReLU(dying ReLU)문제가 있음.\n",
    "  * 학습률이 크면 뉴런 절반이 죽어 있기도 함.\n",
    "  * 뉴런의 가중치가 바뀌어 훈련 세트의 모든 샘플에 대한 입력 가중치 합이 음수가 되면 뉴런이 죽게 됨. 가중치 합이 음수이면 ReLU의 gradient가 0이 되어 경사 하강법이 작동하지 않게 됨.\n",
    "* 이를 해결하기 위해 ReakyReLU라는 ReLU함수의 변종을 사용\n",
    "  * $\\text{LeakyReLU}_{\\alpha}(z)=\\text{max}(\\alpha z, z)$\n",
    "  * $\\alpha$ : 함수가 새는(leaku) 정도. 즉, z<0일때의 함수의 기울기. 일반적으로 0.01로 설정함.\n",
    "  * $\\alpha$ 덕분에 뉴런이 죽지 않게 만들어줌.\n",
    "  * ReLU보다 LeakyReLU가 항상 성능이 높음.\n",
    "  * RReLU(randomized leaky ReLU) : 훈련 시에는 $\\alpha$를 주어진 범위에서 무작위로 선택하고, 테스트 시에는 평균을 사용. 훈련 세트의 과대적합을 줄이는 규제의 역할을 하는 것 처럼 작동.\n",
    "  * PReLU(parametric leaky ReLU) : $\\alpha$가 훈련하는 동안에 학습됨. 대규모 이미지 데이터셋에서는 성능이 좋지만, 소규모 데이터셋에서는 과대적합될 가능성이 높음.\n",
    "* ELU(exponential linear unit) : 훈련 시간이 줄고 신경망의 테스트 세트 성능도 더 높은 함수.\n",
    "$ \\text{ELU}_{\\alpha}(z)=\\begin{cases}\n",
    "\\alpha(\\exp(z)-1), & if\\;z<0 \\\\\n",
    "z, & if\\;z\\geq0\n",
    "\\end{cases}$\n",
    "  * ReLU와 거의 비슷\n",
    "  * z<0때 음수값이 들어오므로 활성화 함수의 평균 출력이 0에 가까워짐. 그레이디언트 소실 문제를 완화해줌.\n",
    "  * $\\alpha$는 z가 큰 음수값일 떄 ELU가 수렴할 값을 정의. 주로 1로 설정하지만 변경 가능.\n",
    "  * z<0 일 때도 gradient가 0이 아니므로 죽은 뉴런을 만들지 않음.\n",
    "  * $\\alpha$=1이면 z=0에서 급격히 변동하지 않으므로 모든 구간에서 매끄럽기 때문에 경사 하강법의 속도를 높여줌.\n",
    "  * 단, ReLU와 다른 변종들보다 계산 속도가 느림.\n",
    "* SELU(Scaled ELU) : 스케일이 조정된 ELU함수의 변종.\n",
    "$ \\text{selu}(z)=\\lambda\\cdot\\text{ELU}_{\\alpha}(z)$\n",
    "  * 완전 연결 층만 쌓고, 모든 은닉층을 SELU활성화 함수를 사용하도록 하면 네트워크가 자기 정규화됨. (훈련하는 동안 각 층의 출력이 평균은 0, 표준편차는 1을 유지하는 경향이 있음). 이는 gradient 소실과 폭주 문제를 막아줌.\n",
    "  * 단, 입력 특성이 반드시 표준화되어야 하고(평균은 0, 표준편차는 1) 모든 은닉층의 가중치는 Lecun 정규분포 초기화로 초기화되어야 함(<code>kernel_initializer=\"lecun_normal\"</code>). 또한, 네트워크는 일렬로 쌓은 층으로 구성되어야 하며 순환 신경망이나 스킵 연결(건너뛰어 연결된 층)과 같은 비 순차적 구조에는 사용하는 것이 힘듦.\n",
    "> 단, 일부의 경우 CNN에서도 SELU 사용 시 성능을 향상시킬 수도 있음.\n",
    "* 심층 신경망 은닉층에 사용할 활성화함수는 일반적으로 SELU > ELU > LeakyReLU(와 그 변종들) > ReLU > tanh > sigmoid 순으로 사용. 만약, 네트워크가 자기 정규화되지 못하면 ELU가 더 나을 수도 있고, 실행 속도가 중요하면 LeakyReLU가 더 좋을 수도 있음. 신경망이 과대적합되었다면 RReLU, 훈련세트가 아주 크다면 PReLU를 사용할 수 있음. 일반적으로는 ReLU가 가장 널리 사용되어왔으므로 대부분의 라이브러리와 하드웨어는 ReLU에 특화되어 있음. 즉, 속도가 중요하면 ReLU가 가장 좋은 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "# LeakyReLU\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ELU\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeakyReLU를 사용하려면 LeakyReLU층을 만들고, 모델에서 적용하려는 층 뒤에 추가함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    # ...\n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "    # ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PReLU를 사용하려면 <code>LeakyReLU</code> 부분을 PReLU로 바꿈. Keras에서는 공식적인 구현은 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SELU를 사용하려면 층을 만들 때 <code>activation=\"selu\", kernel_initalizer=\"lecun_normal\"</code> 지정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ELU나 다른 ReLU 변종과 함께 He 초기화를 사용하면 훈련 초기화 단계에서는 gradient문제를 감소시킬 수 있음.\n",
    "* 하지만, 훈련하는 동안 다시 발생하지 않으리라는 보장은 없음.\n",
    "* 이를 위해 배치 정규화(Batch normalization) 방법 사용.\n",
    "  * 각 층에서 활성화 함수를 통과하기 전이나 후에 연산을 하나 추가함.\n",
    "  * 입력을 원점에 맞추고 정규화한 뒤, 각 층에서 두 개의 새로운 파라미터로 결과값의 스케일을 조절하고 이동시킴. 한 파라미터는 스케일 조정에, 나머지 하나는 이동에 사용함.\n",
    "* 신경망의 첫 번째 층으로 배치 정규화를 추가하면 훈련 세트를 StandardScaler 등을 사용해 표준화할 필요가 없음.\n",
    "* 입력 데이터를 원점에 맞추고 정규화하기 위해 평균과 표준편차를 추정해야 함. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가함.\n",
    "$$ \\boldsymbol{\\mu}_B=\\frac{1}{m_B}\\sum_{i=1}^{m_B}\\mathbf{X}^{(i)} $$\n",
    "$$ \\boldsymbol{\\sigma}_B^2=\\frac{1}{m_B}\\sum_{i=1}^{m_B}\\left(\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_B\\right)^2 $$\n",
    "$$ \\hat{\\mathbf{x}}^{(i)}=\\frac{\\mathbf{x}^{(i)}-\\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}} $$\n",
    "$$ \\mathbf{z}^{(i)}=\\gamma\\otimes\\hat{\\mathbf{x}}^{(i)}+\\boldsymbol{\\beta} $$\n",
    "  * $\\boldsymbol{\\mu}_B$ : 미니배치 B에 대한 입력의 평균 벡터\n",
    "  * $\\boldsymbol{\\sigma}_B$ : 미니배치에 대한 입력의 표준편차 벡터\n",
    "  * $ m_B$ : 미니배치 내의 샘플 수\n",
    "  * $\\hat{\\mathbf{x}}^{(i)}$ : 평균이 0이고 정규화된 샘플 i의 입력\n",
    "  * $\\gamma$ : 층의 출력 스케일 파라미터 벡터. $\\gamma$와 $\\boldsymbol{\\beta}$의 차원은 모두 $\\mathbf{z}^{(i)}$와 동일. (각 층의 뉴런마다 $\\gamma$와 $\\boldsymbol{\\beta}$를 가진다는 의미.)\n",
    "  * $\\otimes$ : 원소별 곱셈\n",
    "  * $\\boldsymbol{\\beta}$ : 층의 출력 이동 파라미터 벡터. 각 입력은 해당 파라미터만큼 이동\n",
    "  * $\\epsilon$ : 분모가 0이 되는 것을 막기 위한 작은 숫자. 안전을 위한 항(smoothing term)이라고 함. 주로 10^-5.\n",
    "  * $\\mathbf{z}^{(i)}$ : 배치 정규화 연산의 출력. 즉, 입력의 스케일을 조정하고 이동시킨 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련하는 동안 입력을 정규화한 다음 스케일을 조정하고 이동시킴.\n",
    "* 단, 테스트 시에는 여러 샘플로 이루어진 배치가 아닌 하나의 샘플에 대한 예측을 만들어야 하므로 테스트 시의 입력의 평균과 표준편차를 사용할 수 없음.\n",
    "  * 배치를 사용하더라도 그 크기가 매우 작거나 독립 동일 분포 조건을 만족하지 못할 수 있음.\n",
    "* 하나의 방법은 훈련이 끝난 후 전체 훈련 세트를 신경망에 통과시켜 배치 정규화 층의 각 입력에 대한 평균과 표준편차를 계산하는 것이 있음. 예측 시 사용한 평균과 표준편차를  최종 입력 평균과 표준편차를 대신 사용할 수 있음.\n",
    "* <code>BatchNormalization</code>층은 층의 입력 평균과 표준편차의 이동 평균을 사용해 훈련하는 동안 최종 통계를 추정.\n",
    "  * $\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}$는 일반적인 역전파를 통해 학습되고, $\\boldsymbol{\\mu}(최종\\;입력\\;평균\\;벡터), \\boldsymbol{\\sigma}(최종\\;입력\\;표준편차\\;벡터$는 지수 평균 이동을 사용해 추정됨. 또한, 훈련하는 동안에 추정되지만 훈련이 끝난 후에 위의 식에서 평균과 표준편차를 대체하기 위해 사용됨.\n",
    "* 배치 정규화는 모델의 성능을 크게 향상시킴.\n",
    "  * 그레이디언트 소실 문제가 크게 감소해 tanh나 sigmoid도 사용 가능.\n",
    "  * 가중치 초기화에 네트워크가 훨씬 덜 민감해짐.\n",
    "  * 규제와 같은 역할을 하여 다른 규제의 필요성을 줄여줌.\n",
    "* 단, 배치 정규화는 모델의 복잡도를 키우고, 실행 시간이 느려짐.\n",
    "  * 훈련이 끝난 후 이전 층과 배치 정규화 층을 합쳐 실행 속도 저하를 피할 수 있음. 이전 층의 가중치를 바꿔 스케일이 조정되고 이동된 출력을 만듦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras에서의 사용은 은닉층의 활성화 함수 전이나 후에 BatchNormalization층을 추가하면 됨.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 배치 정규화 층은 입력마다 4개의 파라미터 $\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma}$를 추가함(ex. 첫 번째 배치 정규화 층에서는 4*784=3136개의 파라미터 존재)\n",
    "> * $\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}$는 역전파로 학습되지 않기 때문에 Non-trainable params로 분류."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번재 배치 정규화 층.\n",
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치 정규화 층은 활성화 함수 이후보다 활성화 함수 이전에 추가하는 것이 좋다고 알려짐. 활성화 함수 이전에 배치 정규화 층을 추가하려면, 은닉층(Dense)에서 활서오하 함수를 지정하지 않고, 배치 정규화 층 뒤에 별도의 층으로 활성화 함수를 추가해야 함.\n",
    "* 배치 정규화 층은 입력마다 이동 파라미터를 포함하기 때문에 이전 층에서 편향을 뺄 수 있음.(<code>use_bias=False</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>BatchNormalization</code>에 조정할 하이퍼파라미터는 기본값으로도 보통 잘 동작하지만, <code>momentum</code> 매개변수(지수 이동 편균을 업데이트할 때 사용하는 값. $\\hat{\\mathbf{v}}\\leftarrow \\hat{\\mathbf{v}}\\times\\text{momentum}+\\mathbf{v}\\times(1-\\text{momentum})$ )는 조정하기도 함. 일반적으로는 1에 가까운 값.\n",
    "  * 데이터셋이 크고 미니배치가 크면 모멘텀 값을 0.99...9와 같이 9를 더 넣어 1에 더 가깝게 만듦.\n",
    "* <code>axis</code>매개변수는 정규화할 축을 결정. 기본값은 -1. (다른 축을 따라 계산한 평균과 표준편차를 이용해 마지막 축을 정규화함.)\n",
    "  * 입력 배치가 2차원(샘플 개수*특성 개수)이라면 각 입력 특성이 배치 내의 모든 샘플에 대해 계산한 평균과 표준편차를 기준으로 정규화.\n",
    "  * 위의 예시에서 첫 번쨰 배치 정규화 층은 784개의 입력 특성마다 독립정으로 정규화됨. 배치 정규화 층을 Flatten층 이전으로 옮기면 입력 배치는 샘플 개수\\*높이\\*너비 크기의 3차원이 되므로 배치 정규화 층은 28개의 평균과 28개의 표준편차 계산하고 동일한 평균과 표준편차를 이용해 모든 픽셀을 정규화함.\n",
    "* 배치 정규화는 훈련하는 동안에는 배치 통계를 사용하고, 훈련이 끝난 뒤에는 최종 통계를 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내부 코드\n",
    "\n",
    "class BatchNormalization(keras.layers.layer):\n",
    "    # ...\n",
    "    def call(self, inputs, training=None):\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * call 내에서 실제 계산 수행. fit()은 훈련하는 동안 call의 training 매개변수를 1로 설정. 훈련과 테스트에 대해 다르게 동작하는 층을 만들려면 call()에 training 매개변수를 추가하여 어떤 것을 계산할 지 결정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치 정규화가 널리 사용되면서 거의 모든 층 뒤에 배치 정규화를 사용하고, 신경망 그림에는 이를 제외해서 그리는 경우도 많음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그레이디언트 폭주 문제를 완화하는 또 다른 방법은 역전파될 때 일정 임곗값을 넘지 못하게 gradient를 잘라내는 방법이 있고, 이를 그레이디언트 클리핑(gradient clipping)이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * gradient 벡터의 모든 원소를 -1.0과 1.0 사이로 클리핑. 즉, 훈련되는 각 파라미터에 대한 손실의 모든 편미분 값을 -1.0에서 1.0으로 잘라냄.\n",
    "* 임곗값은 하이퍼파라미터로 튜닝 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사전 훈련된 층 재사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 큰 규모의 신경망을 새로 훈련하는 것은 힘드므로, 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있다면 해당 신경망의 하위층을 재사용하는 것이 좋고, 이를 전이 학습(transfer learning)이라고 함.\n",
    "  * 훈련 속도를 높여주고 필요한 훈련 데이터도 줄여줌.\n",
    "  * ex) 카테고리 100개로 구분된 이미지를 분류하도록 훈련된 DNN이 있고, 자동차 종류를 구체적으로 분류하는 DNN을 만들려고 할 때, 카테고리 분류 DNN의 하위 층 일부를 재사용하는 것이 좋을 수 있음.\n",
    "  * 새로운 작업에 유용한 고수준 특성은 원본 작업의 특성과는 다르므로 상위 은닉층은 하위 은닉층보다는 덜 유용함.\n",
    "  * 작업이 비슷할 수록 가져올 하위 층의 개수를 늘리는 것이 좋음.\n",
    "* 전이 학습을 사용하려면 재사용하려는 층을 모두 동결(경사 하강법으로 가중치가 바뀌지 않도록 훈련되지 않는 가중치로 만듦)하고 모델을 훈련한 뒤 성능을 평가함. 그 뒤, 맨 위에 있는 한 두개의 은닉층의 동결을 해제하고 역전파를 통해 가중치를 조절하여 성능이 향상되는지 확인.\n",
    "  * 훈련 데이터가 많다면 동결 해제할 층을 늘릴 수 있음.\n",
    "  * 재사용하는 층의 동결을 해제할 때는 학습률을 줄여 가중치를 세밀하게 튜닝하는 게 좋음.\n",
    "  * 여전히 성능이 좋지 않거나 훈련 데이터가 적다면 상위 은닉층을 제거하고 남은 은닉층을 다시 동결하는 식으로 재사용할 은닉층의 적절한 개수를 찾을 때 까지 반복할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ex) fashion_mnist 데이터셋 훈련\n",
    "  * model A(model_fashion)은 \"Sandal\", \"Shirt\" 레이블이 붙은 데이터를 제외하고 훈련시킨 모델. model B는 \"Sandal\", \"Shirt\"를 구분하느 모델(shirt면 양성, sandal이면 음성)\n",
    "  * model A의 은닉층을 재사용하여 model B에 사용\n",
    "  * model B의 훈련 데이터셋은 200개만 존재.\n",
    "* **참고만 할 것**. 이유 후술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashoin_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashoin_mnist.load_data()\n",
    "\n",
    "# model A에 쓸 데이터셋 준비\n",
    "X_train_removed = X_train_full[np.logical_and(y_train_full!=5, y_train_full!=6)] / 255.0\n",
    "y_train_removed = y_train_full[np.logical_and(y_train_full!=5, y_train_full!=6)]\n",
    "X_test_removed = X_test[np.logical_and(y_test!=5, y_test!=6)] / 255.0\n",
    "y_test_removed = y_test[np.logical_and(y_test!=5, y_test!=6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model B에 쓸 데이터셋 준비\n",
    "X_train_forB = X_train_full[np.logical_or(y_train_full==5, y_train_full==6)] / 255.0\n",
    "y_train_forB = y_train_full[np.logical_or(y_train_full==5, y_train_full==6)]\n",
    "X_test_forB = X_test[np.logical_or(y_test==5, y_test==6)] / 255.0\n",
    "y_test_forB = y_test[np.logical_or(y_test==5, y_test==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model B에 쓸 데이터셋 중, 200개만 사용.\n",
    "X_train_forB = X_train_forB[:200]\n",
    "y_train_forB = y_train_forB[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_forB)):\n",
    "    if y_train_forB[i] == 5:\n",
    "        y_train_forB[i]=0\n",
    "    else:\n",
    "        y_train_forB[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test_forB)):\n",
    "    if y_test_forB[i] == 5:\n",
    "        y_test_forB[i]=0\n",
    "    else:\n",
    "        y_test_forB[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train_removed)):\n",
    "    if y_train_removed[i]>=7:\n",
    "        y_train_removed[i]-=2\n",
    "        \n",
    "for i in range(len(y_test_removed)):\n",
    "    if y_test_removed[i]>=7:\n",
    "        y_test_removed[i]-=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_removed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model A\n",
    "model_fashion = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(8, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 266,408\n",
      "Trainable params: 266,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fashion.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fashion.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4984 - accuracy: 0.8432\n",
      "Epoch 2/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3178 - accuracy: 0.8927\n",
      "Epoch 3/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2901 - accuracy: 0.9030\n",
      "Epoch 4/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2725 - accuracy: 0.9092\n",
      "Epoch 5/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2606 - accuracy: 0.9136\n",
      "Epoch 6/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2509 - accuracy: 0.9170\n",
      "Epoch 7/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2414 - accuracy: 0.9190\n",
      "Epoch 8/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2342 - accuracy: 0.9213\n",
      "Epoch 9/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2275 - accuracy: 0.9229\n",
      "Epoch 10/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2220 - accuracy: 0.9246\n",
      "Epoch 11/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2166 - accuracy: 0.9258\n",
      "Epoch 12/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2106 - accuracy: 0.9287\n",
      "Epoch 13/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2051 - accuracy: 0.9305\n",
      "Epoch 14/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2005 - accuracy: 0.9320\n",
      "Epoch 15/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1968 - accuracy: 0.9326\n",
      "Epoch 16/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1928 - accuracy: 0.9344\n",
      "Epoch 17/30\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1875 - accuracy: 0.9348\n",
      "Epoch 18/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1840 - accuracy: 0.9373\n",
      "Epoch 19/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1795 - accuracy: 0.9381\n",
      "Epoch 20/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1765 - accuracy: 0.9394\n",
      "Epoch 21/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1729 - accuracy: 0.9410\n",
      "Epoch 22/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1688 - accuracy: 0.9416\n",
      "Epoch 23/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1659 - accuracy: 0.9424\n",
      "Epoch 24/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1617 - accuracy: 0.9441\n",
      "Epoch 25/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1593 - accuracy: 0.9449\n",
      "Epoch 26/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1560 - accuracy: 0.9463\n",
      "Epoch 27/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1537 - accuracy: 0.9466\n",
      "Epoch 28/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1517 - accuracy: 0.9474\n",
      "Epoch 29/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1476 - accuracy: 0.9486\n",
      "Epoch 30/30\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1452 - accuracy: 0.9484\n"
     ]
    }
   ],
   "source": [
    "history = model_fashion.fit(X_train_removed, y_train_removed, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 2ms/step - loss: 0.2067 - accuracy: 0.9270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20669692754745483, 0.9269999861717224]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model A의 정확도는 약 93%\n",
    "model_fashion.evaluate(X_test_removed, y_test_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 모델 복제 및 가중치 복사\n",
    "\n",
    "model_A = keras.models.clone_model(model_fashion)\n",
    "model_A.set_weights(model_fashion.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 층을 제외한 모든 층 재사용 및 새로운 출력층 추가.\n",
    "\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작업 B(셔츠, 샌들 분류)를 위해 훈련. 단, 새로운 출력층이 랜덤하게 초기화되어있으므로, 큰 오차 그레이디언트가 재사용된 가중치를 망칠 수 있음. 따라서 처음 몇 번의 epoch동안에는 재사용된 층을 동결하고 새로운 층에게 적절한 가중치를 학습할 시간을 줌.\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 층을 동결하거나 동결을 해제했다면 그 다음에는 반드시 모델을 컴파일해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9950\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9950\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.9950\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9950\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0875 - accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0862 - accuracy: 0.9950\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9950\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.9950\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9950\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9950\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9950\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9950\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9950\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9950\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9950\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9950\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0847 - accuracy: 0.9950\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0846 - accuracy: 0.9950\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0846 - accuracy: 0.9950\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9950\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9950\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.9950\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0844 - accuracy: 0.9950\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9950\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9950\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "# 몇 에포크 동안 모델을 훈련한 뒤, 재사용된 층의 동결을 해제하고 훈련을 계속함.\n",
    "# 동결 해제 이후에는 학습률을 낮춰 재사용된 가중치가 망가지는 것을 막아주는 게 좋음.\n",
    "\n",
    "history = model_B_on_A.fit(X_train_forB, y_train_forB, epochs=10)\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)   #기본값은 1e-2\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_forB, y_train_forB, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06825259327888489, 0.9959999918937683]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 99.6%\n",
    "model_B_on_A.evaluate(X_test_forB, y_test_forB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 단, 위의 예시는 높은 성능을 가진 모델을 찾기 까지 여러 설정을 시도해 본 것이므로, 타깃 클래스나 랜덤 초깃값을 바꾸면 성능이 떨어지게 됨.(즉, 위처럼 해야만 성능이 높게 나오는 것이고, 위의 예시는 참고용으로만 제작.)\n",
    "> * 논문들에서 신경망을 다룰 때 결과가 너무 긍정적인 이유는 주로 가장 좋은 결과만을 표시하기 때문이므로 주의해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전이 학습은 작은 완전 연결 네트워크에는 잘 동작하지는 않음.\n",
    "* 주로 일반적인 특성을 감지하는 경향이 있는 심층 합성곱 신경망에서 잘 동작함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 레이블된 훈련 데이터가 많지 않은 경우에, 비슷한 작업에 대한 훈련된 모델도 없는 경우, 비지도 사전훈련(unsupervised pretraining)을 사용할 수 있음.\n",
    "  * 레이블이 없는 데이터를 모으는 것은 쉽지만, 이에 레이블을 부여하는 작업은 많은 비용이 듦.\n",
    "  * 레이블되지 않은 훈련 데이터를 많이 모을 수 있다면 이를 이용해 autoencoder, 생성적 적대 신경망과 같은 비지도 학습 모델을 훈련할 수 있음. 이후 해당 모델의 하위층을 재사용하고, 그 위에 새 작업에 맞는 출력층을 추가해 지도 학습으로 최종 네트워크를 세밀하게 튜닝하는 방법을 사용할 수 있음.\n",
    "* 딥러닝 초기에는 탐욕적 층 단위 사전훈련(greedy layer-wise pretraining) 사용. 하나의 층을 가진 비지도 학습 모델을 훈련한 뒤, 이 층을 동결하고 새 층을 추가한 뒤 다시 훈련. 이후 층을 동결하고 새 층을 추가하여 훈련하는 작업을 반복한 뒤 최종적으로 지도 학습을 진행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 레이블된 훈련 데이터가 많지 않은 경우 또 다른 방법은 레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 훈련한 뒤, 해당 신경망의 하위 층을 재사용하는 방법이 있음.\n",
    "  * ex) 얼굴 인식 시스템을 만들려고 하는데 개인별 이미지가 얼마 없다면, 인터넷에서 무작위로 많은 인물의 이미지를 수집해 두 개의 다른 이미지가 같은 사람의 것인지 감지하는 첫 번째 신경망을 훈련할 수 있고, 해당 신경망은 얼굴의 특성을 잘 감지할 것이므로 하위 신경망을 재사용해 적은 훈련 데이터에서도 얼굴을 잘 구분하는 분류기를 훈련할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 고속 optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 심층 신경망의 훈련 속도는 매우 느릴 수 있으므로, 표준적인 경사 하강법 optimizer대신 더 빠른 optimizer를 사용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 완만한 경사를 굴러가는 공이 있다면, 처음에는 느리게 출발하지만, 종단속도에 도달할 때 까지는 빠르게 가속될 것임.\n",
    "* 이러한 원리를 적용한 것이 모멘텀(Momentum).\n",
    "  * 일반적인 경사 하강법은 경사면을 따라 일정한 크기의 step으로 조금씩 내려가므로 맨 아래에 도착하는 데 더 오랜 시간이 걸림.\n",
    "  * 일반적인 경사 하강법은 가중치에 대한 비용 함수 $J(\\boldsymbol{\\theta})$의 gradient $\\nabla_{\\theta}J(\\boldsymbol{\\theta})$에 대해 학습률 $\\eta$를 곱한 것을 빼 가중치 $\\boldsymbol{\\theta}$를 갱신함($\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$). 즉, gradient가 매우 작으면 속도도 매우 느려짐.\n",
    "* 모멘텀 최적화는 이전의 gradient가 얼마였는지를 고려하여, 매 반복에서 현재 gradient에 학습룰 $\\eta$를 곱한 후, 모멘텀 벡터(momentum vector) $\\mathbf{m}$에 더하고, 이 값을 빼는 방식으로 가중치를 계산.\n",
    "  * 일반적인 경사 하강법은 gradient를 속도로 사용하고, 모멘텀은 가속도로 사용함.\n",
    "* 일종의 마찰저항을 표현하고, 모멘텀이 너무 커지는 것을 막기 위해 모멘텀(momentum)이라는 하이퍼파라미터 $\\beta$를 사용.\n",
    "  * 보통 0(높은 마찰저항)~1(마찰저항 없음)사이로 설정되고, 일반적으로는 0.9 사용.\n",
    "$$1.\\;\\mathbf{m}\\leftarrow\\beta\\mathbf{m}-\\eta\\nabla_{\\theta}\\cdot J(\\boldsymbol{\\theta})$$\n",
    "$$2.\\;\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\mathbf{m}$$\n",
    "* gradient가 일정하면 종단속도(가중치를 갱신하는 최대 크기)는 $\\eta$를 곱한 gradient에 $\\frac{1}{1-\\beta}$를 곱한 것과 같음.\n",
    "  * ex) $\\beta=0.9$라면 종단속도는 gradient\\*학습률\\*10이 되므로, 일반적인 경사 하강법보다 10배 빠르다는 의미.\n",
    "* 더 빠르게 평탄한 지역을 탈출하도록 도움.\n",
    "* 배치 정규화를 사용하지 않는 DNN에서 상위층은 종종 스케일이 매우 다른 입력을 받게 되는데, 모멘텀 최적화를 사용하면 이러한 경우에 큰 도움이 됨.\n",
    "* 지역 최적점을 건너뛰는 데 도움이 됨.\n",
    "* 모멘텀 때문에 optimizer가 최적값에 도달하기 전까지 진동하듯이 움직이는데, 이 때문에 마찰저항이 조금 있는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# keras에서의 모멘텀 사용. 단순히 SGD optimizer에 momentum 매개변수 지정.\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 튜닝할 하이퍼파라미터가 하나 더 늘어나지만, 웬만한 경우 <code>moemntum=0.9</code>에서 잘 작동하며 경사 하강법보다 거의 항상 빠름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네스테로프 가속 경사(Nesterov accelerated gradient, NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모멘텀 최적화의 한 변종.\n",
    "* 현재 위치가 $\\boldsymbol{\\theta}$가 아닌, 모멘텀의 방향으로 조금 앞선 $\\boldsymbol{\\theta}+\\beta\\mathbf{m}$에서 비용 함수의 gradient 계산.\n",
    "$$1.\\;\\mathbf{m}\\leftarrow\\beta\\mathbf{m}-\\eta\\nabla_{\\theta}\\cdot J(\\boldsymbol{\\theta+\\beta\\mathbf{m}})$$\n",
    "$$2.\\;\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\mathbf{m}$$\n",
    "* 일반 모멘텀 방식보다 네스테로프 방식이 최적값에 조금 더 가까워지고, 모멘텀보다 더 빨리 최적점에 도달할 수 있음.\n",
    "  * 진동을 감소시키고 수렴을 빠르게 만들어 줌\n",
    "* SGD optimizer를 만들 떄 <code>use_nesterov=True</code>로 설정하여 사용 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한쪽이 길쭉한 그릇 형태에서, 경사 하강법은 전역 최적점 방향으로 바로 향하지 않고, 가장 가파른 경사를 따라 내려가기 시작해 골짜기 아래로 느리게 이동함.\n",
    "* 이를 일찍 감지하여 전역 최적점 쪽으로 좀 더 정확한 뱡향을 잡게 하는 알고리즘.\n",
    "  * 가장 가파른 차원을 따라 gradient 벡터의 스케일을 감소시킴.\n",
    "$$1.\\;\\mathbf{s}\\leftarrow\\mathbf{s}+\\nabla_{\\theta}J(\\boldsymbol{\\theta})\\otimes\\nabla_{\\theta}j(\\boldsymbol{\\theta})$$\n",
    "$$2.\\;\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}$$\n",
    "  * $\\oplus$ : 원소 별 곱셈\n",
    "  * $\\oslash$ : 원소 별 나눗셈\n",
    "  * 1 : gradient의 제곱을 벡터 $\\mathbf{s}$에 누적함. 즉, $\\mathbf{s}$의 각 원소 $s_i$마다 $s_i\\leftarrow s_i+(\\partial J(\\boldsymbol{\\theta})/\\partial\\theta_i)^2$을 계산하는 것과 동일($\\theta_i$에 대한 비용 함수의 편미분을 제곱하여 누적함). 비용 함수가 i번째 차원을 따라 가파르다면 $s_i$는 반복이 진행되면서 점점 커짐.\n",
    "  * 2 : 경사 하강법과 비슷하지만, gradient 벡터를 $\\sqrt{\\mathbf{s}+\\epsilon}$으로 나눠 스케일을 조정. $\\epsilon$은 0으로 나누는 것을 막기 위한 아주 작은 값.\n",
    "* 학습률을 감소시키지만, 경사가 완만한 차원보다 가파른 차원에 더 빠르게 감소하고, 이를 적응적 학습률(adaptive learning rate)이라고 함.\n",
    "  * 차원별로 학습률은 다르게 감소.\n",
    "  * 전역 최적점으로 더 곧장 가도록 갱신되는 데 도움이 되고, 학습률 파라미터 $\\eta$를 덜 튜닝해도 된다는 장점이 있음.\n",
    "* 간단한 이차방정식 문제에 잘 작동하나, 신경망 훈련 시 학습률이 너무 감소되어 전역 최적점에 도달하기 전에 일찍 멈출 수 있음.\n",
    "  * keras에서 <code>Adagrad</code>를 사용할 수 있지만, 심층 신경망에서는 사용해선 안됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 너무 빨리 느려저 전역 최적점에 도달하지 못하는 AdaGrad의 단점을 개선한 것.\n",
    "* 훈련 시작부터 모든 gradient가 아닌 가장 최근 반복에서 비롯된 gradient만 누적.\n",
    "  * 알고리즘의 첫 번째 단계에서 지수 감소를 사용함.\n",
    "$$1.\\;\\mathbf{s}\\leftarrow\\beta\\mathbf{s}+(1-\\beta)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "$$2.\\;\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}$$\n",
    "* 감쇄율 $\\beta$는 주로 0.9로 설정.\n",
    "  * 이도 조절해야할 하이퍼파라미터이지만, 기본값 0.9일때 대부분의 경우 잘 작동함.\n",
    "* <code>keras.optimizers.RMSprop(lr=0.001, rho=0.9)</code>와 같이 사용. rho값이 $\\beta$값\n",
    "* 간단한 문제를 제외하고는 항상 AdaGrad보다 성능이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam(adaptive moment estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모멘텀 최적화와 RMSProp의 아이디어를 합친 것.\n",
    "* 모멘텀 최적화처럼 지난 gradient의 지수 감소 평균을 따르고, RMSProp처럼 지난 gradient의 제곱의 지수 감소된 평균을 따름.\n",
    "  * gradient의 평균과 분산에 대한 예측을 함. 평균을 첫 번째 모멘트, 분산을 두 번째 모멘트라고 함.\n",
    "$$1.\\;\\mathbf{m}\\leftarrow\\beta_1\\mathbf{m}-(1-\\beta_1)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "$$2.\\;\\mathbf{s}\\leftarrow\\beta_2\\mathbf{s}-(1-\\beta_2)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "$$3.\\;\\hat{\\mathbf{m}}\\leftarrow\\frac{\\mathbf{m}}{1-\\beta_1^t}$$\n",
    "$$4.\\;\\hat{\\mathbf{s}}\\leftarrow\\frac{\\mathbf{s}}{1-\\beta_2^t}$$\n",
    "$$5.\\;\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}+\\eta\\hat{\\mathbf{m}}\\oslash\\sqrt{\\hat{\\mathbf{s}}+\\epsilon}$$\n",
    "  * t는 반복 횟수.\n",
    "  * 1, 2, 5단계가 모멘텀 최적화 및 RMSProp과 비슷. 대신 1에서 지수 감소 합 대신 지수 감소 평균을 계산.\n",
    "  * $\\mathbf{m},\\mathbf{s}$는 처음에 0으로 초기화되기 때문에 훈련 초기에 0쪽으로 치우치게 됨. 3, 4단계가 두 값을 증폭시키는데 도움을 줌. 즉, $\\mathbf{m},\\mathbf{s}$가 0부터 시작하므로 $\\beta_1\\mathbf{m}, \\beta_2\\mathbf{s}$가 반복 초기에 기여를 못하므로, 3,4단계에서 이를 증폭시켜줌. 대신 단계가 많이 진행될수록 3, 4단계의 분모는 1에 가까워져 $\\mathbf{m},\\mathbf{s}$값이 거의 증폭되지 않음.\n",
    "* 모멘텀 감쇠 파라미터 $\\beta_1$은 주로 0.9로 초기화하고 스케일 감쇠 파라미터 $\\beta_2$는 0.999로 주로 초기화함.\n",
    "* $\\epsilon$은 안정적인 계산을 위한 아주 작은 값(10^-7 등.)\n",
    "  * keras에서 <code>epsilon</code>의 기본값은 None으로, 기본값이 10^-7인 <code>keras.backend.epsilon()</code>을 사용.\n",
    "  * <code>keras.backend.set_epsilon()</code>을 사용해 바꿀 수 있음.\n",
    "* 학습률 파라미터 $\\eta$를 튜닝할 필요가 적음. 일반적으로는 기본값 0.001을 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaMax : Adam의 변종. Adam의 2단계에서 $\\mathbf{s}$에 gradient의 제곱을 누적하고(최근 gradient에 더 큰 가중치를 부여하고) 5단계에서 $\\epsilon$과 3,4단계를 무시하면 $\\mathbf{s}$의 제곱근으로 파라미터 업데이트의 스케일을 낮추게 됨. 즉, 시간이 지남에 따라 감쇠된 gradient의 $l_2$ norm(제곱의 합의 제곱근)으로 파라미터 업데이트의 스케일을 낮춤. AdaMax는 $l_2$ norm을 $l_\\infty$로 바꿈. \n",
    "  * Adam에서 2단계를 $\\mathbf{s}\\leftarrow\\text{max}\\left(\\beta_2\\mathbf{s},\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\right)$로 바꾸고, 4단계를 삭제한 것이 AdaMax가 됨.\n",
    "  * 5단계에서 $\\mathbf{s}$에 비례해 gradient 업데이트의 스케일을 낮춤.\n",
    "  * 실전에서 Adam보다 안정적이지만, 데이터셋에 따라 다르고, 일반적으로는 Adam의 성능이 더 좋기 때문에 어떤 작업에서 Adam이 잘 동작하지 않는다면 AdaMax optimizer를 사용할 수 있음.\n",
    "* Nadam : Adam+네스테로프 기법. Adam보다 조금 더 빠르게 수렴하고, 일반적으로 Adam보다 성능이 좋게 나오지만 때로는 RMSProp이 더 나을때도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 모든 최적화 기법들은 1차 편미분(야코비안)에만 의존함.\n",
    "* 2차 편미분(헤시안) 기반 알고리즘들도 존재하지만, 심층 신경망에는 적용하기 어려움.\n",
    "  * 하나의 출력마다 n^2개의 2차 편미분을 계산해야 하기 때문(n은 파라미터 개수)\n",
    "  * 심층 신경망은 수만 개의 파라미터를 가지므로 메모리 용량을 초과할 수 있고, 가능하더라도 시간이 매우 오래 걸림."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 최적화 알고리즘은 대부분의 파라미터가 0이 아닌 밀집(dense) 모델을 만듦. 이 대신에, 매우 빠르게 실행할 모델이 필요하거나 메모리를 적게 차지하는 모델이 필요하면 일부 가중치가 0인 희소(sparse)모델을 만들 수 있음.\n",
    "  * 보통 때처럼 훈련한 뒤, 작은 값의 가중치를 제거(0으로 만듦)하는 방식으로 만들 수 있음.\n",
    "  * 또는, $l_1$규제(이후 설명)를 강하게 적용(optimizer가 가능한 한 많은 가중치를 0으로 만들도록 강제).\n",
    "* 이도 잘 작동하지 않으면, 텐서플로 모델 최적화 툴킷(TF-MOT)을 사용해 훈련하는 동안 반복적으로 연결 가중치를 크기에 맞춰 제거하는 가지치기 API를 제공.\n",
    "  * https://www.tensorflow.org/model_optimization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|클래스|수렴 속도|수렴 품질|\n",
    "|-----|---------|--------|\n",
    "|SGD  |느림     |좋음     |\n",
    "|SGD(momentum=...)|보통|좋음|\n",
    "|SGD(momentum=...,nesterov=True)|보통|좋음|\n",
    "|Adagrad|빠름   |나쁨(너무 빨리 멈춤)|\n",
    "|RMSprop|빠름   |보통 또는 좋음|\n",
    "|Adam|빠름   |보통 또는 좋음|\n",
    "|Nadam|빠름   |보통 또는 좋음|\n",
    "|AdaMax|빠름   |보통 또는 좋음|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 스케줄링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습률을 너무 크게 잡으면 훈련이 발산할 수 있고, 너무 작게 잡으면 최적점에 수렴하는 데 매우 오랜 시간이 걸림.\n",
    "* 컴퓨터 자원이 한정적이라면 차선의 솔루션을 만들기 위해 모델이 완전히 수렴하기 전에 훈련을 멈춰야 함.\n",
    "  * 이렇게 찾은 약간 높은 학습률은 처음에는 매우 빠르게 수렴하는 방향으로 진행하지만, 최적점 근처에서는 요동이 심해져 수렴하지 못하게 됨.\n",
    "* 매우 작은 값에서 시작해 매우 큰값까지 지수적으로 학습률을 중가시키며 모델 훈련을 수백번 반벅하여 좋은 학습률을 찾을 수 있음. 이후 학습 곡선을 보고 다시 상승하는 곡선보다 조금 더 작은 학습률을 선택한 뒤, 모델을 다시 초기화하고 이 학습률로 훈련.\n",
    "* 이 방법 대신, 큰 학습률로 시작해 학습 속도가 느려지면 학습률을 낮추는 방식을 사용할 수 있음. 이런 전략을 학습 스케줄(learning schedule)이라고 함.\n",
    "  * 최적의 고정 학습률보다 좋은 솔루션을 더 빨리 발견할 수 있음.\n",
    "  * 거듭제곱 기반 스케줄링 : 학습률을 반복 횟수 t에 대한 함수 $\\eta(t)=\\eta_0 / (1+t/s)^c$로 설정. $\\eta_0$는 초기 학습률, c는 거듭제곱 수(일반적으로 1), s는 step 횟수로, 이들 모두 하이퍼파라미터. 학습률은 각 step마다 감소되고 s번의 step뒤에 학습률은 $\\eta_0/2$로 줄어듦. s번마다 step을 반복할수록 학습률은 $\\eta_0/3$, $\\eta_0/4$...와 같이 줄어듦. 즉, 처음에는 학습률이 빠르게 감소하다가 나중에는 점점 더 느리게 감소함. $\\eta_0,s$,(아마 c도) 튜닝해야 제대로 작동함.\n",
    "  * 지수 기반 스케줄링 : 학습률을 $\\eta(t)=\\eta_00.1^{t/s}$로 설정. s step마다 10배씩 학습률이 줄어듦. 거듭제곱 기반 스케줄링은 학습률을 갈수록 천천히 감소시키지만, 지수 기반 스케줄링은 매 스텝마다 10배씩 계속 감소함.\n",
    "  * 구간별 고정 스케줄링 : 일정 횟수의 epoch동안 일정한 학습률을 사용하고, 그다음 또 다른 횟수의 epoch동안 작은 학습률을 사용하는 식으로 스케줄링. 잘 동작할 수 있지만, 적절한 학습률과 epoch횟수의 조합을 찾는 것이 힘듦.\n",
    "  * 성능 기반 스케줄링 : 조기 종료처럼 매 N step마다 검증 오차를 측정하고 오차가 줄어들지 않으면 $\\lambda$배만큼 학습률을 감소시킴.\n",
    "  * 1cycle 스케줄링 : 훈련 절반 동안 초기 학습룰 $\\eta_0$을 선형적으로 $\\eta_1$까지 증가시킨 뒤, 나머지 절반 동안 선형적으로 학습률을 다시 $\\eta_0$까지 줄임. 마지막 몇 epoch는 학습률을 소수점 몇 번재 자리까지 선형적으로 줄임. 최대 학습률 $\\eta_1$은 최적의 학습률을 찾을 때와 같은 방식을 이용해 선택하고, 초기 학습률 $\\eta_0$는 이보다 10배정도 낮은 값을 사용함.\n",
    "    * 모멘텀 사용 시 처음에 높은 모멘텀(ex. 0.95)으로 시작해 훈련의 처음 절반 동안 낮은 모멘텀으로 줄어듦(ex. 0.85). 이후 나머지 절반동안 최댓값(ex. 0.95)으로 되돌리고 마지막 몇 epoch는 최댓값을 유지한 채로 진행.\n",
    "    * 훈련 속도를 크게 높여주고 더 높은 성능을 낼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거듭제곱 기반 스케줄링(구현이 가장 쉬움)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>decay</code> : s의 역수. 기본값은 0이고, Adagrad, RMSProp, Adam, Adamax도 동일한 decay 매개변수를 지원함.\n",
    "> * keras에서 c는 1로 가정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지수 기반 스케줄링\n",
    "\n",
    "# 현재 epoch를 받아 학습률을 반환하는 함수 정의\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch/20)\n",
    "\n",
    "# 또는 eta_0와 s를 하드코딩하지 않고 이 변수를 설정한 closure를 반환하는 함수를 만들 수도 있음.\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.01(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# 이후 이 스케줄링 함수를 전달해 LearningRateScheduler 콜백을 만든 뒤, fit()메서드에 전달\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train, y_train, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>LearningRateScheduler</code>는 epoch를 시작할 때 마다 optimizer의 learning_rate 속성을 업데이트함. 더 자주 업데이트 하고싶다면(ex. 매 step마다 업데이트) 사용자 정의 콜백을 만들 수 있음.\n",
    ">   * epoch마다 step이 많다면 스텝마다 학습률을 업데이트하는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케줄 함수는 두 번쨰 매개변수(현재 학습률)도 받을 수 있음.\n",
    "\n",
    "# 이전 학습률에 0.1^(1/20)을 곱해 동일한 지수 감쇠 효과를 내는 스케줄 함수\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr*0.1**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 저장 시 optimizer와 학습률이 같이 저장됨. \n",
    "* 새로운 스케줄 함수를 사용하더라도 훈련된 모델을 불러와 중지된 지점부터 훈련을 계속할 수 있음.\n",
    "  * 단, 스케줄 함수가 epoch매개변수를 사용한다면, 이는 저장되지 않고 fit()을 호출할 때 마다 0으로 초기화됨(중지된 지점에서 모델 훈련을 이어가면 매우 큰 학습률이 만들어질 수 있기 때문).\n",
    "  * 이를 해결하기 위해 epoch에서 시작하도록 fit()의 매개변수 <code>initial_epoch</code>를 수동으로 지정할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구간별 고정 스케줄링\n",
    "# 지수 기반 스케줄링과 동일하게 스케줄 함수로 LearningRateScheduler콜백을 만들어 fit()에 전달하는 방식으로 사용.\n",
    "\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch<5:\n",
    "        return 0.01\n",
    "    elif epoch<15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "    \n",
    "# 또는 좀 더 일반적인 함수를 정의할 수 있음\n",
    "\"\"\"\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 기반 스케줄링\n",
    "# ReduceLROnPlateau콜백을 사용.\n",
    "\n",
    "# 최상의 검증 손실이 5번의 연속적인 epoch동안 향상되지 않을 때 마다 학습률에 0.5를 곱함\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras에서는 <code>keras.optimizers.schedules</code>에 있는 스케줄 중 하나를 사용해 학습률을 정의하고, 이 학습률을 optimizer에 전달할 수 있음. 이렇게 하면 매 epoch가 아닌, 매 스텝마다 학습률을 업데이트함.\n",
    "  * 간결하고 이해하기 편함\n",
    "  * 모델 저장 시 학습률과 현재 상태를 포함한 스케줄도 함께 저장됨\n",
    "  * 단, 표준 keras api는 아니며 tf.keras에서만 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지수 기반 스케줄링\n",
    "s = 20 * len(X_train) // 32 # 20번 epoch에 담긴 전체 step 수. 배치 크기는 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 cycle 방식은 매 반복마다 학습률을 조정하는 사용자 정의 콜백을 만들어 사용 가능.\n",
    "  * <code>self.model.optimizer.lr</code>(또는 <code>self.model.optimizer.learning_rate</code>)을 바꿔 optimizer의 학습률을 업데이트할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지수 기반 스케줄링, 성능 기반 스케줄링, 1cycle 스케줄링이 수렴 속도를 크게 높일 수 있음.\n",
    "> 최근 연구에는 SGD, 모멘텀, 네스테로프 가속 경사 등에서 학습률 감소 대신 배치 크기를 늘리는 것으로 같은 효과를 낼 수 있다고도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 과대적합을 피하는 데 사용함.\n",
    "* 심층 신경망은 파라미터가 매우 많으므로 자유도가 매우 높음. 즉, 대규모의 복잡한 데이터셋을 학습할 수 있음.\n",
    "* 그러나, 자유도가 높으면 훈련 세트에 과대적합될 수 있으므로 규제를 사용해야 함.\n",
    "* 조기 종료, 배치 정규화 등이 규제 방법으로 사용하고, 그 외에 $l_1$ 및 $l_2$규제, dropout, max-norm규제가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $l_1, l_2$ 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 연결 가중치를 제한하기 위해 $l_2$규제를 사용하거나, 많은 가중치가 0인 희소 모델을 만들기 위해 $l_1$규제를 사용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))   # 규제 강도가 0.01인 l2규제 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>l2</code>함수는 훈련하는 동안 규제 손실을 계산하기 위해, 각 step에서 호출되는 규제 객체를 반환하고, 이 손실은 최종 손실에 합산됨.\n",
    "* $l_1$규제가 필요하면 <code>keras.regularizers.l1()</code>을 사용할 수 있고, l1과 l2둘 다 필요하다면 <code>regularizers.l1_l2(l1=..., l2=...)</code>와 같이 사용.\n",
    "* 일반적으로 모든 은닉층에 동일한 활성화 함수, 초기좌 전략, 규제를 적용하기 때문에 반복문을 사용하도록 refactoring하거나 <code>functools.partial()</code> 함수를 사용해 기본 매개변수 값을 사용하여 함수 호출을 감쌀 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가장 인기 있는 규제 기법 중 하나. 대부분의 경우에서 정확도의 상승을 기대할 수 있음.\n",
    "* 매 훈련 step마다 각 뉴런(입력 뉴런은 포함하고, 출력 뉴런은 제외)이 드랍아웃될 확룰 $p$를 가짐. 즉, $p$라는 확률에 따라 각 훈련 step에서 뉴런은 무시될수도 있고, 활성화될수도 있음.\n",
    "  * 하이퍼파라미터 $p$는 드롭아웃 비율(dropout rate)이라고 하고, 주로 10%~50%사이를 지정함.\n",
    "  * 순환 신경망에서는 20~30%, CNN에서는 40~50% 사용.\n",
    "* 훈련이 끝난 뒤에는 dropout을 적용하지 않음.\n",
    "* 드랍아웃으로 훈련된 뉴런은 이웃한 뉴런에 맞추어 적응되지 않고, 각각의 뉴런 자기 자신이 유용해지게 됨. 또한, 몇 개의 입력 뉴런에만 지나치게 의존하지 않고 모든 입력 뉴런에 주의를 기울이게 됨. 이는 입력의 작은 변화에 덜 민감해지게 되어 안정적인 네트워크를 만들어주고, 일반화 성능이 좋아짐.\n",
    "* 또한, 각 훈련 step에서 unique한 네트워크가 생성된다고 생각할 수 있음.\n",
    "  * 드랍아웃이 가능한 뉴런 수가 N개라면 $2^N$개의 unique한 네트워크가 생길 수 있고, 이 수는 매우 크므로 동일한 네트워크가 선택될 가능성이 매우 낮음.\n",
    "  * 이 신경망은 대부분의 가중치를 공유하고 있어 아주 독립적이진 않음에도 형태가 모두 다르므로 결과적으로 만들어진 신경망은 모든 신경망을 평균한 앙상블로 볼 수 있음.\n",
    "* 일반적으로 출력층을 제외하고 맨 위의 층부터 세 번째 층까지 있는 뉴런에만 드랍아웃을 적용함.\n",
    "* $p=50\\%$일 때, 테스트하는 동안에는 하나의 뉴런이 훈련 때보다 평균적으로 두 배 많은 입력 뉴런과 연결된다(테스트할 때는 드롭아웃을 적용하지 않으므로). 이를 보상하기 위해 훈련 뒤에는 각 뉴런의 연결 가중치에 0.5를 곱할 필요가 있음. 그렇지 않으면 각 뉴런이 훈련 시 보다 두배 많은 입력 신호를 받기 때문에 잘 동작하지 않을 수 있음.\n",
    "  * 일반적으로 표현하면 훈련이 끝난 뒤, 각 입력의 연결 가중치에 보존 확률(keep probability) $1-p$를 곱하거나, 훈련하는 동안 각 뉴런의 출력을 보존 확률로 나눌 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras에서의 드롭아웃 구현. keras.layers.Dropout층을 사용\n",
    "#   훈련하는 동안 일부 입력을 랜덤하게 버림(0으로 만듦). 그 뒤 남은 입력을 보존 확률로 나눔.\n",
    "#   훈련이 끝난 뒤에는 입력을 그대로 출력으로 전달함.\n",
    "#   Dropout층 이전의 층의 출력을 입력으로 받아 확률에 따라 그 일부를 dropout함.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 드롭아웃은 훈련하는 동안에만 활성화되므로 훈련 손실과 검증 손실이 다르게 나올 수 있음. 따라서 훈련이 끝난 후 드롭아웃을 빼고 훈련 손실을 평가해야 함.\n",
    "* 모델이 과대적합되면 드롭아웃 비율을 늘릴 수 있고, 과소적합되면 드롭아웃 비율을 낮춰야 함.\n",
    "* 또한, 층이 클 때는 드롭아웃 비율을 늘리고 작은 층에는 드롭아웃 비율을 낮추는 것이 좋음.\n",
    "* 최근의 많은 신경망 구조는 마지막 은닉층 뒤에만 드롭아웃을 사용함(드롭아웃을 전체에 사용하는 것이 너무 강할 때 사용).\n",
    "* 드롭아웃은 수렴을 상당히 느리게 만들지만, 적절히 튜닝하면 훨씬 좋은 모델을 만듦.\n",
    "> SELU 활성화 함수를 기반으로 한 자기 정규화 네트워크를 규제하려면 alpha dropout(https://arxiv.org/pdf/1706.02515.pdf, New Dropout Technique)을 사용해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 몬테 카를로 드롭아웃\n",
    "* 훈련된 드롭아웃 모델을 재훈련하거나 수정하지 않고 성능을 향상시킬 수 있는 방법.\n",
    "* 드롭아웃을 활성화한 상태로 여러 개의 예측을 만들어 예측 결과를 평균함.\n",
    "* 드롭아웃 모델의 성능을 높여주고 더 정확한 불확실성 추정을 제공함.\n",
    "  * 훈련하는 동안은 일반적인 드롭아웃과 동일하므로 규제처럼 작동."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashoin_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashoin_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3931 - accuracy: 0.8535 - val_loss: 0.3310 - val_accuracy: 0.8786\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3892 - accuracy: 0.8567 - val_loss: 0.3295 - val_accuracy: 0.8822\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3914 - accuracy: 0.8554 - val_loss: 0.3349 - val_accuracy: 0.8784\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3873 - accuracy: 0.8559 - val_loss: 0.3263 - val_accuracy: 0.8800\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3861 - accuracy: 0.8571 - val_loss: 0.3248 - val_accuracy: 0.8814\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3884 - accuracy: 0.8566 - val_loss: 0.3243 - val_accuracy: 0.8798\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3833 - accuracy: 0.8580 - val_loss: 0.3246 - val_accuracy: 0.8812\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3816 - accuracy: 0.8581 - val_loss: 0.3247 - val_accuracy: 0.8812\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3815 - accuracy: 0.8605 - val_loss: 0.3200 - val_accuracy: 0.8826\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3784 - accuracy: 0.8594 - val_loss: 0.3225 - val_accuracy: 0.8822\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3786 - accuracy: 0.8598 - val_loss: 0.3204 - val_accuracy: 0.8834\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3741 - accuracy: 0.8600 - val_loss: 0.3246 - val_accuracy: 0.8812\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3753 - accuracy: 0.8603 - val_loss: 0.3192 - val_accuracy: 0.8838\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3744 - accuracy: 0.8603 - val_loss: 0.3236 - val_accuracy: 0.8822\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3741 - accuracy: 0.8603 - val_loss: 0.3169 - val_accuracy: 0.8838\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3740 - accuracy: 0.8616 - val_loss: 0.3166 - val_accuracy: 0.8820\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3711 - accuracy: 0.8615 - val_loss: 0.3231 - val_accuracy: 0.8814\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3712 - accuracy: 0.8637 - val_loss: 0.3163 - val_accuracy: 0.8814\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3708 - accuracy: 0.8627 - val_loss: 0.3144 - val_accuracy: 0.8848\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3687 - accuracy: 0.8645 - val_loss: 0.3176 - val_accuracy: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24f5ea1d430>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test,  training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>model(X)</code> : <code>model.predict(X)</code>와 비슷. 단, tensor를 반환함.\n",
    "> * <code>training=True</code> 로 지정했기 때문에 Dropout층이 활성화되어 예측이 달라짐.\n",
    "> * test set으로부터 100개의 예측을 만들어 쌓음. 모델을 호출할 때 마다 샘플이 행이고, 클래스마다 하나의 열을 가진 행렬이 반환됨. 즉, test set에 10000개의 샘플과 10개의 클래스가 있으므로 행렬의 크기는 [10000, 10]이 됨. 이를 100개 쌓았기 때문에 y_probas는 [100, 10000, 10]크기가 됨.\n",
    "> * <code>.mean(axis=0)</code>은 첫 번재 차원을 기준으로 평균을 냄. 결과로 나온 y_proba의 크기는 [10000, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위처럼 드롧아웃으로 만든 예측들을 평균하면 드롭아웃 없이 예측한 하나의 결과보다 더 안정적으로 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout없이 예측한 결과\n",
    "pred = np.round(model.predict(X_test[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.13, 0.  , 0.84]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24f56dc8fa0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title(f\"label : {y_test[:1][0]}, predict : {np.argmax(pred)}\")\n",
    "plt.imshow(X_test[:1][0], cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.23, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.13, 0.02, 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.03, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.12, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.16, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.05, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.18, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.11, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.05, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.09, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.02, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.42, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.06, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.22, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.04, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.2 , 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.02, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.29, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.06, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.13, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.07, 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.18, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.13, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.19, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.07, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.05, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.21, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.48, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.08, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.21, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.16, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.05, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.04, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.18, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.12, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.06, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.05, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.04, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.12, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.39, 0.  , 0.44]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.08, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.13, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.14, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.12, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 0.  , 0.1 , 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.64, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.21, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.42, 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.27, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.19, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.25, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.18, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.03, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.21, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.15, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.27, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.18, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.46, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.09, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.11, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.33, 0.  , 0.12, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.45, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.03, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.2 , 0.01, 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.17, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.  , 0.01, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.14, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.05, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.16, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.61, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.33, 0.03, 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.26, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.27, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.18, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.77, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.19, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.02, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.3 , 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.06, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.25, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.2 , 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.01, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.07, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.11, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.2 , 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.37, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.28, 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.53, 0.  , 0.41]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropout을 활성화한 상태에서의 여러 예측\n",
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * dropout을 끈 상태에서 모델은 첫 번째 test set 데이터에 대해 84%의 확률로 ankle boot라고 예측\n",
    "> * dropout을 활성화한 상태에서는 끈 상태보다 ankle boot라는 예측을 좀 더 약하게 함.\n",
    ">   * 신발이라는 공통점을 가진 라벨인 샌들(5)과 스니커즈(7)로 예측하기도 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.17, 0.  , 0.71]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_probas를 평균낸 결과.\n",
    "# dropout없이 예측한 결과보다 확신을 덜 함.(71%)\n",
    "\n",
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.15, 0.  , 0.18]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확률 추정의 표준 분포 확인\n",
    "\n",
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 드롭아웃이 적용된 예측으로 정확도 출력\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "np.sum(y_pred == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 의료 및 금융 서비스 등 위험에 민감한 시스템을 만들 때 불확실한 예측을 주의깊게 다뤄야 함.\n",
    "  * 드롭아웃을 적용하지 않은 예측과 같은 높은 확신을 가진 예측으로 다루어서는 안됨.\n",
    "* 몬테 카를로 샘플의 숫자(위에서는 100개)는 튜닝 가능.\n",
    "  * 값이 높을수록 예측과 불확실성 추정이 정확해지지만, 수가 두 배로 늘어나면 예측 시간도 두 배로 늘어남.\n",
    "  * 또한, 일정 수 이상 넘어가면 성능 향상이 크지 않음.\n",
    "* <code>BatchNormalization</code>층과 같이 모델이 훈련하는 동안 다르게 작동하는 층이 있다면 훈련 모드를 강제로 설정하면 안 됨.\n",
    "  * <code>Dropout</code>층을 아래와 같은 <code>MCDropout</code>로 바꿔 해결가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>Dropout</code>을 상속해 <code>call()</code>을 오버라이드하여 training 매개변수를 강제로 True로 설정\n",
    "> * 모든 keras API에서 사용가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Norm 규제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 뉴런에 대한 입력 연결 가중치 $\\mathbf{w}$가 $\\parallel\\mathbf{w}\\parallel_2\\leq r$이 되도록 제한.\n",
    "  * r은 max-norm 하이퍼파라미터이고, $\\parallel\\;\\parallel_2$는 $l_2$ norm\n",
    "* 전체 손실 함수에 규제 손실 항을 추가하지 않고 매 훈련 스텝이 끝날 때 $\\parallel\\mathbf{w}\\parallel_2$를 계산하고 필요하면 $\\mathbf{w}$의 스케일을 조정함($\\displaystyle\\mathbf{w}\\leftarrow\\mathbf{w}\\frac{r}{\\parallel\\!w\\!\\parallel_2}$)\n",
    "* r을 줄이면 규제의 양이 커져 과대적합을 감소시킬 수 있음.\n",
    "* 불안정한 gradient문제를 완화하는 데 도움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x24f56de8fa0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras에서의 max-norm 규제 사용\n",
    "# 적절한 최댓값으로 지정한 max_norm()이 반환한 객체로 은닉층의 kernel_constraint 매개변수 지정\n",
    "\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 매 훈련 반복이 끝난 후 fit()이 층이 가중치와 함깨 max_norm()이 반환한 객체를 호출하고 스케일이 조정된 가중치를 받음.\n",
    "* max_norm()에는 axis 매개변수(기본값 0) 존재. Dense층은 보통 [샘플 개수, 뉴런 개수] 크기의 가중치를 가지는데 axis=0를 사용하면 각 뉴런의 가중치 벡터에 독립적으로 적용\n",
    "  * CNN에서 axis 매개변수를 적절히 조정해야함(<code>axis=[0,1,2]</code>와 같이)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리 및 가이드라인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일반적으로 잘 맞는 설정(절대적인 것은 아님.)\n",
    "\n",
    "|하이퍼파라미터|기본값|\n",
    "|------------|------|\n",
    "|커널 초기화|He 초기화|\n",
    "|활성화 함수 |ELU     |\n",
    "|정규화      |얕은 신경망-없음, 깊은 신경망-배치 정규화|\n",
    "|규제        |조기 종료(필요 시 l2 추가)|\n",
    "|optimizer  |모멘텀, RMSProp, Nadam|\n",
    "|학습률 스케줄|1cycle|\n",
    "\n",
    "* 네트워크가 완전 연결 층을 쌓은 단순한 모델이면 자기 정규화 사용 가능. 이런 경우에서의 일반적으로 잘 맞는 설정\n",
    "\n",
    "|하이퍼파라미터|기본값|\n",
    "|------------|------|\n",
    "|커널 초기화|LeCun 초기화|\n",
    "|활성화 함수 |SELU     |\n",
    "|정규화      |없음(자기 정규화됨)|\n",
    "|규제        |알파 드롭아웃(필요 시)|\n",
    "|optimizer  |모멘텀, RMSProp, Nadam|\n",
    "|학습률 스케줄|1cycle|\n",
    "\n",
    "* 입력 특성은 정규화가 필요.\n",
    "* 비슷한 문제를 해결한 모델을 찾을 수 있다면 사전훈련된 신경망의 일부를 재사용하는 것이 좋음.\n",
    "* 레이블이 없는 데이터가 많다면 비지도 사전훈련 사용\n",
    "* 비슷한 작업을 위한 레이블된 데이터가 많다면 보조 작업에서 사전훈련 수행\n",
    "* 희소 모델이 필요하다면 $l_1$규제를 사용할 수 있음.\n",
    "  * 매우 희소한 모델이 필요하다면 TF-MOT 사용 가능. 단, 자기 정규화를 깨뜨리므로 위의 일반적으로 잘 맞는 설정을 사용해야 함.\n",
    "* 빠른 응답을 하는 모델(예측이 매우 빠른 모델)이 필요하다면 층 개수를 줄이고 배치 정규화 층을 이전 층에 합침. 또한, LeakyReLU와 ReLU같이 빠른 활성화 함수를 써도 되고 희소 모델을 만드는 것도 도움이 됨. 부동소수점 정밀도를 낮출 수도 있음.\n",
    "* 위험에 민감하고 예측 속도가 중요하지 않다면 MC 드롭아웃 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100개의 뉴런을 가진 은닉층 20개로 만들어진 심층 신경망\n",
    "# activation함수는 ELU, 초기화 전략은 He 초기화\n",
    "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model_cifar = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 image 데이터셋 로드\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "X_train_normalized = X_train/255\n",
    "X_test_normalized = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_normalized, X_valid, y_train, y_valid = train_test_split(X_train_normalized, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 별 이름\n",
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24f1b4de760>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 0번째 이미지 및 레이블 출력(개구리)\n",
    "plt.title(class_names[y_train[0][0]])\n",
    "plt.imshow(X_train_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10장에서 사용한 최적의 학습률 찾기\n",
    "K = keras.backend\n",
    "\n",
    "class ExponentialLrCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.lr = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.lr.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam optimizer 이용, 학습률 1e-5부터 0.5%씩 증가\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "expon_lr = ExponentialLrCallback(1.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "176/176 [==============================] - 7s 24ms/step - loss: 2.3375 - accuracy: 0.1858\n",
      "Epoch 2/5\n",
      "176/176 [==============================] - 4s 24ms/step - loss: 1.9856 - accuracy: 0.2794\n",
      "Epoch 3/5\n",
      "176/176 [==============================] - 4s 24ms/step - loss: 1.9322 - accuracy: 0.3015\n",
      "Epoch 4/5\n",
      "176/176 [==============================] - 4s 24ms/step - loss: 1.9120 - accuracy: 0.3124\n",
      "Epoch 5/5\n",
      "176/176 [==============================] - 4s 24ms/step - loss: 1.9771 - accuracy: 0.2904\n"
     ]
    }
   ],
   "source": [
    "# 작은 epoch로 fit\n",
    "history = model_cifar.fit(X_train_normalized, y_train, epochs=5, batch_size=256, callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습률에 따른 loss 곡선 출력\n",
    "plt.plot(expon_lr.lr, expon_lr.losses, 'r-')\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.lr), max(expon_lr.lr))\n",
    "plt.axis([min(expon_lr.lr), max(expon_lr.lr), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.ylim([1.2, 2.5])\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "176/176 [==============================] - 9s 28ms/step - loss: 1.6909 - accuracy: 0.3938 - val_loss: 1.6808 - val_accuracy: 0.3906\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.6211 - accuracy: 0.4174 - val_loss: 1.6593 - val_accuracy: 0.3966\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 5s 29ms/step - loss: 1.5817 - accuracy: 0.4326 - val_loss: 1.6155 - val_accuracy: 0.4170\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.5504 - accuracy: 0.4475 - val_loss: 1.5954 - val_accuracy: 0.4198\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.5261 - accuracy: 0.4548 - val_loss: 1.5725 - val_accuracy: 0.4310\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.5035 - accuracy: 0.4617 - val_loss: 1.5705 - val_accuracy: 0.4316\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.4830 - accuracy: 0.4710 - val_loss: 1.5549 - val_accuracy: 0.4406\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.4672 - accuracy: 0.4764 - val_loss: 1.5441 - val_accuracy: 0.4440\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.4520 - accuracy: 0.4817 - val_loss: 1.5324 - val_accuracy: 0.4488\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.4361 - accuracy: 0.4874 - val_loss: 1.5273 - val_accuracy: 0.4538\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.4183 - accuracy: 0.4950 - val_loss: 1.5201 - val_accuracy: 0.4516\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.4061 - accuracy: 0.4987 - val_loss: 1.5165 - val_accuracy: 0.4574\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 5s 29ms/step - loss: 1.3930 - accuracy: 0.5052 - val_loss: 1.5059 - val_accuracy: 0.4626\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.3795 - accuracy: 0.5101 - val_loss: 1.5111 - val_accuracy: 0.4564\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.3658 - accuracy: 0.5148 - val_loss: 1.5114 - val_accuracy: 0.4574\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.3552 - accuracy: 0.5175 - val_loss: 1.5009 - val_accuracy: 0.4626\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.3436 - accuracy: 0.5243 - val_loss: 1.5222 - val_accuracy: 0.4652\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.3305 - accuracy: 0.5277 - val_loss: 1.4962 - val_accuracy: 0.4664\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.3216 - accuracy: 0.5305 - val_loss: 1.4990 - val_accuracy: 0.4714\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.3098 - accuracy: 0.5359 - val_loss: 1.4895 - val_accuracy: 0.4668\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.2998 - accuracy: 0.5373 - val_loss: 1.4984 - val_accuracy: 0.4684\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.2862 - accuracy: 0.5433 - val_loss: 1.4995 - val_accuracy: 0.4662\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.2769 - accuracy: 0.5469 - val_loss: 1.4882 - val_accuracy: 0.4662\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.2681 - accuracy: 0.5503 - val_loss: 1.4950 - val_accuracy: 0.4688\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.2578 - accuracy: 0.5546 - val_loss: 1.4964 - val_accuracy: 0.4708\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.2485 - accuracy: 0.5559 - val_loss: 1.4897 - val_accuracy: 0.4704\n",
      "Epoch 27/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.2361 - accuracy: 0.5596 - val_loss: 1.4900 - val_accuracy: 0.4782\n",
      "Epoch 28/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.2301 - accuracy: 0.5633 - val_loss: 1.5036 - val_accuracy: 0.4686\n",
      "Epoch 29/100\n",
      "176/176 [==============================] - 5s 26ms/step - loss: 1.2205 - accuracy: 0.5670 - val_loss: 1.4970 - val_accuracy: 0.4726\n",
      "Epoch 30/100\n",
      "176/176 [==============================] - 5s 27ms/step - loss: 1.2099 - accuracy: 0.5700 - val_loss: 1.4973 - val_accuracy: 0.4724\n",
      "Epoch 31/100\n",
      "176/176 [==============================] - 5s 28ms/step - loss: 1.2007 - accuracy: 0.5746 - val_loss: 1.4980 - val_accuracy: 0.4740\n",
      "Epoch 32/100\n",
      "176/176 [==============================] - 5s 29ms/step - loss: 1.1922 - accuracy: 0.5776 - val_loss: 1.5221 - val_accuracy: 0.4732\n",
      "Epoch 33/100\n",
      "176/176 [==============================] - 4s 24ms/step - loss: 1.1841 - accuracy: 0.5799 - val_loss: 1.4972 - val_accuracy: 0.4756\n",
      "Epoch 34/100\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 1.1740 - accuracy: 0.5839 - val_loss: 1.5017 - val_accuracy: 0.4754\n",
      "Epoch 35/100\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 1.1671 - accuracy: 0.5861 - val_loss: 1.5003 - val_accuracy: 0.4800\n",
      "Epoch 36/100\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 1.1576 - accuracy: 0.5900 - val_loss: 1.4976 - val_accuracy: 0.4840\n",
      "Epoch 37/100\n",
      "176/176 [==============================] - 4s 21ms/step - loss: 1.1511 - accuracy: 0.5926 - val_loss: 1.5064 - val_accuracy: 0.4770\n",
      "Epoch 38/100\n",
      "176/176 [==============================] - 4s 22ms/step - loss: 1.1415 - accuracy: 0.5940 - val_loss: 1.5134 - val_accuracy: 0.4774\n"
     ]
    }
   ],
   "source": [
    "# Nadam optimizer의 학습률은 5e-5로 결정.\n",
    "# 조기 종료 추가, 최선의 모델을 저장하는 체크포인트 추가, 100 epoch\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_cifar.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_cifar.fit(X_train_normalized, y_train, validation_data=(X_valid, y_valid) ,epochs=100, batch_size=256, callbacks=[keras.callbacks.EarlyStopping(patience=15), keras.callbacks.ModelCheckpoint(\"cifar_model.h5\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4882 - accuracy: 0.4662\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4549 - accuracy: 0.4906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4549211263656616, 0.49059998989105225]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습된 모델로 평가\n",
    "# validation set 정확도 약 50.7%\n",
    "# test set 정확도 약 51%\n",
    "model = keras.models.load_model('cifar_model.h5')\n",
    "model.evaluate(X_valid, y_valid)\n",
    "model.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0.4, 1.5) # 수직축의 범위 설정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 층마다 배치 정규화 적용\n",
    "model_cifar_batchnorm = keras.models.Sequential()\n",
    "model_cifar_batchnorm.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model_cifar_batchnorm.add(keras.layers.BatchNormalization())\n",
    "\n",
    "for _ in range(20):\n",
    "    model_cifar_batchnorm.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model_cifar_batchnorm.add(keras.layers.BatchNormalization())\n",
    "    model_cifar_batchnorm.add(keras.layers.Activation(\"elu\"))\n",
    "model_cifar_batchnorm.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "176/176 [==============================] - 19s 53ms/step - loss: 1.8045 - accuracy: 0.3574 - val_loss: 1.7791 - val_accuracy: 0.3860\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.5291 - accuracy: 0.4528 - val_loss: 1.5764 - val_accuracy: 0.4358\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 9s 51ms/step - loss: 1.4381 - accuracy: 0.4896 - val_loss: 1.5274 - val_accuracy: 0.4518\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.3656 - accuracy: 0.5146 - val_loss: 1.5062 - val_accuracy: 0.4680\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.3101 - accuracy: 0.5357 - val_loss: 1.4712 - val_accuracy: 0.4784\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 9s 49ms/step - loss: 1.2621 - accuracy: 0.5538 - val_loss: 1.4796 - val_accuracy: 0.4850\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.2140 - accuracy: 0.5699 - val_loss: 1.4307 - val_accuracy: 0.4958\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.1732 - accuracy: 0.5838 - val_loss: 1.4832 - val_accuracy: 0.4802\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 9s 51ms/step - loss: 1.1335 - accuracy: 0.5985 - val_loss: 1.4423 - val_accuracy: 0.4990\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.1018 - accuracy: 0.6090 - val_loss: 1.4761 - val_accuracy: 0.4906\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.0637 - accuracy: 0.6221 - val_loss: 1.4940 - val_accuracy: 0.5032\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.0305 - accuracy: 0.6351 - val_loss: 1.4978 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 1.0027 - accuracy: 0.6452 - val_loss: 1.4836 - val_accuracy: 0.5076\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 0.9747 - accuracy: 0.6564 - val_loss: 1.5292 - val_accuracy: 0.4942\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 9s 51ms/step - loss: 0.9423 - accuracy: 0.6678 - val_loss: 1.5420 - val_accuracy: 0.4980\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 0.9137 - accuracy: 0.6752 - val_loss: 1.5232 - val_accuracy: 0.4986\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 9s 51ms/step - loss: 0.8885 - accuracy: 0.6849 - val_loss: 1.5793 - val_accuracy: 0.4974\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 9s 51ms/step - loss: 0.8593 - accuracy: 0.6961 - val_loss: 1.5752 - val_accuracy: 0.5008\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 0.8305 - accuracy: 0.7064 - val_loss: 1.6010 - val_accuracy: 0.5044\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 9s 50ms/step - loss: 0.8139 - accuracy: 0.7118 - val_loss: 1.6031 - val_accuracy: 0.5010\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 9s 49ms/step - loss: 0.7863 - accuracy: 0.7212 - val_loss: 1.6079 - val_accuracy: 0.5078\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 9s 49ms/step - loss: 0.7530 - accuracy: 0.7338 - val_loss: 1.6703 - val_accuracy: 0.4996\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_cifar_batchnorm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "history_batchnorm = model_cifar_batchnorm.fit(X_train_normalized, y_train, validation_data=(X_valid, y_valid) ,epochs=100, batch_size=256, callbacks=[keras.callbacks.EarlyStopping(patience=15), keras.callbacks.ModelCheckpoint(\"cifar_model_batchnorm.h5\", save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 5ms/step - loss: 1.4307 - accuracy: 0.4958\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.4097 - accuracy: 0.5016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4097179174423218, 0.5016000270843506]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도가 조금 더 높고, 적은 epoch만으로 더 빨리 수렴함.\n",
    "model = keras.models.load_model('cifar_model_batchnorm.h5')\n",
    "model.evaluate(X_valid, y_valid)\n",
    "model.evaluate(X_test_normalized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selu 활성화 함수를 위한 데이터셋 수정\n",
    "# 평균 0, 표준편차 1로 만듦.\n",
    "from scipy import stats\n",
    "X_train_selu = stats.zscore(X_train_normalized)\n",
    "X_valid_selu = stats.zscore(X_valid)\n",
    "X_test_selu = stats.zscore(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0012733866963876e-14"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selu.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999972"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_selu.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 함수는 selu, 가중치는 lecun 정규분포 초기화\n",
    "model_cifar_selu = keras.models.Sequential()\n",
    "model_cifar_selu.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(20):\n",
    "    model_cifar_selu.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_selu.add(RegularizedDense(10, activation=\"softmax\"))\n",
    "\n",
    "earlystop_callback = keras.callbacks.EarlyStopping(patience=15)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"cifar_model_selu.h5\", save_best_only=True)\n",
    "callbacks = [earlystop_callback, checkpoint_callback]\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "model_cifar_selu.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "176/176 [==============================] - 9s 34ms/step - loss: 1.8988 - accuracy: 0.3211 - val_loss: 1.7321 - val_accuracy: 0.3874\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.6481 - accuracy: 0.4143 - val_loss: 1.6956 - val_accuracy: 0.3936\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 1.5383 - accuracy: 0.4562 - val_loss: 1.6285 - val_accuracy: 0.4284\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 1.4621 - accuracy: 0.4819 - val_loss: 1.5521 - val_accuracy: 0.4476\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 1.3926 - accuracy: 0.5062 - val_loss: 1.5280 - val_accuracy: 0.4638\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 1.3380 - accuracy: 0.5284 - val_loss: 1.5128 - val_accuracy: 0.4764\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 6s 31ms/step - loss: 1.2900 - accuracy: 0.5434 - val_loss: 1.5152 - val_accuracy: 0.4736\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.2490 - accuracy: 0.5580 - val_loss: 1.5694 - val_accuracy: 0.4622\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.1986 - accuracy: 0.5771 - val_loss: 1.5104 - val_accuracy: 0.4764\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 6s 31ms/step - loss: 1.1561 - accuracy: 0.5907 - val_loss: 1.5041 - val_accuracy: 0.4850\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.1192 - accuracy: 0.6060 - val_loss: 1.4706 - val_accuracy: 0.5082\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 5s 31ms/step - loss: 1.0786 - accuracy: 0.6200 - val_loss: 1.4962 - val_accuracy: 0.5026\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.0429 - accuracy: 0.6340 - val_loss: 1.5800 - val_accuracy: 0.4922\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 1.0126 - accuracy: 0.6423 - val_loss: 1.5394 - val_accuracy: 0.4954\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.9773 - accuracy: 0.6561 - val_loss: 1.5356 - val_accuracy: 0.4928\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.9455 - accuracy: 0.6673 - val_loss: 1.5659 - val_accuracy: 0.4982\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.9123 - accuracy: 0.6797 - val_loss: 1.5939 - val_accuracy: 0.4942\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.8838 - accuracy: 0.6895 - val_loss: 1.6053 - val_accuracy: 0.5002\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.8578 - accuracy: 0.6996 - val_loss: 1.6315 - val_accuracy: 0.5010\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.8250 - accuracy: 0.7118 - val_loss: 1.6126 - val_accuracy: 0.5082\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.7972 - accuracy: 0.7194 - val_loss: 1.6739 - val_accuracy: 0.4970\n",
      "Epoch 22/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.7756 - accuracy: 0.7259 - val_loss: 1.7067 - val_accuracy: 0.4940\n",
      "Epoch 23/100\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 0.7491 - accuracy: 0.7360 - val_loss: 1.7503 - val_accuracy: 0.4912\n",
      "Epoch 24/100\n",
      "176/176 [==============================] - 6s 33ms/step - loss: 0.7216 - accuracy: 0.7474 - val_loss: 1.7572 - val_accuracy: 0.4930\n",
      "Epoch 25/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.6992 - accuracy: 0.7566 - val_loss: 1.8164 - val_accuracy: 0.4942\n",
      "Epoch 26/100\n",
      "176/176 [==============================] - 6s 32ms/step - loss: 0.6796 - accuracy: 0.7622 - val_loss: 1.8397 - val_accuracy: 0.4948\n"
     ]
    }
   ],
   "source": [
    "history_selu = model_cifar_selu.fit(X_train_selu, y_train, validation_data=(X_valid_selu, y_valid) ,epochs=100, batch_size=256, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4706 - accuracy: 0.5082\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.4438 - accuracy: 0.5065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.443793535232544, 0.5065000057220459]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('cifar_model_selu.h5')\n",
    "model.evaluate(X_valid_selu, y_valid)\n",
    "model.evaluate(X_test_selu, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha dropout 사용(selu에서 사용하는 dropout)\n",
    "model_cifar_alpha = keras.models.Sequential()\n",
    "model_cifar_alpha.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(17):\n",
    "    model_cifar_alpha.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_alpha.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_alpha.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_alpha.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_alpha.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_alpha.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_alpha.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_alpha.add(RegularizedDense(10, activation=\"softmax\"))\n",
    "\n",
    "earlystop_callback = keras.callbacks.EarlyStopping(patience=15)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\"cifar_model_alpha.h5\", save_best_only=True)\n",
    "callbacks = [earlystop_callback, checkpoint_callback]\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "model_cifar_alpha.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "176/176 [==============================] - 12s 43ms/step - loss: 2.1640 - accuracy: 0.2622 - val_loss: 1.9302 - val_accuracy: 0.3762\n",
      "Epoch 2/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 1.7599 - accuracy: 0.3726 - val_loss: 1.7760 - val_accuracy: 0.3956\n",
      "Epoch 3/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.6274 - accuracy: 0.4300 - val_loss: 1.6805 - val_accuracy: 0.4338\n",
      "Epoch 4/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.5443 - accuracy: 0.4575 - val_loss: 1.7173 - val_accuracy: 0.4490\n",
      "Epoch 5/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.4782 - accuracy: 0.4809 - val_loss: 1.7230 - val_accuracy: 0.4506\n",
      "Epoch 6/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.4150 - accuracy: 0.5033 - val_loss: 1.6492 - val_accuracy: 0.4652\n",
      "Epoch 7/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.3672 - accuracy: 0.5240 - val_loss: 1.7240 - val_accuracy: 0.4578\n",
      "Epoch 8/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.3187 - accuracy: 0.5414 - val_loss: 1.8776 - val_accuracy: 0.4722\n",
      "Epoch 9/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.2727 - accuracy: 0.5596 - val_loss: 1.7816 - val_accuracy: 0.4832\n",
      "Epoch 10/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.2353 - accuracy: 0.5718 - val_loss: 1.7899 - val_accuracy: 0.4818\n",
      "Epoch 11/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.1974 - accuracy: 0.5856 - val_loss: 1.7660 - val_accuracy: 0.4820\n",
      "Epoch 12/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 1.1556 - accuracy: 0.5990 - val_loss: 1.8645 - val_accuracy: 0.4922\n",
      "Epoch 13/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 1.1192 - accuracy: 0.6120 - val_loss: 1.9073 - val_accuracy: 0.4918\n",
      "Epoch 14/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.0888 - accuracy: 0.6245 - val_loss: 1.8954 - val_accuracy: 0.5020\n",
      "Epoch 15/100\n",
      "176/176 [==============================] - 7s 40ms/step - loss: 1.0586 - accuracy: 0.6350 - val_loss: 1.9739 - val_accuracy: 0.4954\n",
      "Epoch 16/100\n",
      "176/176 [==============================] - 7s 41ms/step - loss: 1.0195 - accuracy: 0.6464 - val_loss: 2.0099 - val_accuracy: 0.4908\n",
      "Epoch 17/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 0.9966 - accuracy: 0.6586 - val_loss: 2.1944 - val_accuracy: 0.4816\n",
      "Epoch 18/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 0.9709 - accuracy: 0.6673 - val_loss: 2.1860 - val_accuracy: 0.4924\n",
      "Epoch 19/100\n",
      "176/176 [==============================] - 7s 39ms/step - loss: 0.9325 - accuracy: 0.6768 - val_loss: 2.1142 - val_accuracy: 0.4976\n",
      "Epoch 20/100\n",
      "176/176 [==============================] - 8s 46ms/step - loss: 0.9109 - accuracy: 0.6867 - val_loss: 2.3610 - val_accuracy: 0.4832\n",
      "Epoch 21/100\n",
      "176/176 [==============================] - 7s 41ms/step - loss: 0.8894 - accuracy: 0.6927 - val_loss: 2.3872 - val_accuracy: 0.4904\n"
     ]
    }
   ],
   "source": [
    "history_alpha = model_cifar_alpha.fit(X_train_selu, y_train, validation_data=(X_valid_selu, y_valid) ,epochs=100, batch_size=256, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델에서 드롭아웃을 제거하지 않고 평가하는 몬테 카를로 드롭아웃(MC dropout) 사용\n",
    "model = keras.models.load_model('cifar_model_alpha.h5')\n",
    "y_probas = np.stack([model(X_test_selu, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4859"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "np.sum(y_pred == y_test.T) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha dropout 사용(selu에서 사용하는 dropout)\n",
    "model_cifar_1cycle = keras.models.Sequential()\n",
    "model_cifar_1cycle.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(17):\n",
    "    model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(RegularizedDense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model_cifar_1cycle.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 8s 19ms/step - loss: nan - accuracy: 0.1362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.7913196086883545,\n",
       " 4.165768963950021)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1cycle scheduling 사용\n",
    "# from https://github.com/rickiepark/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb\n",
    "import math\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "        \n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "        \n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "        \n",
    "rates, losses = find_learning_rate(model_cifar_1cycle, X_train_selu, y_train, epochs=1, batch_size=128)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cifar_1cycle = keras.models.Sequential()\n",
    "model_cifar_1cycle.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "\n",
    "for _ in range(17):\n",
    "    model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model_cifar_1cycle.add(keras.layers.AlphaDropout(0.2))\n",
    "model_cifar_1cycle.add(RegularizedDense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-1)\n",
    "model_cifar_1cycle.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.9972 - accuracy: 0.3038 - val_loss: 2.1459 - val_accuracy: 0.3866\n",
      "Epoch 2/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.8494 - accuracy: 0.3500 - val_loss: 2.0126 - val_accuracy: 0.4072\n",
      "Epoch 3/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.7323 - accuracy: 0.3889 - val_loss: 1.9714 - val_accuracy: 0.4064\n",
      "Epoch 4/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.6496 - accuracy: 0.4131 - val_loss: 1.8822 - val_accuracy: 0.4234\n",
      "Epoch 5/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.5835 - accuracy: 0.4422 - val_loss: 1.8704 - val_accuracy: 0.4298\n",
      "Epoch 6/30\n",
      "352/352 [==============================] - 7s 18ms/step - loss: 1.5332 - accuracy: 0.4594 - val_loss: 1.8568 - val_accuracy: 0.4492\n",
      "Epoch 7/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.4903 - accuracy: 0.4780 - val_loss: 1.7751 - val_accuracy: 0.4502\n",
      "Epoch 8/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.4523 - accuracy: 0.4913 - val_loss: 1.9696 - val_accuracy: 0.4482\n",
      "Epoch 9/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.4188 - accuracy: 0.5047 - val_loss: 1.9304 - val_accuracy: 0.4460\n",
      "Epoch 10/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.3886 - accuracy: 0.5164 - val_loss: 1.9010 - val_accuracy: 0.4594\n",
      "Epoch 11/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.3609 - accuracy: 0.5244 - val_loss: 1.8437 - val_accuracy: 0.4714\n",
      "Epoch 12/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.3303 - accuracy: 0.5336 - val_loss: 1.8613 - val_accuracy: 0.4752\n",
      "Epoch 13/30\n",
      "352/352 [==============================] - 6s 18ms/step - loss: 1.3166 - accuracy: 0.5412 - val_loss: 1.9854 - val_accuracy: 0.4588\n",
      "Epoch 14/30\n",
      "352/352 [==============================] - 6s 18ms/step - loss: 1.2939 - accuracy: 0.5491 - val_loss: 2.0546 - val_accuracy: 0.4618\n",
      "Epoch 15/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.2726 - accuracy: 0.5583 - val_loss: 2.1686 - val_accuracy: 0.4672\n",
      "Epoch 16/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.2411 - accuracy: 0.5701 - val_loss: 1.9989 - val_accuracy: 0.4818\n",
      "Epoch 17/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.1928 - accuracy: 0.5848 - val_loss: 2.1206 - val_accuracy: 0.4782\n",
      "Epoch 18/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.1530 - accuracy: 0.6028 - val_loss: 2.2492 - val_accuracy: 0.4844\n",
      "Epoch 19/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 1.1153 - accuracy: 0.6142 - val_loss: 2.0418 - val_accuracy: 0.5018\n",
      "Epoch 20/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 1.0716 - accuracy: 0.6318 - val_loss: 2.3108 - val_accuracy: 0.4940\n",
      "Epoch 21/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 1.0294 - accuracy: 0.6443 - val_loss: 2.1670 - val_accuracy: 0.4922\n",
      "Epoch 22/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 0.9865 - accuracy: 0.6582 - val_loss: 2.2804 - val_accuracy: 0.4898\n",
      "Epoch 23/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.9478 - accuracy: 0.6724 - val_loss: 2.2698 - val_accuracy: 0.4926\n",
      "Epoch 24/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.9018 - accuracy: 0.6889 - val_loss: 2.5071 - val_accuracy: 0.4984\n",
      "Epoch 25/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.8618 - accuracy: 0.7043 - val_loss: 2.7176 - val_accuracy: 0.4948\n",
      "Epoch 26/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.8165 - accuracy: 0.7180 - val_loss: 2.7949 - val_accuracy: 0.5046\n",
      "Epoch 27/30\n",
      "352/352 [==============================] - 8s 22ms/step - loss: 0.7742 - accuracy: 0.7332 - val_loss: 2.8925 - val_accuracy: 0.5014\n",
      "Epoch 28/30\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.7280 - accuracy: 0.7500 - val_loss: 3.0345 - val_accuracy: 0.5078\n",
      "Epoch 29/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 0.6851 - accuracy: 0.7652 - val_loss: 3.1900 - val_accuracy: 0.5050\n",
      "Epoch 30/30\n",
      "352/352 [==============================] - 7s 19ms/step - loss: 0.6389 - accuracy: 0.7815 - val_loss: 3.4003 - val_accuracy: 0.5050\n"
     ]
    }
   ],
   "source": [
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / 128) * 30, max_rate=0.05)\n",
    "history = model_cifar_1cycle.fit(X_train_selu, y_train, epochs=30, batch_size=128,\n",
    "                    validation_data=(X_valid_selu, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4211a37325b67dc4346d743008529599135417ca8cd1eae7178b10ad33111a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
