{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인공 뉴런"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 초기 신경망 모델 : 하나 이상의 이진 입력과 이진 출력 하나를 가짐.\n",
    "* 퍼셉트론 : 입력과 출력이 이진 값이 아닌 어떠한 숫자이고, 각 입력 연결이 가중치와 연관되어 있는 인공 뉴런(TLU, 또는 LTU)을 기반으로 한 인공 신경망 구조.\n",
    "  * TLU는 입력의 가중치 합을 계산($z=w_1x_1+w_2x_2+...+w_nx_n=\\mathbf{x}^T\\mathbf{w}$)한 뒤 그 합에 계단 함수(step function)를 적용하여 결과를 출력함\n",
    "  * $h_w(\\mathbf{x})=step(z), z=\\mathbf{x}^T\\mathbf{w}$\n",
    "  * 자주 사용되는 계단 함수는 heaviside 계단 함수 $\\text{heaviside}(z)=\\begin{cases}\n",
    "0, & z<0일 때 \\\\\n",
    "1, & z\\geq0일 때\n",
    "\\end{cases}$ 또는 부호 함수도 사용$\\text{sgn}(z)=\\begin{cases}\n",
    "-1, & z<0일 때 \\\\\n",
    "0, & z=0일 때 \\\\\n",
    "1, & z>0일 때\n",
    "\\end{cases}$\n",
    "* 하나의 TLU로는 간단한 선형 분류 문제에 사용할 수 있음. 로지스틱 회귀나 SVM 분류기처럼 임의의 선형 조합을 계산해 그 결과가 임곗값을 넘으면 양성 클래스를 출력하고 그렇지 않으면 음성 클래스 출력.\n",
    "* 퍼셉트론은 층이 하나뿐인 TLU로 구성. 각 TLU는 모든 입력에 연결되어 있음. 퍼셉트론의 입력을 입력 뉴런이라는 특별한 통과 뉴련에 주입.\n",
    "  * 입력 뉴런 : 어떤 입력이 주입되든 출력으로 통과시키는 것.\n",
    "  * 입력층은 모두 입력 뉴런으로 구성되고, 편향 특성($x_0=1$)이 더해짐. 이 편향 특성은 입력 없이 항상 1을 출력하는 특별한 종류의 뉴런인 편향 뉴런으로 표현됨. 즉, 출력 층의 입력으로 입력 층 뉴런과 편향 뉴런이 들어옴.\n",
    "* 완전 연결 층(fully connected layer, 또는 밀집 층(dense layer)) : 한 층의 모든 뉴런이 이전 층의 모든 뉴련과 연결되어 있는 것.\n",
    "* 출력의 수학적 표현 : $h_{\\mathbf{W}, b}(\\mathbf{X})=\\phi(\\mathbf{XW}+\\mathbf{b})$\n",
    "  * $\\mathbf{X}$ : 입력 특성의 행렬. 행은 샘플, 열은 특성\n",
    "  * $\\mathbf{W}$ : 편향 뉴런을 제외한 모든 연결 가중치가 포함된 행렬. 행은 입려 뉴런, 열은 출력층에 있는 인공 뉴런\n",
    "  * $\\mathbf{b}$ : 편향 뉴런과 인공 뉴런 사이의 연결 가중치.\n",
    "  * $\\phi$ : 활성화 함수. 여기서는 계단 함수.\n",
    "* 퍼셉트론은 생물학적 뉴런이 다른 뉴런을 활성화시킬 때 이 두 뉴런의 연결이 더 강해진다는 헤브의 규칙에 영향을 받아 학습함. 퍼셉트론의 학습 규칙은 오차가 감소되도록 연결이 강화됨.\n",
    "  * 학습 규칙 : $w_{i,j}^{(next step)}=w_{i,j}+\\eta(y_j-\\hat{y}_j)x_i$\n",
    "    * $w_{i,j}$ : i번째 입력 뉴런과 j번째 출력 뉴런 사이를 연결하는 가중치\n",
    "    * $x_i$ : 현재 훈련 샘플의 i번째 뉴런의 입력값\n",
    "    * $\\hat{y}_j$ : 현재 훈련 샘플의 j번째 출력 뉴런의 출력값\n",
    "    * $y_i$ : 현재 훈련 샘플의 j번째 출력 뉴런의 타깃값\n",
    "    * $\\eta$ : 학습률\n",
    "* 각 출력 뉴런의 결정 경계가 선형이므로 퍼셉트론은 복잡한 패턴을 학습하지 못하지만, 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것이 증명되었고, 이를 퍼셉트론 수렴 이론이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-2c506bbdd20d>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = (iris.target==0).astype(np.int) # 꽃잎이 Iris Setosa인지 아닌지\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron # 하나의 TLU 네트워크를 구현한 클래스.\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]     # 꽃잎의 길이와 너비\n",
    "y = (iris.target==0).astype(np.int) # 꽃잎이 Iris Setosa인지 아닌지\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])    # 길이와 너비가 2, 0.5인 꽃잎은 Iris Setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
    "\n",
    "axes = [0, 5, 0, 2]\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = per_clf.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.plot(2, 0.5, \"rs\", label=\"target\")\n",
    "\n",
    "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>Perceptron</code>클래스는 매개변수가 <code>loss=\"perceptron\", learning_rate=\"constant\", eta0=1, penalty=None</code>인 SGDClassifier(확률정 경사 하강법)과 같음.\n",
    "* 클래스에 속할 확률은 제공하지 않으며, 고정된 입곗값을 기준으로 예측을 만들기 때문에 퍼셉트론보다는 로지스틱 회귀가 선호됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단, 다른 선형 분류기와 바찬가지로 XOR 분류 문제와 같은 일부 간단한 문제는 풀 수 없음.\n",
    "* 퍼셉트론을 여러 개 쌓아올리면 일부 제약을 줄일 수 있는데, 이러한 인공 신경망을 다층 퍼셉트론(MLP)아리고 함.\n",
    "  * 다층 퍼셉트론은 XOR문제를 풀 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력층(통과, input layer) 하나, 하나 이상의 은닉층(hidden layer), 출력층(output layer)으로 구성됨.\n",
    "  * 입력층과 가까운 층을 하위 층이라고 부르고, 출력층에 가까운 층을 상위 층이라고 부름\n",
    "  * 모든 층은 편향 뉴런을 포함해 다음 층과 완전히 연결됨.\n",
    "  * 신호는 입력에서 출력으로 한 방향으로만 흐르고, 이러한 구조를 피드포워드 신경망(feedforward neural network, FNN)이라고 함.\n",
    "* 은닉층을 여러 개 쌓아 올린 인공 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 함. - 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다층 퍼셉트론 훈련에는 역전파(backpropagation) 알고리즘을 사용함.\n",
    "  * 효율적인 기법으로 gradient를 자동으로 계산하는 경사 하강법.\n",
    "  * 네트워크를 정방향으로 한번, 역방향으로 한번 통과하는 것 만으로도 모든 모델 파라미터에 대한 네트워크 오차의 gradient를 계산할 수 있음. 즉, 오차를 감소시키기 위해 각 연결 가중치 값과 편향 값이 어떻게 바뀌어야 하는지 알 수 있음.\n",
    "  * gradient를 구하면 평범한 경사 하강법을 수행하고 전체 과정은 네트워크가 어떠한 해결책으로 수렴할 때 까지 반복.\n",
    "  * 자동으로 gradient를 계산하는 것을 자동 미분(automatic differentiation)이라고 하고, 이 중 역전파에서 사용하는 기법은 후진 모드 자동 미분(reverse-mode autodiff)을 사용. 미분할 함수가 연결 가중치와 같은 변수가 많고, 출력이 적은 경우 잘 맞음.\n",
    "* 과정\n",
    "  * 한 번에 하나의 미니배치씩 진행하여 전체 훈련 세트를 처리하고, 이러한 과정 하나하나를 에포크(epoch)라고 함.\n",
    "    * 여러 epoch를 반복\n",
    "  * 각 미니배치는 입력층으로 전달되어 첫 번째 은닉층으로 보내진 뒤, 해당 층에 있는 모든 뉴런의 출력을 계산함. 이 결과는 다음 층으로 전달됨.\n",
    "  * 다음 층에서 다시 출력을 계산하고 그 결과는 그 다음 층으로 전달됨.\n",
    "  * 이러한 과정을 출력층의 출력을 계산할 때 까지 계속됨. 이를 정방향 계산(forward pass)이라고 함.\n",
    "  * 그 다음으로, 알고리즘이 손실 함수를 사용하여 기대하는 출력과(실제 값) 네트워크의 실제 출력(결과 값)을 비교하고 오차 측정 값을 반환함.\n",
    "  * 각 출력 연결이 이 오차에 기여하는 정도를 계산. 연쇄 법칙(chain rule)을 적용하면 이 단계를 빠르고 정확하게 수행 가능.\n",
    "  * 다시 연쇄 법칙을 적용하여 이전 층의 연결 가중치가 오차의 기여 정도에 얼마나 기여했는지 측정함. 이를 입력층에 도달할 때 까지 역방향으로 계속함.\n",
    "  * 마지막으로 알고리즘은 경사 하강법을 수행하여 방금 계산한 오차 그레이디언트를 사용해 네트워크의 모든 연결 가중치를 수정함.\n",
    "> 은닉층의 연결 가중치를 랜덤하게 초기화해야 훈련이 실패하지 않음.(ex. 모든 가중치와 편향을 0으로 초기화하면 층의 모든 뉴런이 완전히 같아지므로 역전파도 뉴런을 동일하게 바꾸어 모든 뉴런이 똑같아진 채로 남으므로 뉴런이 아무리 많아도 뉴런이 하나인 것 처럼 동작함.)\n",
    "* 활성화 함수는 계단 함수 대신 로지스틱(sigmoid) 함수 $\\sigma(z) = 1/(1+exp(-z))$로 바꿈.\n",
    "  * 계단 함수는 수평선밖에 없으니 계산할 gradient가 없음.\n",
    "  * 출력 범위는 0에서 1 사이\n",
    "> 활송화 함수는 그 외에 tanh함수 ($\\tanh(z) = 2\\sigma(2z)-1$. S자 모양이며 연속적이고 미분 가능함. 출력 범위는 -1에서 1 사이. 훈련 초기에 각 층의 출력을 원점 근처로 모으는 영향이 있으므로 수렴이 빠르게 진행되도록 도와줌), ReLU 함수($\\text{ReLU}(z) = \\text{max}(0,z)$. 연속적이지만 z=0에서 미분 가능하지 않고, z<0인 경우 도함수는 0. 잘 작동하며 계산 속도가 빠르고, 출력에 최댓값이 없어서 경사 하강법의 일부 문제점을 해결함.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from https://github.com/rickiepark/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 선형 변환을 여러 개 연결해봤자 또 다른 선형 변환이 되므로(ex. f(x)=2x+3과 g(x)=5x-1을 연결하면 f(g(x))=10x+1이 됨) 층 사이에 위와 같은 비선형성을 추가해야 복잡한 문제를 해결할 수 있음.\n",
    "* 비선형 함수가 있는 충분히 큰 심층 신경망은 이론상 어떠한 연속 함수로도 근사할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다층 퍼셉트론은 회귀 작업에 사용 가능.\n",
    "  * 값 하나를 예측하는 데 출력 뉴런이 하나만 필요함. 이 출력값이 예측된 값.\n",
    "* 다변량 회귀(동시에 여러 값을 예측하는 것)는 출력 차원마다 출력 뉴런이 하나씩 필요\n",
    "  * ex) 이미지에서 물체의 중심 위치를 파악하려면 2D좌표를 예측해야하므로 출력 뉴런 2개가 필요.\n",
    "* 보통 회귀용 다층 퍼셉트론을 만들 때는 출력 뉴런에 활성화 함수를 사용하지 않고 어떤 범위의 값도 출력되도록 함. 단, 출력이 양수여야 한다면 출력층에 ReLU함수나 softplus(ReLU의 변종. $\\text{softplus}(z)=\\log(1+\\exp(z))$. z가 음수일 때 0에 가까워지고 z가 큰 양수일수록 z에 가까움)함수를 사용. 어떠한 범위 안의 값을 예측하려면 sigmoid함수나 tanh함수를 사용하고 레이블의 스케일을 적절한 범위로 조정할 수 있음.\n",
    "* 훈련 시 사용하는 손실 함수는 일반적으로 평균 제곱 오차를 사용.\n",
    "  * 훈련 세트에 이상치가 많다면 평균 절댓값 오차를 사용할 수 있고, 또는 이 둘을 조합한 후버(Huber, 오차가 임곗값 $\\sigma$(주로 1)보다 작을 때는 이차함수. 이보다 클 때는 선형 함수. 선형 함수 부분은 평균 제곱 오차보다 이상치에 덜 민감함.) 손실을 사용할 수 있음.\n",
    "* 전형적인 회귀 MLP 구조    \n",
    "\n",
    "|하이퍼파라미터     |일반적인 값|\n",
    "|------------------|-----------|\n",
    "|입력 뉴런 수       |특성마다 하나(ex. MNIST 데이터셋은 28*28크기 이미지이므로 784개의 뉴런)|\n",
    "|은닉층 수          |문제에 따라 다름(보통 1~5)|\n",
    "|은닉층 뉴런 수     |문제에 따라 다름(보통 10~100)|\n",
    "|출력 뉴런 수       |예측 차원마다 하나|\n",
    "|은닉층의 활성화 함수|ReLU(또는 SELU 등)|\n",
    "|출력층의 활성화 함수|없음. 출력이 양수라면 ReLU, softplus. 출력을 특정 범위로 제한하면 sigmoid, tanh)|\n",
    "|손실 함수          |MSE. 이상치가 있다면 MAE, Huber|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다층 퍼셉트론은 분류 작업에 사용 가능.\n",
    "  * 이진 분류에서는 sigmoid 활성화 함수를 가진 하나의 출력 뉴런만을 필요로 하고, 출력은 0과 1사이의 실수값이 됨. 이를 양성 클래스에 대한 예측 확률로 해석가능.\n",
    "  * 음성 클래스에 대한 예측 확률은 1-양성 클래스 예측 확률\n",
    "* 다중 레이블 이진 분류 문제를 쉽게 처리 가능\n",
    "  * ex) 이메일이 스팸 메일인지/아닌지, 긴급 메일인지/아닌지를 구별하기 위해 sigmoid 활성화 함수를 가진 두 출력 뉴런이 필요. 첫 번째 뉴런은 이메일이 스팸일 확률을 출력하고, 두 번째 뉴런은 이메일이 긴급한 메일일 화률을 출력. 출력된 확률의 하이 1이 될 필요는 없고 어떠한 레이블 조합도 출력할 수 있음.\n",
    "* 각 샘플이 3개 이상의 클래스 중 한 클래스에만 속할 수 있다면(ex. 0~9까지 숫자 이미지 분류) 클래스마다 하나의 출력 뉴런이 필요하고 이를 다중 분류라고 함. 출력층은 소프트맥스 함수.\n",
    "> * 소프트맥스 함수는 샘플 $ \\mathbf{x} $ 각각에 대한 점수에 지수 함수를 적용한 뒤 정규화함(모든 지수 함수의 결과의 합으로 나눔.). $$ \\hat{p_k}=\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k=\\frac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum_{j=1}^K\\exp\\left(s_j(\\mathbf{x})\\right)} $$\n",
    ">   * $ K $ 는 클래스 수\n",
    ">   * $ \\mathbf{s}(\\mathbf{x}) $ 는 샘플 $ \\mathbf{x} $ 에 대한 각 클래스의 점수를 탐은 벡터\n",
    ">   * $ \\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k $ 는 샘플 $ \\mathbf{x} $에 대한 각 클래스의 점수가 주어졌을 때, 이 샘플이 클래스 k에 속할 추정 확률\n",
    "> * 모든 예측 확률을 0과 1사이로 만들고 이들을 더했을 때 1이 되도록 만듦(클래스가 서로 배타적일 경우 필요)\n",
    "* 손실 함수는 일반적으로 크로스 엔트로피 손실(cross-entropy loss, 또는 logloss)을 선택\n",
    "* 전형적인 분류 MLP 구조\n",
    "\n",
    "|하이퍼파라미터|이진 분류|다중 레이블 분류|다중 분류|\n",
    "|-------------|--------|--------------|--------|\n",
    "|입력층과 은닉층|회귀와 동일|회귀와 동일|회귀와 동일|\n",
    "|출력 뉴런 수  |1개        |레이블 당 1개|클래스 당 1개|\n",
    "|출력 층의 활성화 함수|sigmoid|sigmoid|softmax|\n",
    "|손실 함수|cross-entropy|cross-entropy|cross-entropy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tensorflow Playground(https://playground.tensorflow.org) : 텐서플로를 활용한 신경망 시뮬레이터\n",
    "> * ReLU사용 시 선형 경계가 만들어짐\n",
    "> * 은닉층이 줄어들면 훈련에 걸리는 시간에 차이가 나고 지역 최솟값에 갇히기도 함.\n",
    "> * 뉴런 수가 작으면 좋은 솔류선을 찾을 수 없고 파라미터가 너무 적어 훈련 세트에 과소적합됨.\n",
    "> * 신경망이 너무 크면 빠르게 훈련되고 지역 최솟값에 갇히지 않음.\n",
    ">   * 대규모 신경망은 거의 절대로 지역 최솟값에 갇히지 않음.\n",
    "> * 나선형 데이터셋+4개의 은닉층+각 은닉층 별 8개의 뉴런으로 학습하면 훈련 시간이 오래 걸리고 한번씩 긴 시간동안 평탄한 지역에 갇힘. 상위 층(오른쪽)에 있는 뉴런이 하위 층(왼쪽)에 있는 뉴런보다 도 빨리 학습되는 경향이 있고, 이를 그레이디언트 소실(vanishing gradient)라고 함. 이 문제는 더 좋은 가중치 초기화, 고급 optimizer(AdaGrad, Adam 등), 배치 정규화 등을 이용해 해결 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 종류의 신경망을 손쉽게 만들고 훈련, 평가, 실행할 수 있는 고수준 딥러닝 API.\n",
    "* API 문서 : http://keras.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API를 이용한 이미지 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST 데이터셋을 이용한 이미지 분류기\n",
    "# 숫자 손글씨 MNIST 데이터셋과 형태는 동일(28*28 크기의 흑백 이미지 70000개 및 10개의 클래스로 이루어짐), 단 손글씨 숫자가 아닌 패션 아이템을 나타내는 이미지.\n",
    "# 클래스마다 샘플이 다양해 MNIST보다 훨씬 어려운 문제.\n",
    "\n",
    "# Fashion MNIST 데이터 적재\n",
    "fashoin_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashoin_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 784크기의 1D가 아닌 28*28크기의 2D\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 픽셀 강도는 정수형\n",
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 train 데이터셋을 validation set과 train set으로 나눔.\n",
    "# 경사 하강법을 사용해야 하므로 픽셀 강도를 255.0으로 나누어 0~1사이의 범위로 조정(자동으로 실수로 변환됨)\n",
    "#   만일 새로운 데이터가 들어온다면 이에 대해서도 스케일 조정이 필요함.\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 레이블에 해당하는 아이템을 나타내기 위해 클래스 이름의 리스트 생성\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]] # 훈련 세트의 첫 번째 이미지의 클래스 출력(코트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMdUlEQVR4nO3dTYidZxnG8etJJsnkYzIzyeSjUsY2lCSFFtOhNYZKJKkLd7oQF0oW0lJcdNNmEaGlotiAC/EDddFNBAVBCbooKK4EkRgMKZhPkn5NM0lIZiaTmaSZTibt42JGKaXvfdc5mZ7r6P+3EefimfOek1x967l9nrfUWgXAz5J2XwCAj0Y5AVOUEzBFOQFTlBMwRTkBU5QTMEU5O0Ap5eullGOllJullMullD+WUj7f4u/8Synlqbt1jbj7KKe5Uspzkn4s6aCkTZIGJf1C0pfbeFn4BBT+H0K+Sim9ki5K+mat9Xcfka+Q9ANJX5v/0W8lHai1zpRS+iX9StJOSV2S/ibpW7XWkVLKS5K+LWlW0h1Jv6y1PrPobwj/Fe6c3nZJ6pb0+4b8eUmfk7RD0mckfVbSC/PZEkmHJH1ac3fbaUk/k6Ra6/OS/irpmVrrGorpiXJ6Wy9prNZ6pyH/hqTv1Vqv1lpHJX1X0j5JqrWO11oP11pv1VpvSHpJ0hc+kavGXdHV7gtAaFzSQCmlq6Ggn5I0/IH/Pjz/M5VSVkn6kaQvSeqfz3tKKUtrre8t4jXjLuHO6e2IpBlJX2nIL2nuX1v/bXD+Z5K0X9I2STtrrWsl7Z7/eZn/T75sMMed01itdbKU8qKkn5dS7kj6s+a+xPmipD2SfiPphVLKPzRXthcl/Xp+eY/m/nfm9VLKOknf+dCvvyJpy+K/CywUd05ztdYfSnpOc1/0jEq6IOkZSX+Q9H1JxyT9U9IJScfnfybNjV9WShqT9HdJf/rQr/6JpK+WUiZKKT9d1DeBBWGUApjizgmYopyAKcoJmKKcgKlslMK3Rbhrsi8fSylh/j/sI984d07AFOUETFFOwBTlBExRTsAU5QRMUU7AFFvG2uDkyZON2eHDh8O1R48eDfP33ov3UW/evDnMH3zwwcZsz5494dqdO3eG+f/xHHNBuHMCpignYIpyAqYoJ2CKcgKmKCdginICpphzLsDp06fD/MknnwzzY8eONWZ37jQd7j6nqyv+I1uyJP7nbZa/++67C167devWMN+/f3+YP/UUDz37IO6cgCnKCZiinIApygmYopyAKcoJmMqeldKxR2O+//77jVk2Eshs2rQpzMfGxsK8t7e3McuOj1y2bFmYZ6OYpUuXhnm25SwyMTER5vfee2+YX7hwYcGv3ao2H9vJ0ZhAJ6GcgCnKCZiinIApygmYopyAKcoJmOrYLWPRHFNqbZZ5/fr1MM/mnN3d3WG+atWqxmz79u3h2my7WjaPy649mnO+/fbb4dq+vr4w7+npCfPjx483ZkNDQ+HazGL+fVksflcEQBLlBGxRTsAU5QRMUU7AFOUETFFOwJTtfs7FnEvt2rUrzIeHh8M8u7Zs1jg5OdmYRY/gk6Spqakwf/3118M8m8Fu27atMcvmlNl+zOjYTUm6fft2Y5b9eY+OjoZ5JtvHmu2DbRH7OYFOQjkBU5QTMEU5AVOUEzBFOQFTlBMwZbufs9VzQg8cONCYvfbaa+HawcHBMM/Ohs1midG8L5sVPvTQQ2EezVClfM9ldG1vvfVWuDazZcuWMI/O833jjTfCtU8//XSYv/zyy2G+yHPMBeHOCZiinIApygmYopyAKcoJmKKcgCnbLWOt2r17d2M2MzMTrs3GONPT02G+YsWKMF+5cmVjduPGjXDtmjVrwnz16tVhnm0pi17//vvvD9fec889YZ59bu+8886CrkvKP/MjR46EeZuxZQzoJJQTMEU5AVOUEzBFOQFTlBMwRTkBU7ZbxjLZUYbXrl1rzKI5oyStXbs2zKNH+EnxEY9Zns3rshltq8d2Pvroo41ZNmPNHp2Ybftav359Y9bVFf9VHRsbC/Ps8YXZNsF24M4JmKKcgCnKCZiinIApygmYopyAKcoJmOrYOWf2mL5o/182r5udnQ3zbOaWzSqjGW127Gb2uzdu3Bjm2Qw22lN59erVcO3y5cvDvL+/P8yjzyWb72aPF8zmoMw5AXxslBMwRTkBU5QTMEU5AVOUEzBFOQFTHTvnzPYGRm7duhXm0axPyuek2SwymmVmZ7tme1Fv3rwZ5tl7j2a42Rwze4xedm1TU1ONWXYeb7a/99SpU2E+NDQU5u3AnRMwRTkBU5QTMEU5AVOUEzBFOQFTlBMw1bFzzmxutWRJ8z93JiYmwrUXL14M84cffjjMs3lfNMvM9ltm59L29PSEebZfNLq2bJaYzXezPZdXrlxpzAYGBsK12WeePZ9z3759Yd4O3DkBU5QTMEU5AVOUEzBFOQFTlBMw1bGjlJGRkTCPRg7Z1+611jDPRgbZlrPo6M3s2rJRSHaEZDRikqRly5aFeSS7tmyUEn1u2Ygoeyzj2bNnw9wRd07AFOUETFFOwBTlBExRTsAU5QRMUU7AVMfOOc+cORPm0ayylNLSa2ezyGxrVTRLzGaBrcq2nEUz2OzRh9n7ztZHR45ms+Xs2M6TJ0+GuSPunIApygmYopyAKcoJmKKcgCnKCZiinICpjp1znjhxIsyjWWQ0y/s4ssfoZXsmW5nBZrPCbC9qKzPebEaa5d3d3WEeHQua/e7M6OhomJ87dy7Mt27d2tLrLwR3TsAU5QRMUU7AFOUETFFOwBTlBExRTsBUx845L1++HObr1q1rzLI9k319fWGezdyyvYXRPC+bBWYz2uzc2kw0J832a2avnc1Yo7Nns/ednZmbyR4pyZwTwH9QTsAU5QRMUU7AFOUETFFOwBTlBEx17Jwz2zMZzcWyeVx2Rmo2i8zOtY3mfdl+zGyelz1fM5s1Rr8/20vayvvOXjt75mk2W8709va2tH4xcOcETFFOwBTlBExRTsAU5QRMUU7AVMeOUrKv5aOv1q9fvx6u3bBhQ5hnI4WbN2+G+cqVKxuz6enpcG32vlevXh3m2RGRrbx2tOVLkiYmJsL8gQceaMzOnj0brs1Ga/39/WGeHY25d+/eMF8M3DkBU5QTMEU5AVOUEzBFOQFTlBMwRTkBU7Zzzuwxe9n2pDVr1jRm4+Pj4dqBgYEwz2Qzt8VaK+XHfmZb0qItZ9nRmNlWuyx/7LHHGrM333wzXJtt+cpm0+fPnw/zduDOCZiinIApygmYopyAKcoJmKKcgCnKCZiynXNmRyFmeXTMYrbncePGjWF+6dKlMI8ePyhJk5OTYR7J9lS2uj763LIZbHZk6MjISJhHM9i1a9eGa4eHh8M8e2xj9kjJduDOCZiinIApygmYopyAKcoJmKKcgCnKCZiynXNmZ8tGZ79K8d7DbOa1ZcuWMJ+amgrzbB4Y5dm1ZbI9k5noc8vOpc3mnD09PWEe/Zlmr53NvbM5abT/t124cwKmKCdginICpignYIpyAqYoJ2DKdpSSPaouGxlE24+yUUh2vGR0fKQkzc7Ohnkroi1dUn5kaPa5RUeSZiOi7DjTVh6dmB3LmclGb9nn1g7cOQFTlBMwRTkBU5QTMEU5AVOUEzBFOQFTtnPObGa2fPnyMI+OgMy2B61fvz7MT58+HeatzGCzR/Rl7zuTHY0ZzXBbnbG2Mv/dvn17mL/yyithvmHDhjDP3ls7cOcETFFOwBTlBExRTsAU5QRMUU7AFOUETNnOOW/cuBHm2TGM0TzvvvvuW/BaSRofHw/z7GjNaL9otpc0m6Feu3YtzMfGxsI8OkIym2O2MnuW4sfw7du3L1ybzTmzPbjZ36d24M4JmKKcgCnKCZiinIApygmYopyAKcoJmLKdc2aPdOvt7Q3z6NzbvXv3hms3b94c5tmj7LLH+M3MzDRm2Twuk63v6+sL82g/abYfM8uzx/hFc9AnnngiXJvJzr3N/r61A3dOwBTlBExRTsAU5QRMUU7AFOUETFFOwJTtnDOb12XPeozmdY888ki49ujRo2H+6quvhnl2xur09HRjlu15zGasrc4iW3k+5+3btxf8u6X4+ZybNm0K12bn0mazZ+acAD42ygmYopyAKcoJmKKcgCnKCZiyHaVkX/lnR0hGzp8/H+aHDh0K88HBwTCfmJgI8+hr++x9ZUeGZqOY7NjOaOQQjTqkfDtaNh57/PHHwzySjXGi8ZUknTlzZsGvvVi4cwKmKCdginICpignYIpyAqYoJ2CKcgKmbOecO3bsCPOhoaEwP3XqVGOWbTfL5nEHDx4Mc3zynn322TDPtrtl2wjbgTsnYIpyAqYoJ2CKcgKmKCdginICpignYKpER0gCaB/unIApygmYopyAKcoJmKKcgCnKCZj6F8bedb0Cga1wAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.title(class_names[y_train[0]])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 은닉층으로 이루어진 분류용 다층 퍼셉트론\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>2</code> : 가장 간단한 케라스의 신경망 모델인 <code>Sequential</code>모델을 생성. 순서대로 연결된 층을 일렬로 쌓아서 구성하고, 이를 Sequential API라고 부름.\n",
    "> * <code>3</code> : 입력 이미지를 1D 배열로 반환하는 Flatten층을 첫 번째 층으로 추가. 입력 데이터 X를 받으면 X.reshape(-1, 28*28)을 계산.\n",
    ">   * 모델 파라미터를 가지지 않고 간단한 전처리만 수행\n",
    ">   * 첫 번째 층이므로 <code>input_shape</code>를 지정해야 함. 배치 크기를 제외하고 샘플의 크기만 써야 함.\n",
    ">   * 또는 <code>input_shape=[28,28]</code>로 지정된 <code>keras.layers.InputLayer</code>층을 추가할 수도 있음.\n",
    "> * <code>4, 5</code> : 각각 뉴런 300개, 100개를 가진 Dense 은닉층 추가. ReLU 활성화 함수 사용.\n",
    ">   * Dense층마다 각자의 가중치 행렬을 관리하고, 이 행렬에는 해당 층의 뉴런과 입력 사이의 모든 연결 가중치가 포함되고 뉴런마다 하나씩 있는 편향도 벡터로 관리함. \n",
    ">   * 이 층은 입력 데이터를 받으면 $h_{\\mathbf{W}, b}(\\mathbf{X})=\\phi(\\mathbf{XW}+\\mathbf{b})$를 계산.\n",
    ">   * <code>activation=\"relu\"</code>대신 <code>activation=keras.activation.relu</code>사용 가능\n",
    "> * <code>6</code> : 뉴런 10개를 가진 Dense 출력층 추가. 클래스끼리 배타적이므로 소프트맥스 활성화 함수 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 처럼 하나씩 층을 추가하지 않고 Sequential 모델을 만들 때 각 층의 list를 전달할 수도 있음\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 모든 층을 출력.\n",
    "# 각 층의 이름(미 지정 시 자동으로 생성. name 매개변수에 이름 지정 가능), 출력 크기(None은 배치 크기에 어떤 값도 가능하다는 의미), 파라미터 개수를 출력하고, 마지막에는 훈련되는 파라미터와 훈련되지 않는 파라미터를 포함하여 전체 파라미터 개수를 출력\n",
    "#   여기서는 훈련되는 파라미터만 존재.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras 모델을 이미지로 출력\n",
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Dense층은 보통 많은 파라미터를 가짐\n",
    ">   * ex) 첫 번째 은닉층은 784*300개의 연결 가중치와 300개의 편향을 가지므로 이들을 더하면 235,500개의 파라미터를 가짐. 이정도 크기의 모델은 훈련 데이터를 학습하기 충분한 유연성을 가지나, 과대적합의 위험 또한 가짐(특히 훈련 데이터가 많지 않을 때)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x14b280ba790>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x14b2debd0a0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x14b2debd640>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x14b2debda30>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 있는 층의 리스트 출력\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스로 모델의 층 선택 가능\n",
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이름으로 모델의 층 선택\n",
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층의 모든 파라미터는 get_weights()와 set_weights()를 사용해 접근 가능. Dense 층의 경우 연결 가중치와 편향이 모두 포함되어있음.\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01909524,  0.04575285, -0.02377108, ..., -0.02618043,\n",
       "        -0.02459623,  0.04009929],\n",
       "       [ 0.00255943, -0.01277498, -0.05328912, ...,  0.04174976,\n",
       "         0.00738778, -0.06766993],\n",
       "       [-0.02151511,  0.02083602,  0.05235937, ...,  0.05112466,\n",
       "         0.06476593,  0.00542057],\n",
       "       ...,\n",
       "       [ 0.03200036,  0.01330142, -0.02683904, ..., -0.07099872,\n",
       "        -0.04456173, -0.0673126 ],\n",
       "       [-0.02107367, -0.04594501,  0.01402111, ..., -0.00265069,\n",
       "        -0.04471175, -0.01878432],\n",
       "       [ 0.04602553,  0.03855234, -0.0232105 , ..., -0.00712659,\n",
       "        -0.06544694, -0.04471882]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Dense층은 연결 가중치를 무작위로 초기화하고, 편향은 0으로 초기화함.\n",
    "> * 다른 초기화 방법을 사용하고 싶다면 <code>kernel_initializer</code> (kernel은 연결 가중치 행렬의 또 다른 이름)와 <code>bias_initializer</code> 매개변수를 설정 가능\n",
    ">   * https://keras.io/initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile을이용해 사용할 손실 함수와 optimizer 지정\n",
    "# 추가적으로 훈련 및 평가 시 사용할 지표 지정 가능.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 레이블이 정수 하나로 이루어져 있고 클래스가 배타적이므로 <code>\"sparse_categorical_crossentropy\"</code> 사용\n",
    ">   * 만약, 샘플마다 클래스 별 타깃 확률을 갖고 있다면 <code>\"categorical_crossentropy\"</code> 사용\n",
    ">   * 이진 분류나 다중 레이블 이진 분류를 수행한다면 출력층은 <code>\"sigmoid\"</code>를 사용하고 <code>\"binary_crossentropy\"</code> 손실을 사용\n",
    "> * <code>\"sgd\"</code>는 기본 확률적 경사 하강법을 사용하여 모델을 훈련한다는 의미.\n",
    ">   * 보통은 학습률을 <cpde>optimizer=keras.optimizers.SGD(lr=...)</cpde>과 같이 지정함. 기본값은 0.01\n",
    "> * 모델 평가 시 지표는 정확도(<code>\"accuracy\"</code>)를 측정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.7120 - accuracy: 0.7656 - val_loss: 0.4956 - val_accuracy: 0.8304\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4868 - accuracy: 0.8290 - val_loss: 0.4876 - val_accuracy: 0.8172\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4444 - accuracy: 0.8434 - val_loss: 0.4417 - val_accuracy: 0.85306 - ac\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4158 - accuracy: 0.8543 - val_loss: 0.3916 - val_accuracy: 0.8668\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3962 - accuracy: 0.8612 - val_loss: 0.3823 - val_accuracy: 0.8712\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3801 - accuracy: 0.8659 - val_loss: 0.4068 - val_accuracy: 0.8498\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3659 - accuracy: 0.8707 - val_loss: 0.3787 - val_accuracy: 0.8688\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3544 - accuracy: 0.8745 - val_loss: 0.3492 - val_accuracy: 0.8770\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3447 - accuracy: 0.8771 - val_loss: 0.3428 - val_accuracy: 0.8798\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3353 - accuracy: 0.8811 - val_loss: 0.3450 - val_accuracy: 0.8782\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3261 - accuracy: 0.8838 - val_loss: 0.3623 - val_accuracy: 0.8698\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3173 - accuracy: 0.8869 - val_loss: 0.3321 - val_accuracy: 0.8818\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3106 - accuracy: 0.8901 - val_loss: 0.3218 - val_accuracy: 0.8832\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3027 - accuracy: 0.8919 - val_loss: 0.3277 - val_accuracy: 0.8798\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2966 - accuracy: 0.8934 - val_loss: 0.3202 - val_accuracy: 0.8862\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2904 - accuracy: 0.8947 - val_loss: 0.3157 - val_accuracy: 0.8870\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2850 - accuracy: 0.8976 - val_loss: 0.3107 - val_accuracy: 0.8904os\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2783 - accuracy: 0.8989 - val_loss: 0.3280 - val_accuracy: 0.8778\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2735 - accuracy: 0.9020 - val_loss: 0.3082 - val_accuracy: 0.8878\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2689 - accuracy: 0.9030 - val_loss: 0.3170 - val_accuracy: 0.8852\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2639 - accuracy: 0.9048 - val_loss: 0.3130 - val_accuracy: 0.8846\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2589 - accuracy: 0.9072 - val_loss: 0.3023 - val_accuracy: 0.8900\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2544 - accuracy: 0.9085 - val_loss: 0.2973 - val_accuracy: 0.8914\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2499 - accuracy: 0.9091 - val_loss: 0.2962 - val_accuracy: 0.8914\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2446 - accuracy: 0.9110 - val_loss: 0.2928 - val_accuracy: 0.8936\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2407 - accuracy: 0.9127 - val_loss: 0.3161 - val_accuracy: 0.8802\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2382 - accuracy: 0.9149 - val_loss: 0.2908 - val_accuracy: 0.8918\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2340 - accuracy: 0.9159 - val_loss: 0.3035 - val_accuracy: 0.8890\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2301 - accuracy: 0.9174 - val_loss: 0.2844 - val_accuracy: 0.8952\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2248 - accuracy: 0.9205 - val_loss: 0.3062 - val_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 매개변수로 입력 특성(<code>X_train</code>), 타깃 클래스(<code>y_train</code>), epoch 횟수(기본값은 1), 검증 세트(필수는 아님)를 전달.\n",
    "> * 한 epoch가 끝날 때 마다 검증 세트를 이용해 손실 및 추가적인 측정 지표를 계산함.\n",
    ">   * 훈련 세트 성능이 검증 세트보다 훨씬 높다면 과대적합되었다고 볼 수 있음.  (또는 훈련 세트와 검증 세트간의 데이터가 올바르지 않는 경우 등의 버그가 생긴 것.)\n",
    "> * 여기서는 검증 정확도가 88.8%, 훈련 정확도가 91.8%이므로 둘의 차이가 크지 않아 과대적합이 많이 일어나지 않았다는 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>validation_data</code> 대신 keras에서 직접 검증에 사용할 훈련 세트의 비율을 지정할 수 있음.\n",
    "  * <code>validation_split=0.1</code>과 같이 사용(섞기 전의 데이터의 마지막 10%를 검증 세트로 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 클래스의 출현 빈도가 다르다면 <code>fit()</code>호출 시 <code>class_weight</code>를 지정하는 것이 좋음. 적게 등장하는 클래스에는 높은 가중치를 부여하고 많이 등장하는 클래스는 낮은 가중치를 부여함.\n",
    "  * 손실 계산 시 이 가중치를 사용\n",
    "* 샘플 별로 가중치를 부여하고 싶다면 <code>sample_weight</code>사용.\n",
    "  * 예를 들어, 샘플의 레이블링이 전문가에 의해 이루어졌는지 크라우드소싱 플랫폼을 이용하여 이루어졌는지에 따라 앞선 레이블링에 더 높은 가중치를 부여할 수 있음.\n",
    "* class_weight와 sample_weight 둘 다 사용한다면 두 값을 곱하여 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit()에 의해 반환된 History 객체에는 훈련 파라미터(<code>history.params</code>), 수행된 epoch 리스트가 포함(<code>history.epoch</code>).\n",
    "* history.history에는 epoch가 끝날 때 마다 훈련 세트와 검증 세트에 대한 손실과 측정한 지표를 담은 dict가 있음. 이를 이용해 pandas Dataframe을 만들고 학습 곡선을 그릴 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # 수직축의 범위 설정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 훈련 정확도와 검증 정확도가 꾸준히 상승하고 훈련 손실과 검증 손실은 감소함.\n",
    "> * 검증 곡선이 훈련 곡선과 가까움. 즉, 크게 과대적합되지 않았다는 의미.\n",
    "> * 초기에 검증 세트의 성능이 더 좋아보이는 것은 검증 손실은 epoch가 끝난 후에 계산되고 훈련 손실은 epoch 진행 중에 계산되므로 훈련 곡선은 epoch의 절반만큼 왼쪽으로 옮겨서 봐야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일반적으로 충분히 오래 훈련하면 훈련 세트의 성능이 검증 세트의 성능보다 좋아짐. 검증 손실이 계속해서 감소한다면 모델이 아직 수렴되지 않았다고 볼 수 있음.\n",
    "  * keras에서 fit()을 다시 호출하면 중지되었던 곳에서부터 다시 훈련을 이어갈 수 있음.\n",
    "* 모델 성능이 좋지 않다면 하이퍼파라미터 튜닝이 필요\n",
    "  * 학습률, optimizer 순으로 확인.\n",
    "  * 다른 하이퍼파라미터를 바꾸게 되면 학습률을 다시 튜닝해야 함.\n",
    "  * 그 외에 층 개수, 각 충의 뉴런 개수, 은닉층의 활성화 함수와 같은 모델 하이퍼파라미터 튜닝을 시도할 수 있음.\n",
    "  * <code>batch_size</code>매개변수를 이용해 배치 크기 지정 가능(기본값은 32)\n",
    "* <code>evaluate()</code>를 이용해 테스트 세트로 모델 평가 가능\n",
    "  * 일반적으로는 검증 세트보다 성능이 낮게 나옴(하이퍼파라미터 튜닝이 검증 세트를 기반으로 했기 때문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3421423137187958, 0.8780999779701233]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>predict()</code>메서드를 이용해 새로운 샘플에 대한 예측 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]  # 테스트 세트의 처음 3개 샘플을 새로운 샘플로 사용\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 각 이미지에 대한 클래스 추정 확률을 반환.\n",
    "> * 첫 번째 이미지의 경우 클래스 9(Ankle boot)의 확률이 제일 높음. 그 다음으로는 클래스 7(Sneaker)의 확률이 높고 그 외의 확률은 매우 낮음(즉, 거의 확실히 신발 종류라는 의미)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# 클래스 확률 대신 가장 높은 확률을 가진 클래스만 반환\n",
    "y_pred = model.predict_classes(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_test[:3]]   # 예측값이 실제 레이블과 동일함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API를 이용한 회귀용 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캘리포니아 주택 가격 dataset 사용\n",
    "# 누락된 데이터는 없음.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "# 모든 특성의 스케일 조정\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(housing.data, columns = housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0401 - val_loss: 0.6503\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5519 - val_loss: 0.5291\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5573 - val_loss: 0.5189\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4817 - val_loss: 0.5025\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4616 - val_loss: 0.4757\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4455 - val_loss: 0.4541\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4539\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4362\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4290\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4151 - val_loss: 0.4233\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4222\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4046 - val_loss: 0.4214\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4118\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3967 - val_loss: 0.4088\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4058\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3900 - val_loss: 0.4021\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3874 - val_loss: 0.3981\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4009\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3820 - val_loss: 0.3953\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3798 - val_loss: 0.3948\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3747\n"
     ]
    }
   ],
   "source": [
    "# 회귀용 MLP는 분류에서 사용했던 것과 거의 동일.\n",
    "# 하지만 출력층은 활성화 함수가 없는 하나의 뉴런(하나의 값을 예측하기 때문), 손실 함수는 MSE.\n",
    "# 데이터셋에 잡음이 많으므로 과대적합을 막기 위해 뉴런 수가 적은 은닉층 하나만 사용.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # 수직축의 범위 설정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1304464 ],\n",
       "       [0.94138616],\n",
       "       [1.4678792 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.159, 0.934, 1.788])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수형 API로 복잡한 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력과 출력이 여러 개 이거나 더 복잡한 네트워크를 갖는 신경망을 만들려면 함수형 API사용.\n",
    "* 순차적이지 않은 신경망의 예로는 Wide&Deep 신경망이 있음.\n",
    "  * 입력의 일부 또는 전체가 출력층에 바로 연결됨. 이를 사용하면 신경망이 깊게 쌓은 층을 사용한 복잡한 패턴과 짧은 경로를 사용한 간단한 규칙을 모두 학습할 수 있음.\n",
    "  * 일반적인 MLP는 네트워크에 있는 층 전체에 모든 데이터를 통과시키므로 데이터에 있는 간단한 패턴이 왜곡될 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])            # 1\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)     # 2\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)    # 3\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])          # 4\n",
    "output = keras.layers.Dense(1)(concat)                          # 5\n",
    "model = keras.Model(inputs=[input_], outputs=[output])          # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 1 : Input 객체 생성. <code>shape</code>, <code>dtype</code>을 포함해 모델의 입력을 정의. 한 모델이 여러 개의 입력을 가질 수 있음.\n",
    "> * 2 : 30개의 뉴런과 활성화 함수로 ReLU를 사용하는 Dense층을 만듦. 만들 때 입력과 함께 함수처럼 호출됨.\n",
    ">   * python에서 객체를 함수처럼 호출하면 __call__()이 실행되고, 여기서 build()를 호출해 층의 가중치를 생성함.\n",
    ">   * keras에게 층이 연결될 방법을 알려주고, 데이터는 처리하지 않음.\n",
    "> * 3 : 두 번째 Dense 은닉층 생성. 첫 번째 은닉층의 출력을 전달함.\n",
    "> * 4 : Concatenate층을 만들어 두 번째 은닉층의 출력과 입력층을 연결.\n",
    ">   * <code>axis</code>매개변수를 이용해 연결할 기준 차원을 지정할 수 있음(기본값은 -1, 즉 마지막 차원)\n",
    ">   * <code>keras.layers.concatenate()</code> 함수를 이용해 주어진 입력으로 바로 Concatenate층을 만들 수 있음.\n",
    "> * 5 : 활성화 함수가 없는 하나의 뉴런으로 이루어진 출력층을 만듦. Concatenate층이 만든 결과를 사용해 호출\n",
    "> * 6 : 사용할 입력과 출력을 지정해 keras Model을 만듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 만들어진 모델은 동일하게 컴파일, 훈련, 평가, 예측이 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8547 - val_loss: 0.6471\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7453 - val_loss: 0.8266\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.4318 - val_loss: 17.6417\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
      "162/162 [==============================] - 0s 1ms/step - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.159, 0.934, 1.788])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일부 특성은 짧은 경로(입력-출력)로 전달하고, 다른 특성들은 깊은 경로(입력-은닉1-은닉2-출력)로 전달한다면 여러 입력을 사용하면 됨.\n",
    "  * 짧은 경로와 깊은 경로에 전달되는 특성은 중복될 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 index 0~4는 짧은 경로로, 특성 index 2~7은 깊은 경로로 보냄\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 복잡한 모델은 <code>name</code>파라미터를 이용해 이름을 붙여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit()호출 시 입력은 하나의 입력 행렬 <code>X_train</code>대신 입력마다 하나씩 행렬의 튜플 <code>(X_train_A, X_train_B)</code>를 전달해야 함. validation set과, <code>evaluate(), predict()</code> 호출 시의 X_test와 X_new도 동일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0807 - val_loss: 0.9482\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7955 - val_loss: 0.7735\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7077 - val_loss: 0.7152\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6610 - val_loss: 0.6793\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6314 - val_loss: 0.6462\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6042 - val_loss: 0.6222\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5841 - val_loss: 0.6034\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5678 - val_loss: 0.5876\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5546 - val_loss: 0.5727\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5423 - val_loss: 0.5617\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5321 - val_loss: 0.5549\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5253 - val_loss: 0.5432\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5166 - val_loss: 0.5363\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5104 - val_loss: 0.5294\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5053 - val_loss: 0.5240\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5005 - val_loss: 0.5188\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4967 - val_loss: 0.5152\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4930 - val_loss: 0.5098\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4889 - val_loss: 0.5069\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4856 - val_loss: 0.5088\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4748\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1986513],\n",
       "       [1.4334967],\n",
       "       [1.4815218]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.159, 0.934, 1.788])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여러 개의 출력을 가지는 모델도 생성 가능.\n",
    "  * 여러 출력이 필요한 경우들:\n",
    "  * ex) 그림에 있는 주요 물체를 분류하고 그 위치를 알아야 할 때 회귀 작업(물체 중심의 좌표, 너비, 높이)과 분류 작업을 함께 해야 함\n",
    "  * 동일한 데이터에 독립적인 여러 작업을 수행할 때.\n",
    "    * 작업마다 새로운 신경망을 훈련하는 것 보다 작업마다 하나의 출력을 가진 단일 신경망을 훈련하는 것이 여러 작업에 걸쳐 유용한 특성을 학습할 수 있기 때문에 보통 더 나은 결과가 나옴.(ex. 얼굴 사진으로 다중 작업 분류. 한 출력은 얼굴 표정을 분류하고 다른 출력은 안경을 썼는 지 분류하는 등.)\n",
    "    * 규제 기법으로 사용할 때. (ex. 신경망 구조 안에 보조 출력을 사용해 하위 네트워크가 나머지 네트워크에 의존하지 않고 그 자체로 유용한 것을 학습하는 지 확인하는 데 사용.)\n",
    "* 각 출력별로 자신만의 손실 함수가 필요하므로 모델 compile시 손실의 리스트를 전달해야 함.\n",
    "  * 하나만 전달하면 모든 출력의 손실 함수가 동일하다고 가정하게 됨.\n",
    "  * 리스트로 전달된 손실을 모두 더하여 최종 손실을 구해 훈련에 사용함.\n",
    "  * 보조 출력은 규제로 사용하므로 주 출력의 손실에 더 많은 가중치를 부여해야 함. <code>loss_weight</code> 매개변수로 지정가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보조 출력 추가\n",
    "\n",
    "# 출력층 앞까지는 이전과 동일.\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2) # 보조 출력층\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit() 출력층이 여러개일 때 각 출력에 대한 레이블을 제공해야 함. 여기서는 두 출력이 같은 것을 예측해야 하므로 <code>(y_train, y_train)</code>을 사용.\n",
    "  * y_valid와 y_test도 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3355 - main_output_loss: 1.2775 - aux_output_loss: 1.8572 - val_loss: 0.6792 - val_main_output_loss: 0.6047 - val_aux_output_loss: 1.3499\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6406 - main_output_loss: 0.5799 - aux_output_loss: 1.1871 - val_loss: 0.5849 - val_main_output_loss: 0.5308 - val_aux_output_loss: 1.0717\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5393 - main_output_loss: 0.4925 - aux_output_loss: 0.9606 - val_loss: 0.5771 - val_main_output_loss: 0.5373 - val_aux_output_loss: 0.9356\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5064 - main_output_loss: 0.4694 - aux_output_loss: 0.8389 - val_loss: 0.5048 - val_main_output_loss: 0.4686 - val_aux_output_loss: 0.8310\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4939 - main_output_loss: 0.4645 - aux_output_loss: 0.7592 - val_loss: 0.4954 - val_main_output_loss: 0.4650 - val_aux_output_loss: 0.7685\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4824 - main_output_loss: 0.4579 - aux_output_loss: 0.7035 - val_loss: 0.4709 - val_main_output_loss: 0.4440 - val_aux_output_loss: 0.7136\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4545 - main_output_loss: 0.4313 - aux_output_loss: 0.6629 - val_loss: 0.4582 - val_main_output_loss: 0.4339 - val_aux_output_loss: 0.6771\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4412 - main_output_loss: 0.4200 - aux_output_loss: 0.6329 - val_loss: 0.4502 - val_main_output_loss: 0.4286 - val_aux_output_loss: 0.6446\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4336 - main_output_loss: 0.4140 - aux_output_loss: 0.6102 - val_loss: 0.4344 - val_main_output_loss: 0.4135 - val_aux_output_loss: 0.6223\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4211 - main_output_loss: 0.4024 - aux_output_loss: 0.5892 - val_loss: 0.4466 - val_main_output_loss: 0.4277 - val_aux_output_loss: 0.6170\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4127 - main_output_loss: 0.3947 - aux_output_loss: 0.5743 - val_loss: 0.4189 - val_main_output_loss: 0.3993 - val_aux_output_loss: 0.5955\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4057 - main_output_loss: 0.3887 - aux_output_loss: 0.5590 - val_loss: 0.4149 - val_main_output_loss: 0.3965 - val_aux_output_loss: 0.5801\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4006 - main_output_loss: 0.3844 - aux_output_loss: 0.5461 - val_loss: 0.4038 - val_main_output_loss: 0.3863 - val_aux_output_loss: 0.5615\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3974 - main_output_loss: 0.3822 - aux_output_loss: 0.5348 - val_loss: 0.4024 - val_main_output_loss: 0.3853 - val_aux_output_loss: 0.5557\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4667 - main_output_loss: 0.4565 - aux_output_loss: 0.5592 - val_loss: 0.4167 - val_main_output_loss: 0.3988 - val_aux_output_loss: 0.5782\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4110 - main_output_loss: 0.3976 - aux_output_loss: 0.5323 - val_loss: 0.4093 - val_main_output_loss: 0.3938 - val_aux_output_loss: 0.5485\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3847 - main_output_loss: 0.3708 - aux_output_loss: 0.5095 - val_loss: 0.4175 - val_main_output_loss: 0.4030 - val_aux_output_loss: 0.5478\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3856 - main_output_loss: 0.3725 - aux_output_loss: 0.5036 - val_loss: 0.3899 - val_main_output_loss: 0.3752 - val_aux_output_loss: 0.5223\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3727 - main_output_loss: 0.3594 - aux_output_loss: 0.4916 - val_loss: 0.3831 - val_main_output_loss: 0.3691 - val_aux_output_loss: 0.5088\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3660 - main_output_loss: 0.3530 - aux_output_loss: 0.4832 - val_loss: 0.3954 - val_main_output_loss: 0.3823 - val_aux_output_loss: 0.5134\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3831 - main_output_loss: 0.3699 - aux_output_loss: 0.5016\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014B2D4B20D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])    # 모델 평가 시 각 출력층에 대한 손실과 총 손실을 반환\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])                                 # 예측 시 각 출력층에 대한 예측을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.383091002702713, 0.3699224293231964, 0.5016090273857117)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4.0464153],\n",
       "        [1.3733103],\n",
       "        [1.3480754]], dtype=float32),\n",
       " array([[2.9403977],\n",
       "        [1.5696578],\n",
       "        [1.0235612]], dtype=float32))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 서브클래싱 API로 동적 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequential, 함수형 API는 사용할 층과 연결 방식을 먼저 정의해야 하고, 그 다음 모델에 데이터를 주입하여 훈련이나 추론을 시작할 수 있음.\n",
    "  * 모델 저장, 복사, 공유가 쉽고 모델 구조의 출력 및 분석이 좋음.\n",
    "  * 모델에 데이터를 넣기 전에 에러를 일찍 발견할 수 있음\n",
    "  * 전체 모델이 층으로 구성된 정적 그래프이므로 디버깅이 쉬움.\n",
    "* 단, 어떤 모델은 반복문을 포함하고 다양한 크기를 다루어야 하며 조건문을 가지는 등 여러 동적 구조를 필요로 하기도 함.\n",
    "* 명령형 프로그래밍 스타일이 필요할 때 서브클래싱 API(Subclassing API) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>Model</code>클래스를 상속한 뒤 생성자 내에서 필요한 층을 만듦.\n",
    "* <code>call()</code>메서드 안에 수행하려는 연산 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 함수형 API로 만든 모델과 동일한 기능을 수행.\n",
    "\n",
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # 표준 매개변수 처리\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 함수형 API와 달리 Input클래스의 객체를 만들 필요가 없음. 대신 call() 메서드의 inputs매개변수를 사용\n",
    "> * 생성자에는 층 구성, call()에는 정방향 계산을 분리.\n",
    "> * call()내에는 원하는 어떠한 계산도 사용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유연성이 높으나, 모델 구조가 call()안에 숨겨져 있으므로 keras가 쉽게 이를 분석할 수 없음. 즉, 모델 저장과 복사가 불가능.\n",
    "* <code>summary()</code>호출 시 층의 목록만 나열되고 층 간 연결 정보를 알 수 없음.\n",
    "* keras가 타입과 크기를 미리 확인할 수 없어 실수가 발생하기 쉬움.\n",
    "  * 보통 높은 유연성이 필요없다면 sequential API나 함수형 API를 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential API와 함수형 API는 아래와 같은 방법으로 저장.\n",
    "model.save(\"my_keras_model.h5\")\n",
    "# 불러오기\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 서브클래싱 API는 <code>save_weights()</code>와 <code>load_weights()</code>를 이용해 모델 파라미터를 저장 및 복원할 수 있고 그 외에는 모두 수동으로 저장하고 복구해야 함. 또는 pickle모듈을 사용하여 모델 객체를 직렬화할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련이 오래 지속되는 경우, 문제가 생겨 훈련과 관련된 정보를 잃을 수도 있으므로, 훈련 도중 체크포인트를 지정하여야 함.\n",
    "* <code>fit()</code>의 <code>callback</code> 매개변수를 이용해 훈련의 시작 또는 끝에 호출할 객체의 리스트를 지정할 수 있음.\n",
    "  * 또는 epoch의 시작이나 끝, 각 배치 처리 전후에 호출할 수도 있음.\n",
    "  * ModelCheckpoint는 매 epoch의 끝에 호출됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.7816\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5571\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4867\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4568\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4374\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4245A: 0s - loss: 0.421\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4153\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4086\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4028\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3981\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 중 검증 세트를 사용한다면 <code>ModelCheckpoint</code>의 매개변수에 <code>save_best_only=True</code>로 지정하면 최상의 검증 세트 점수에서만 모델을 저장하게 됨. 즉, 훈련이 오래 되어 훈련 세트에 과대적합될 걱정을 하지 않아도 됨.\n",
    "  * 훈련이 끝난 후 저장된 모델을 복원하기만 하면 해당 모델이 검증 세트에서 최상의 점수를 낸 모델.(조기 종료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3935 - val_loss: 0.4031\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3908 - val_loss: 0.4083\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3865 - val_loss: 0.3993\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3832 - val_loss: 0.4013\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3809 - val_loss: 0.3934\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3778 - val_loss: 0.3912\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3750 - val_loss: 0.3913\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3903\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3922\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3832\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3657 - val_loss: 0.3789\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3639 - val_loss: 0.3781\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3785\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3791\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.3732\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3683\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3556 - val_loss: 0.3684\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3686\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3519 - val_loss: 0.3731\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3671\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3634\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3676\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3625\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3651\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.3642\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.3572\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3601\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 956us/step - loss: 0.3404 - val_loss: 0.3569\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3573\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3566\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.3566\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3537\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3330 - val_loss: 0.3584\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3479\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3324 - val_loss: 0.3518\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.3483\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.3525\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3474\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3507\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3525\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3261 - val_loss: 0.3470\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3468\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.3457\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3468\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3450\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3430\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3428\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3379\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3442\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3423\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3452\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3199 - val_loss: 0.3367\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.3408\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3356\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3405\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3168 - val_loss: 0.3372\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3382\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3168 - val_loss: 0.3404\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3353\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.3386\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3144 - val_loss: 0.3343\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3130 - val_loss: 0.3432\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3158 - val_loss: 0.3397\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3140 - val_loss: 0.3342\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3132 - val_loss: 0.3314\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3117 - val_loss: 0.3411\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3130 - val_loss: 0.3335\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3114 - val_loss: 0.3397\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3107 - val_loss: 0.3295\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3107 - val_loss: 0.3321\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3378\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3313\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3131 - val_loss: 0.3324\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3112 - val_loss: 0.3336\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3277\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3091 - val_loss: 0.3289\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3087 - val_loss: 0.3352\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3111 - val_loss: 0.3283\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3132 - val_loss: 0.3318\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3283\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3084 - val_loss: 0.3327\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3307\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3087 - val_loss: 0.3294\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.3225\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3082 - val_loss: 0.3343\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.3311\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3256\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3270\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3072 - val_loss: 0.3259\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.3069 - val_loss: 0.3307\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3074 - val_loss: 0.3353\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3061 - val_loss: 0.3220\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.3219\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3058 - val_loss: 0.3335\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3045 - val_loss: 0.3244\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3253\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3045 - val_loss: 0.3207\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3051 - val_loss: 0.3286\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3319\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3069 - val_loss: 0.3231\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3053 - val_loss: 0.3218\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3043 - val_loss: 0.3306\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3044 - val_loss: 0.3248\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3046 - val_loss: 0.3200\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3045 - val_loss: 0.3220\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3048 - val_loss: 0.3210\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3218\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3027 - val_loss: 0.3248\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3033 - val_loss: 0.3216\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3365\n"
     ]
    }
   ],
   "source": [
    "# 조기 종료의 또 다른 구현\n",
    "# EarlyStopping 콜백을 사용. patience 매개변수로 지정된 수 만큼의 에포크 동안 검증 세트에 대한 점수가 향상되지 않으면 훈련을 멈춤.\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * 선택적으로 최상의 모델을 복원할 수 있음(<code>restore_best_weight=True</code>)\n",
    "> * 컴퓨터가 문제를 일으키는 것을 대비해 체크포인트 저장 콜백과 시간과 자원을 아끼기 위해 조기 종료 콜백을 함께 사용할 수 있음.\n",
    "> * EarlyStopping 사용 시 에포크 크기가 커도 상관없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - ETA: 0s - loss: 0.303 - 0s 1ms/step - loss: 0.3036 - val_loss: 0.3220\n",
      "\n",
      "val/train: 1.06\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.3261\n",
      "\n",
      "val/train: 1.08\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.3234\n",
      "\n",
      "val/train: 1.07\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3034 - val_loss: 0.3223\n",
      "\n",
      "val/train: 1.06\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3026 - val_loss: 0.3199\n",
      "\n",
      "val/train: 1.06\n"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 콜백. 더 많은 제어를 원할 때 사용\n",
    "\n",
    "# ex) 훈련하는 동안 검증 손실과 훈련 손실의 비율을 출력(과대적합 감지용)\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "printvaltrainratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid), callbacks=[printvaltrainratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), on_batch_end()</code> 사용 가능\n",
    "> * 검증과 예측 단계에서도 콜백 사용 가능\n",
    "> * 평가에 사용하려면 <code>on_test_begin(), on_test_end(), on_test_batch_begin(), on_test_batch_end()</code>구현 필요. <code>evaluate()</code>사용 시 호출됨.\n",
    "> * 예측에 사용하려면 <code>on_predict_begin(), on_predict_end(), on_predict_batch_begin(), on_predict_batch_end()</code>구현 필요. <code>predict()</code>사용 시 호출됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서플로우 인터렉티브 시각화 도구\n",
    "* 훈련 동안 학습 곡선을 그리거나 여러 실행 간의 학습 곡선을 비교하고 계산 그래프 시각화와 훈련 통계 분석을 수행할 수 있음. 또한 모델이 생성한 이미지 확인이나 3D에 투영된 다차원 데이터를 시각화하여 자동으로 클러스터링 하는 등의 여러 기능도 제공.\n",
    "* 사용하려면 이벤트 파일이라는 특별한 이진 로그 파일에 시각화하려는 데이터를 출력해야 함.\n",
    "  * 각 이진 데이터 코드를 summary라고 함.\n",
    "  * 텐서보드 서버에서 로그 디렉터리를 모니터링하고, 변경사항을 확인하여 그래프를 업데이트함.\n",
    "* 학습 곡선의 실시간 시각화가 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 2.2274 - val_loss: 0.9292\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8869 - val_loss: 0.7560\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7807 - val_loss: 0.6919\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7253 - val_loss: 0.6486\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6852 - val_loss: 0.6149\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6533 - val_loss: 0.5887\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6281 - val_loss: 0.5669\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6072 - val_loss: 0.5490\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5902 - val_loss: 0.5343\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5753 - val_loss: 0.5209\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5619 - val_loss: 0.5098\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5507 - val_loss: 0.5003\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5405 - val_loss: 0.4915\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5326 - val_loss: 0.4842\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5250 - val_loss: 0.4776\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5173 - val_loss: 0.4733\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5130 - val_loss: 0.4681\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5074 - val_loss: 0.4625\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5010 - val_loss: 0.4579\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4978 - val_loss: 0.4550\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)    # my_logs위치에 로그 디렉터리를 생성. 훈련하는 동안 이벤트 파일을 만들고 summary를 기록.\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "D:.\n",
    "└─run_2022_05_07-00_58_16\n",
    "    ├─train\n",
    "    │  │  events.out.tfevents.1651852755.DESKTOP.18848.237644.v2\n",
    "    │  │  events.out.tfevents.1651852755.DESKTOP.profile-empty\n",
    "    │  │  events.out.tfevents.1651852834.DESKTOP.18848.253307.v2\n",
    "    │  │\n",
    "    │  └─plugins\n",
    "    │      └─profile\n",
    "    │          ├─2022_05_06_15_59_15\n",
    "    │          │      DESKTOP.input_pipeline.pb\n",
    "    │          │      DESKTOP.kernel_stats.pb\n",
    "    │          │      DESKTOP.memory_profile.json.gz\n",
    "    │          │      [...]\n",
    "    │          └─2022_05_06_16_00_35\n",
    "    │                  DESKTOP.input_pipeline.pb\n",
    "    │                  DESKTOP.kernel_stats.pb\n",
    "    │                  DESKTOP.memory_profile.json.gz\n",
    "    │                  [...]\n",
    "    └─validation\n",
    "            events.out.tfevents.1651852756.DESKTOP.18848.238100.v2\n",
    "            events.out.tfevents.1651852835.DESKTOP.18848.253763.v2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서보드는 profiling trace 파일을 사용해 전체 디바이스에 걸쳐 모델의 각 부분에서 시간이 얼마나 걸렸는지 보여주므로 병목 지점을 찾는 데 도움이 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ```bash\n",
    "> > tensorboard --logdir=./my_logs --port=6006\n",
    "> ```\n",
    "> ```bash\n",
    "> Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "> TensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 터미널에서 명령어 실행 후, 아래 주소를 통해 접속 가능\n",
    "* http://localhost:6006\n",
    "* 또는, 주피터 안에서 바로 사용할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SCALARS 탭에서 학습 곡선 확인 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <code>tf.summary</code>라는 저수준 API 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_file_writer()를 이용해 시각화할 수 있는 스칼라, 히스토그램, 이미지, 오디오, 텍스트 기록\n",
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2)*step/100  # 랜덤 데이터\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3)   # 32*32 RGB 이미지\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000)/48000*2*np.pi*step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여러 하이퍼파라미터를 시도하는 k-fold cross validation 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 운닉층 개수, 뉴런 개수로 단방향 회귀 Sequential model 생성 및 반환.\n",
    "# 주어진 학습률을 사용하는 SGD optimizer로 모델 컴파일\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KerasRegressor 객체 생성\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>KerasRegressor</code> : <code>build_model()</code>로 만들어진 모델을 감싸는 wrapper class.\n",
    "> * 넘겨준 build_model에 파라미터를 넘겨주지 않았으므로 기본값 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 만들어진 KerasRegressor객체는 일반적인 사이킷런 회귀 추정기처럼 사용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.4839 - val_loss: 0.7527\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6995 - val_loss: 0.6600\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6075 - val_loss: 0.6025\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5662 - val_loss: 0.5677\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5406 - val_loss: 0.5603\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5307\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 0.5286\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4967 - val_loss: 0.5096\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4901 - val_loss: 0.5068\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4806 - val_loss: 0.4952\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4877\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4809 - val_loss: 0.4805\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4655 - val_loss: 0.4750\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.4714\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4561 - val_loss: 0.4687\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4522 - val_loss: 0.4666\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4504 - val_loss: 0.4653\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4473 - val_loss: 0.4566\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4545\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4507\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4389 - val_loss: 0.4482\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4360 - val_loss: 0.4452\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4459\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4410\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.4392\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4370\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4347\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4350\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4208 - val_loss: 0.4305\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.4282\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.4281\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4249\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.4238\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4225\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4192\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.4205\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4193\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.4153\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.4135\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4131\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.4132\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.4105\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.4075\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3958 - val_loss: 0.4076\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.4043\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.4059\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.4045\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4038\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.4015\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3903 - val_loss: 0.4099\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.4249\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.4102\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4105\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4058\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.4000\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.3989\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.4015\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.3927\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.3909\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3811 - val_loss: 0.3900\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3897\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 912us/step - loss: 0.3767 - val_loss: 0.3874\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.3876\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3885\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3840\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3839\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.3833\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.3841\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.3818\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3833\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3823\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3827\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.3822\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3741 - val_loss: 0.3845\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.3698 - val_loss: 0.3789\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3658 - val_loss: 0.3780\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3648 - val_loss: 0.3778\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4247\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.3777\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3757\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3785\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3746\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3618 - val_loss: 0.3751\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3740\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3736\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.3745\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.4366\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.3718\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.3740\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3723\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.4360\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.3925\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3845\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.3737\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.3696\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3686\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3753\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.3696\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3694\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.3692\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3567\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014B2FBBEAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * fit()로 지정한 매개변수가 keras 모델로 전달됨.\n",
    "> * scikit은 손실이 아니라 점수를 계산하므로 높을수록 좋음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여러 모델을 훈련하고 검증 세트에서 최선의 모델을 선택해야 하는데, 하이퍼파라미터가 많으므로 RandomizedSearchCV를 사용하는 것이 좋음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "182/182 [==============================] - 1s 2ms/step - loss: 1.2045 - val_loss: 0.9341\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.9176\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5509 - val_loss: 0.9490\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.9944\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5060 - val_loss: 1.0252\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5024 - val_loss: 1.0607\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5032 - val_loss: 1.1656\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5103 - val_loss: 1.1462\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5072 - val_loss: 1.1548\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5046 - val_loss: 1.1971\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 1.2090\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5005 - val_loss: 1.2360\n",
      "182/182 [==============================] - ETA: 0s - loss: 3.633 - 0s 840us/step - loss: 2.9027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "182/182 [==============================] - 1s 2ms/step - loss: 4.1357 - val_loss: 3.7441\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 59.5545 - val_loss: 147.5015\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 195.0778 - val_loss: 6888.6147\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 243277.5781 - val_loss: 292793.0000\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 4597635.5000 - val_loss: 11744786.0000\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 16941984.0000 - val_loss: 503340352.0000\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 7982097920.0000 - val_loss: 21002217472.0000\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 334162755584.0000 - val_loss: 928532135936.0000\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 32010064101376.0000 - val_loss: 36748977504256.0000\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1326164031307776.0000 - val_loss: 1387695477620736.0000\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1858226324766720.0000 - val_loss: 60246953664970752.0000\n",
      "182/182 [==============================] - 0s 801us/step - loss: 1259902886477824.0000\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 5.3689 - val_loss: 4.9887\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 4.2248 - val_loss: 3.9731\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 3.3572 - val_loss: 3.1989\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.6967 - val_loss: 2.6088\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.1942 - val_loss: 2.1592\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.8118 - val_loss: 1.8146\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.5194 - val_loss: 1.5519\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.2968 - val_loss: 1.3499\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.1258 - val_loss: 1.1949\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.9947 - val_loss: 1.0756\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8941 - val_loss: 0.9836\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8166 - val_loss: 0.9123\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7568 - val_loss: 0.8575\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7108 - val_loss: 0.8150\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6752 - val_loss: 0.7819\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6475 - val_loss: 0.7561\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6259 - val_loss: 0.7359\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6089 - val_loss: 0.7200\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5956 - val_loss: 0.7076\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.6978\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.6901\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5701 - val_loss: 0.6840\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5647 - val_loss: 0.6792\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5604 - val_loss: 0.6753\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5568 - val_loss: 0.6721\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5539 - val_loss: 0.6697\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.6677\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.6662\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5476 - val_loss: 0.6649\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.6639\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5446 - val_loss: 0.6631\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 0.6625\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5424 - val_loss: 0.6620\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.6616\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5405 - val_loss: 0.6613\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.6611\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5389 - val_loss: 0.6610\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.6610\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5374 - val_loss: 0.6609\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.6609\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5361 - val_loss: 0.6610\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.6611\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5349 - val_loss: 0.6612\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.6614\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.6615\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5332 - val_loss: 0.6617\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5327 - val_loss: 0.6620\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.6622\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.6624\n",
      "182/182 [==============================] - 0s 771us/step - loss: 0.8755\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 7.1050 - val_loss: 5.7485\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 5.1894 - val_loss: 4.3126\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 3.8897 - val_loss: 3.3068\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 2.9841 - val_loss: 2.5910\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 2.3415 - val_loss: 2.0762\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 1.8823 - val_loss: 1.7012\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.5490 - val_loss: 1.4243\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.3035 - val_loss: 1.2190\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.1216 - val_loss: 1.0667\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.9871 - val_loss: 0.9523\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8860 - val_loss: 0.8662\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.8101 - val_loss: 0.8014\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7531 - val_loss: 0.7523\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7098 - val_loss: 0.7151\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6771 - val_loss: 0.6866\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6521 - val_loss: 0.6650\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6331 - val_loss: 0.6485\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6185 - val_loss: 0.6357\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6072 - val_loss: 0.6258\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.6181\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.6121\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5865 - val_loss: 0.6073\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5823 - val_loss: 0.6035\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5790 - val_loss: 0.6004\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5763 - val_loss: 0.5979\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5958\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5941\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.5925\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5693 - val_loss: 0.5913\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5681 - val_loss: 0.5901\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5890\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5661 - val_loss: 0.5881\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5652 - val_loss: 0.5872\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5644 - val_loss: 0.5864\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5637 - val_loss: 0.5856\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5630 - val_loss: 0.5849\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5624 - val_loss: 0.5842\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5617 - val_loss: 0.5835\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5611 - val_loss: 0.5829\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5605 - val_loss: 0.5823\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5599 - val_loss: 0.5817\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5594 - val_loss: 0.5811\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5589 - val_loss: 0.5805\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5583 - val_loss: 0.5799\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.5794\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5573 - val_loss: 0.5788\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5569 - val_loss: 0.5783\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5778\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5559 - val_loss: 0.5773\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5554 - val_loss: 0.5768\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5550 - val_loss: 0.5763\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5546 - val_loss: 0.5758\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 0.5753\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.5748\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.5744\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5529 - val_loss: 0.5740\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5525 - val_loss: 0.5736\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5521 - val_loss: 0.5731\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5517 - val_loss: 0.5727\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5723\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5510 - val_loss: 0.5719\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5506 - val_loss: 0.5715\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.5711\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5499 - val_loss: 0.5708\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5496 - val_loss: 0.5704\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5493 - val_loss: 0.5700\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 989us/step - loss: 0.5489 - val_loss: 0.5696\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5486 - val_loss: 0.5693\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5483 - val_loss: 0.5689\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5480 - val_loss: 0.5686\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5477 - val_loss: 0.5683\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5474 - val_loss: 0.5679\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5471 - val_loss: 0.5676\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 0.5673\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5466 - val_loss: 0.5670\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5463 - val_loss: 0.5667\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.5664\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5458 - val_loss: 0.5660\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5455 - val_loss: 0.5658\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5453 - val_loss: 0.5655\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5652\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5448 - val_loss: 0.5649\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5445 - val_loss: 0.5647\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5443 - val_loss: 0.5644\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5441 - val_loss: 0.5642\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5438 - val_loss: 0.5639\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5436 - val_loss: 0.5636\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5634\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 0.5631\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5430 - val_loss: 0.5629\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5428 - val_loss: 0.5626\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5426 - val_loss: 0.5624\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5424 - val_loss: 0.5622\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5422 - val_loss: 0.5619\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5420 - val_loss: 0.5617\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5418 - val_loss: 0.5614\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5416 - val_loss: 0.5613\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5611\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5412 - val_loss: 0.5609\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5606\n",
      "182/182 [==============================] - 0s 690us/step - loss: 0.5343\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 0s 2ms/step - loss: 2.1490 - val_loss: 1.3007\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.0048 - val_loss: 0.9285\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8160 - val_loss: 0.8363\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7623 - val_loss: 0.7901\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7313 - val_loss: 0.7579\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7070 - val_loss: 0.7307\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6865 - val_loss: 0.7084\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.6888\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6520 - val_loss: 0.6715\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6372 - val_loss: 0.6553\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6235 - val_loss: 0.6411\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.6282\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 0.6163\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5891 - val_loss: 0.6055\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.5958\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5704 - val_loss: 0.5867\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5620 - val_loss: 0.5789\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5542 - val_loss: 0.5714\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5472 - val_loss: 0.5644\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5580\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5526\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 1s 3ms/step - loss: 0.5292 - val_loss: 0.5472\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 0.5429\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.518 - 0s 2ms/step - loss: 0.5196 - val_loss: 0.5387\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.5349\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5114 - val_loss: 0.5311\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5078 - val_loss: 0.5277\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5043 - val_loss: 0.5250\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.5226\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4982 - val_loss: 0.5197\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4955 - val_loss: 0.5175\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4927 - val_loss: 0.5159\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4904 - val_loss: 0.5139\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.5121\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.5107\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4839 - val_loss: 0.5098\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.5080\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4803 - val_loss: 0.5066\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4785 - val_loss: 0.5056\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.5047\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4754 - val_loss: 0.5037\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.5025\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.5016\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4713 - val_loss: 0.5009\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.5010\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4687 - val_loss: 0.5003\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4674 - val_loss: 0.4992\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4662 - val_loss: 0.4985\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.4977\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4640 - val_loss: 0.4972\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4629 - val_loss: 0.4967\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4619 - val_loss: 0.4966\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4609 - val_loss: 0.4964\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4599 - val_loss: 0.4959\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4589 - val_loss: 0.4961\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4581 - val_loss: 0.4951\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4572 - val_loss: 0.4948\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4562 - val_loss: 0.4945\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4940\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4546 - val_loss: 0.4938\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.4940\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4530 - val_loss: 0.4937\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4522 - val_loss: 0.4933\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4513 - val_loss: 0.4931\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.4935\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4930\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4492 - val_loss: 0.4926\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4930\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 1s 3ms/step - loss: 0.4478 - val_loss: 0.4926\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4927\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4466 - val_loss: 0.4927\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4926\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4453 - val_loss: 0.4922\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4921\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4919\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4436 - val_loss: 0.4920\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4917\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4425 - val_loss: 0.4916\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4420 - val_loss: 0.4915\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4915\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4408 - val_loss: 0.4917\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4922\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4399 - val_loss: 0.4920\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4393 - val_loss: 0.4922\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4388 - val_loss: 0.4921\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4918\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4914\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4915\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4913\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4913\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4911\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4913\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4913\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4912\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4917\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4916\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4333 - val_loss: 0.4919\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4911\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4912\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4319 - val_loss: 0.4913\n",
      "182/182 [==============================] - 0s 950us/step - loss: 0.5855\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 3ms/step - loss: 3.0403 - val_loss: 1.5730\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 1.1889 - val_loss: 0.9391\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8241 - val_loss: 0.7891\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7387 - val_loss: 0.7383\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7052 - val_loss: 0.7093\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6830 - val_loss: 0.6870\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6646 - val_loss: 0.6690\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6484 - val_loss: 0.6533\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6337 - val_loss: 0.6391\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6206 - val_loss: 0.6263\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6148\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.6046\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5953\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5785 - val_loss: 0.5866\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 0.5787\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5627 - val_loss: 0.5718\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5556 - val_loss: 0.5648\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5588\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 0.5532\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.5477\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5325 - val_loss: 0.5429\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5393\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5235 - val_loss: 0.5345\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5307\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5155 - val_loss: 0.5269\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5118 - val_loss: 0.5233\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5085 - val_loss: 0.5203\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5052 - val_loss: 0.5174\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5025 - val_loss: 0.5146\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4995 - val_loss: 0.5119\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4969 - val_loss: 0.5092\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4945 - val_loss: 0.5071\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4921 - val_loss: 0.5049\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4897 - val_loss: 0.5026\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4878 - val_loss: 0.5008\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4856 - val_loss: 0.4989\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.4973\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4818 - val_loss: 0.4955\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4939\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4785 - val_loss: 0.4923\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.4907\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4753 - val_loss: 0.4891\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4741 - val_loss: 0.4884\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4869\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4716 - val_loss: 0.4859\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4848\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.4839\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4680 - val_loss: 0.4829\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4670 - val_loss: 0.4818\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4660 - val_loss: 0.4811\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4649 - val_loss: 0.4800\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4641 - val_loss: 0.4794\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4783\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4622 - val_loss: 0.4776\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4768\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4604 - val_loss: 0.4762\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4596 - val_loss: 0.4753\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4588 - val_loss: 0.4750\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4739\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4571 - val_loss: 0.4732\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4563 - val_loss: 0.4728\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4557 - val_loss: 0.4720\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.4711\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4707\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4535 - val_loss: 0.4708\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4529 - val_loss: 0.4690\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4523 - val_loss: 0.4693\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4516 - val_loss: 0.4689\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.4684\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4502 - val_loss: 0.4669\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4497 - val_loss: 0.4668\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4491 - val_loss: 0.4661\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4658\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.4654\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4643\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.4640\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4633\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4634\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4627\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4444 - val_loss: 0.4619\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.4618\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4612\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4606\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4603\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4594\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4591\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4588\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4580\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4581\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4576\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4567\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4565\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4559\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4552\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4551\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4549\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4544\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4537\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4348 - val_loss: 0.4529\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4527\n",
      "182/182 [==============================] - 0s 732us/step - loss: 0.4360\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 1.8043 - val_loss: 0.9056\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7192 - val_loss: 0.7509\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6482 - val_loss: 0.6842\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6046 - val_loss: 0.6359\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5729 - val_loss: 0.5976\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5476 - val_loss: 0.5700\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5273 - val_loss: 0.5474\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5115 - val_loss: 0.5291\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4990 - val_loss: 0.5167\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4887 - val_loss: 0.5044\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4815 - val_loss: 0.4975\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.4924\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4696 - val_loss: 0.4880\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4649 - val_loss: 0.4875\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4604 - val_loss: 0.4876\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4571 - val_loss: 0.4849\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4846\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4829\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4853\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4850\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4920\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4902\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4939\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4954\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4971\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.5010\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.5038\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.5075\n",
      "182/182 [==============================] - 0s 718us/step - loss: 0.6602\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.1055 - val_loss: 0.8114\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8512 - val_loss: 0.6910\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6370 - val_loss: 0.6365\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5952 - val_loss: 0.6020\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5657 - val_loss: 0.5774\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5437 - val_loss: 0.5554\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5410\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5151 - val_loss: 0.5270\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5055 - val_loss: 0.5187\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4968 - val_loss: 0.5163\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4895 - val_loss: 0.5027\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4854 - val_loss: 0.5016\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4793 - val_loss: 0.4947\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4765 - val_loss: 0.4916\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4728 - val_loss: 0.4869\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4704 - val_loss: 0.4850\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4652 - val_loss: 0.4821\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4654 - val_loss: 0.4800\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4776\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4581 - val_loss: 0.4733\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4755\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4547 - val_loss: 0.4684\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4515 - val_loss: 0.4675\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4505 - val_loss: 0.4649\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4633\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4456 - val_loss: 0.4608\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4605\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4425 - val_loss: 0.4576\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4404 - val_loss: 0.4584\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4551\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4537\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4507\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4506\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4487\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4492\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4477\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4450\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4454\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4274 - val_loss: 0.4420\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.4423\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4248 - val_loss: 0.4411\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4237 - val_loss: 0.4405\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4374\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4360\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4351\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4357\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4335\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4321\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4320\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4309\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.4278\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4282\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4285\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4111 - val_loss: 0.4272\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4268\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.4251\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.4248\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4221\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.4226\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4195\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4197\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4194\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4029 - val_loss: 0.4178\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4183\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4040 - val_loss: 0.4187\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.4156\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.4154\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4127\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.4135\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.4132\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.4110\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4134\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.4089\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3923 - val_loss: 0.4133\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.4364\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4114\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.4090\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.4067\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4069\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3896 - val_loss: 0.4471\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.4164\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.4014\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.4056\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4049\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3997\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.3997\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.3995\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.4017\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.3978\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.3985\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3956\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3784 - val_loss: 0.3948\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3990\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3947\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.3947\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.3928\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3953\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.3967\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.3923\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.3980\n",
      "182/182 [==============================] - 0s 696us/step - loss: 0.3833\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 0.7682 - val_loss: 0.5909\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5027 - val_loss: 0.4952\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4468 - val_loss: 0.4513\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4212 - val_loss: 0.4443\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4268\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4187\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3862 - val_loss: 0.4839\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4216\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.4157\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3976\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.5474\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.4390\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3945\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3477 - val_loss: 0.3778\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3415 - val_loss: 0.3701\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3377 - val_loss: 0.3799\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3362 - val_loss: 0.3761\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3309 - val_loss: 0.3655\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3299 - val_loss: 0.3554\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3246 - val_loss: 0.3520\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3200 - val_loss: 0.3525\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3218 - val_loss: 0.3511\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3720\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3116 - val_loss: 0.3544\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3715\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3108 - val_loss: 0.3423\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3101 - val_loss: 0.3521\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3118 - val_loss: 0.3392\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3059 - val_loss: 0.4042\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3029 - val_loss: 0.3646\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 0.4329\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3004 - val_loss: 0.3616\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2964 - val_loss: 0.3804\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2977 - val_loss: 0.3596\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2973 - val_loss: 0.3346\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3003 - val_loss: 0.3398\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2969 - val_loss: 0.3936\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2909 - val_loss: 0.3443\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2944 - val_loss: 0.4055\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2962 - val_loss: 0.3397\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2907 - val_loss: 0.3743\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2901 - val_loss: 0.3367\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2859 - val_loss: 0.3597\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2829 - val_loss: 0.3572\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2857 - val_loss: 0.4039\n",
      "182/182 [==============================] - 0s 919us/step - loss: 0.3977\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 0.7775 - val_loss: 1.2748\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5753 - val_loss: 0.5096\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4509\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.4406\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4278\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.4253\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3954 - val_loss: 0.4128\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.4695\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4772\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.4444\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.3875\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3824\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.4095\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3480 - val_loss: 0.4051\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3449 - val_loss: 0.3897\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3466 - val_loss: 0.3714\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3370 - val_loss: 0.3485\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3543\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.3934\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.4281\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3432\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3243 - val_loss: 0.3751\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3821\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3196 - val_loss: 0.3946\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3133 - val_loss: 0.3637\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3618\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3060 - val_loss: 0.3647\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3094 - val_loss: 0.3332\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3395\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3074 - val_loss: 0.3509\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3002 - val_loss: 0.3271\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2941 - val_loss: 0.3890\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2949 - val_loss: 0.3320\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.3735\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2964 - val_loss: 0.3420\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2923 - val_loss: 0.3466\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2856 - val_loss: 0.3301\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2874 - val_loss: 0.3240\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.290 - 0s 2ms/step - loss: 0.2853 - val_loss: 0.3521\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2836 - val_loss: 0.3475\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2887 - val_loss: 0.4038\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2857 - val_loss: 0.3334\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2807 - val_loss: 0.3366\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2801 - val_loss: 0.3231\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.2752 - val_loss: 0.3439\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2795 - val_loss: 0.3349\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2734 - val_loss: 0.3603\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2770 - val_loss: 0.3772\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2746 - val_loss: 0.3548\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2760 - val_loss: 0.3352\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2732 - val_loss: 0.3317\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2647 - val_loss: 0.3279\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2713 - val_loss: 0.3240\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.2656 - val_loss: 0.3363\n",
      "182/182 [==============================] - 0s 780us/step - loss: 0.3220\n",
      "Epoch 1/100\n",
      "  1/182 [..............................] - ETA: 27s - loss: 6.7328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 0s 2ms/step - loss: 1.2646 - val_loss: 0.6996\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5585 - val_loss: 0.7061\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5361 - val_loss: 0.7079\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5202 - val_loss: 0.7269\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5133 - val_loss: 0.7472\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5090 - val_loss: 0.7801\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5138 - val_loss: 0.8063\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.8244\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5087 - val_loss: 0.8518\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5028 - val_loss: 0.8728\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5053 - val_loss: 0.8979\n",
      "182/182 [==============================] - 0s 696us/step - loss: 1.7442\n",
      "Epoch 1/100\n",
      "  1/182 [..............................] - ETA: 27s - loss: 4.2443"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 0s 1ms/step - loss: 1.3572 - val_loss: 0.6801\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6753 - val_loss: 0.5995\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5696 - val_loss: 0.5749\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5639 - val_loss: 0.6210\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.2730 - val_loss: 0.5909\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.6047\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.0490 - val_loss: 0.6139\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6112 - val_loss: 0.7104\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.7981 - val_loss: 0.7630\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.5880 - val_loss: 0.6031\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.0772 - val_loss: 0.6556\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7912 - val_loss: 0.5478\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7156 - val_loss: 0.5581\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.7000\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.5042 - val_loss: 0.7717\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6782 - val_loss: 0.8425\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 6.9108 - val_loss: 0.7105\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6967 - val_loss: 0.7525\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 4.9998 - val_loss: 0.6841\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.1268 - val_loss: 0.5973\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.2351 - val_loss: 0.5577\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8049 - val_loss: 0.5466\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5788 - val_loss: 0.5835\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5620 - val_loss: 0.7582\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 5.2143 - val_loss: 0.7269\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.4754 - val_loss: 0.8189\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6532 - val_loss: 0.6885\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6669 - val_loss: 1.1955\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 14.4946 - val_loss: 0.9641\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 9.0665 - val_loss: 0.8608\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8508 - val_loss: 1.1135\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 8.4491 - val_loss: 0.9148\n",
      "182/182 [==============================] - 0s 713us/step - loss: 0.6423\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.1566 - val_loss: 1.1740\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8985 - val_loss: 0.9364\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7740 - val_loss: 0.8603\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7334 - val_loss: 0.8162\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7070 - val_loss: 0.7851\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6844 - val_loss: 0.7563\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6645 - val_loss: 0.7311\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6465 - val_loss: 0.7102\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6295 - val_loss: 0.6905\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6140 - val_loss: 0.6706\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6001 - val_loss: 0.6543\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.6375\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5747 - val_loss: 0.6227\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5638 - val_loss: 0.6094\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.5988\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.5859\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5349 - val_loss: 0.5750\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5648\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5189 - val_loss: 0.5566\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5121 - val_loss: 0.5480\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5058 - val_loss: 0.5393\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5003 - val_loss: 0.5324\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4952 - val_loss: 0.5264\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4904 - val_loss: 0.5207\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4861 - val_loss: 0.5151\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4817 - val_loss: 0.5102\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4784 - val_loss: 0.5062\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4750 - val_loss: 0.5019\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.4989\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4683 - val_loss: 0.4954\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4656 - val_loss: 0.4921\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4896\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4864\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4836\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4808\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4529 - val_loss: 0.4789\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4765\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4488 - val_loss: 0.4750\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4468 - val_loss: 0.4729\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4450 - val_loss: 0.4715\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.4697\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4413 - val_loss: 0.4683\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4395 - val_loss: 0.4672\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4375 - val_loss: 0.4664\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4656\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4348 - val_loss: 0.4640\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4638\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4314 - val_loss: 0.4620\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4304 - val_loss: 0.4612\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4291 - val_loss: 0.4606\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4277 - val_loss: 0.4601\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4263 - val_loss: 0.4596\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4250 - val_loss: 0.4590\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4240 - val_loss: 0.4590\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4590\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4572\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4573\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.4562\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4558\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.4553\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4566\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.4557\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4141 - val_loss: 0.4551\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.4548\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.4565\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.4551\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.4549\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4092 - val_loss: 0.4547\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.4539\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.4540\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4551\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4560\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4543\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.4551\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.4539\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.4537\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4536\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.4536\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3996 - val_loss: 0.4538\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.4553\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3984 - val_loss: 0.4540\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.4538\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3969 - val_loss: 0.4540\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.4549\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.4540\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3946 - val_loss: 0.4542\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3941 - val_loss: 0.4553\n",
      "182/182 [==============================] - 0s 945us/step - loss: 0.5238\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.7879 - val_loss: 1.1523\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.9926 - val_loss: 0.8207\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7801 - val_loss: 0.7547\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7121 - val_loss: 0.7111\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6753 - val_loss: 0.6863\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6519 - val_loss: 0.6630\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6307 - val_loss: 0.6453\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.6249\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.6111\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5933\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5670 - val_loss: 0.5800\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.5682\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5422 - val_loss: 0.5580\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5323 - val_loss: 0.5477\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5379\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5153 - val_loss: 0.5297\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5077 - val_loss: 0.5235\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5014 - val_loss: 0.5154\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4954 - val_loss: 0.5097\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4897 - val_loss: 0.5051\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4853 - val_loss: 0.5000\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4806 - val_loss: 0.4955\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 1s 3ms/step - loss: 0.4768 - val_loss: 0.4912\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.4878\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4695 - val_loss: 0.4841\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4806\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4646 - val_loss: 0.4793\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4624 - val_loss: 0.4755\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4595 - val_loss: 0.4730\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4564 - val_loss: 0.4708\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.4696\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4521 - val_loss: 0.4674\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.4654\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4482 - val_loss: 0.4630\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4466 - val_loss: 0.4626\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4451 - val_loss: 0.4610\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4434 - val_loss: 0.4599\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4428 - val_loss: 0.4573\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.4560\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4547\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4541\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4367 - val_loss: 0.4521\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4522\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4510\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4317 - val_loss: 0.4484\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4478\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4460\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4450\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.4442\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.4441\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4255 - val_loss: 0.4416\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.4420\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4221 - val_loss: 0.4419\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4398\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4384\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4187 - val_loss: 0.4386\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4379\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4170 - val_loss: 0.4364\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4356\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4360\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4349\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4189 - val_loss: 0.4342\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4172 - val_loss: 0.4337\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.4349\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4205 - val_loss: 0.4303\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4305\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.4301\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.4324\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4166 - val_loss: 0.4289\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.4275\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.4262\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4260\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.4254\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.4237\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.4233\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4023 - val_loss: 0.4225\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4023 - val_loss: 0.4217\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.4224\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.4202\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.4206\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4200\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4185\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.4178\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.4171\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.4180\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3956 - val_loss: 0.4157\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.4152\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.4150\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4147\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.4138\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.4135\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.4140\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3928 - val_loss: 0.4127\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.4121\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4114\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3920 - val_loss: 0.4096\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4102\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4087\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3866 - val_loss: 0.4087\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4075\n",
      "182/182 [==============================] - 0s 818us/step - loss: 0.3988\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 4.8802 - val_loss: 3.9146\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 2.6052 - val_loss: 2.3867\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.6131 - val_loss: 1.6987\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.1594 - val_loss: 1.3758\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.9423 - val_loss: 1.2202\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8342 - val_loss: 1.1400\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7765 - val_loss: 1.0968\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7432 - val_loss: 1.0716\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7215 - val_loss: 1.0551\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7058 - val_loss: 1.0445\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6935 - val_loss: 1.0346\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6823 - val_loss: 1.0271\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6727 - val_loss: 1.0207\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6636 - val_loss: 1.0151\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6553 - val_loss: 1.0099\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6475 - val_loss: 1.0054\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6401 - val_loss: 1.0014\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6329 - val_loss: 0.9976\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6263 - val_loss: 0.9943\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6199 - val_loss: 0.9914\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.9886\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.9861\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.9840\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.9820\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.9803\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5879 - val_loss: 0.9789\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5834 - val_loss: 0.9776\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.583 - 0s 1ms/step - loss: 0.5793 - val_loss: 0.9765\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5752 - val_loss: 0.9758\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5714 - val_loss: 0.9750\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5678 - val_loss: 0.9745\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5643 - val_loss: 0.9740\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5611 - val_loss: 0.9736\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5580 - val_loss: 0.9737\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5550 - val_loss: 0.9737\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5523 - val_loss: 0.9740\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5495 - val_loss: 0.9743\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5470 - val_loss: 0.9747\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5446 - val_loss: 0.9752\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5423 - val_loss: 0.9757\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.9763\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5381 - val_loss: 0.9771\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5361 - val_loss: 0.9781\n",
      "182/182 [==============================] - 0s 613us/step - loss: 1.9697\n",
      "Epoch 1/100\n",
      "  1/182 [..............................] - ETA: 26s - loss: 9.4419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 0s 2ms/step - loss: 6.1993 - val_loss: 4.1550\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 2.9593 - val_loss: 2.1619\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.6480 - val_loss: 1.3056\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.0706 - val_loss: 0.9257\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8102 - val_loss: 0.7514\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6891 - val_loss: 0.6702\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6314 - val_loss: 0.6306\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6031 - val_loss: 0.6109\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.6002\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5806 - val_loss: 0.5945\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5757 - val_loss: 0.5905\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5725 - val_loss: 0.5878\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5700 - val_loss: 0.5855\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5678 - val_loss: 0.5831\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5658 - val_loss: 0.5808\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5642 - val_loss: 0.5790\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5627 - val_loss: 0.5777\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5611 - val_loss: 0.5770\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5596 - val_loss: 0.5749\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5584 - val_loss: 0.5734\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5572 - val_loss: 0.5727\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5558 - val_loss: 0.5710\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5548 - val_loss: 0.5705\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 0.5693\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5526 - val_loss: 0.5677\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5517 - val_loss: 0.5673\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5508 - val_loss: 0.5666\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.5649\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5491 - val_loss: 0.5643\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5483 - val_loss: 0.5634\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5624\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 0.5617\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5461 - val_loss: 0.5608\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5455 - val_loss: 0.5599\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5451 - val_loss: 0.5597\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5443 - val_loss: 0.5592\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 0.5595\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5432 - val_loss: 0.5584\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5428 - val_loss: 0.5579\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5421 - val_loss: 0.5570\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5420 - val_loss: 0.5574\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5567\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5410 - val_loss: 0.5562\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5405 - val_loss: 0.5553\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5403 - val_loss: 0.5553\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5398 - val_loss: 0.5545\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5396 - val_loss: 0.5554\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5392 - val_loss: 0.5551\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5540\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5385 - val_loss: 0.5533\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5384 - val_loss: 0.5532\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5380 - val_loss: 0.5527\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5525\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.5525\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5374 - val_loss: 0.5523\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.5515\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5371 - val_loss: 0.5517\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.5527\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5530\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5362 - val_loss: 0.5517\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5362 - val_loss: 0.5521\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5359 - val_loss: 0.5513\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5359 - val_loss: 0.5515\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5358 - val_loss: 0.5513\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5505\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5504\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5498\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5496\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5497\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5351 - val_loss: 0.5503\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5347 - val_loss: 0.5495\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5349 - val_loss: 0.5503\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5346 - val_loss: 0.5508\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5344 - val_loss: 0.5498\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5345 - val_loss: 0.5496\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5493\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5506\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5342 - val_loss: 0.5579\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5519\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5501\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5504\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5502\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5497\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5498\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5496\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5337 - val_loss: 0.5494\n",
      "182/182 [==============================] - 0s 767us/step - loss: 0.5356\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.2721 - val_loss: 1.2481\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.9034 - val_loss: 0.8625\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.7214 - val_loss: 0.7968\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6791 - val_loss: 0.7579\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.7222\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6260 - val_loss: 0.6969\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.6748\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.6548\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.6338\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5600 - val_loss: 0.6189\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5484 - val_loss: 0.6041\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 0.5898\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5288 - val_loss: 0.5784\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.5664\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5128 - val_loss: 0.5594\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5060 - val_loss: 0.5473\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4996 - val_loss: 0.5396\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4941 - val_loss: 0.5318\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4884 - val_loss: 0.5260\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4841 - val_loss: 0.5175\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4792 - val_loss: 0.5126\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4756 - val_loss: 0.5065\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4717 - val_loss: 0.5028\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4681 - val_loss: 0.4977\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4939\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4618 - val_loss: 0.4896\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4587 - val_loss: 0.4863\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4836\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4534 - val_loss: 0.4807\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4777\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4479 - val_loss: 0.4763\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4733\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4436 - val_loss: 0.4710\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4691\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4390 - val_loss: 0.4662\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4372 - val_loss: 0.4641\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4622\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4330 - val_loss: 0.4602\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4586\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4575\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4556\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4532\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.4517\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4222 - val_loss: 0.4518\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4211 - val_loss: 0.4490\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4188 - val_loss: 0.4490\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4174 - val_loss: 0.4460\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4157 - val_loss: 0.4453\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.4447\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4436\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4115 - val_loss: 0.4436\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4098 - val_loss: 0.4400\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4087 - val_loss: 0.4390\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4070 - val_loss: 0.4389\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.4377\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4042 - val_loss: 0.4370\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.4350\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.4344\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4333\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3995 - val_loss: 0.4333\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.4336\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4310\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3957 - val_loss: 0.4287\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.4290\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3934 - val_loss: 0.4309\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.4292\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.4282\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3907 - val_loss: 0.4272\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4256\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3879 - val_loss: 0.4254\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4254\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3857 - val_loss: 0.4246\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4237\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3834 - val_loss: 0.4244\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3825 - val_loss: 0.4245\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3819 - val_loss: 0.4215\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4218\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3798 - val_loss: 0.4217\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4204\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3778 - val_loss: 0.4203\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.4191\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4189\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.4172\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.4185\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3733 - val_loss: 0.4174\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4170\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.4154\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3705 - val_loss: 0.4165\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.4145\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.4155\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3679 - val_loss: 0.4146\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.4144\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3664 - val_loss: 0.4134\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4153\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.4119\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.4135\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.4130\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.4119\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3617 - val_loss: 0.4122\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.4124\n",
      "182/182 [==============================] - 0s 845us/step - loss: 0.4448\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.5655 - val_loss: 1.1261\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.9243 - val_loss: 0.8288\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7365 - val_loss: 0.7372\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6730 - val_loss: 0.6829\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6333 - val_loss: 0.6454\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6051 - val_loss: 0.6176\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.5955\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5646 - val_loss: 0.5775\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5489 - val_loss: 0.5605\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5470\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 0.5360\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.5240\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5029 - val_loss: 0.5146\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4946 - val_loss: 0.5058\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.4982\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4800 - val_loss: 0.4911\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4739 - val_loss: 0.4849\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.4791\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4625 - val_loss: 0.4737\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4695\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4535 - val_loss: 0.4645\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4489 - val_loss: 0.4619\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4569\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4422 - val_loss: 0.4545\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4516\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4360 - val_loss: 0.4482\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4467\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.4435\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4281 - val_loss: 0.4411\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4381\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4238 - val_loss: 0.4363\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4343\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4325\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4323\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4292\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.4281\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.4259\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4245\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4234\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.4226\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.4206\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.4206\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4190\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.4164\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4162\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.4156\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.4147\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4134\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.4116\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4103\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.4113\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.4086\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3900 - val_loss: 0.4087\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3889 - val_loss: 0.4070\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3875 - val_loss: 0.4063\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3865 - val_loss: 0.4059\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3854 - val_loss: 0.4039\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.4056\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3834 - val_loss: 0.4030\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 3ms/step - loss: 0.3822 - val_loss: 0.4012\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.4009\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3987\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.382 - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3979\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3986\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.3971\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3756 - val_loss: 0.3967\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3960\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3947\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3725 - val_loss: 0.3943\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3722 - val_loss: 0.3936\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3917\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3919\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.3932\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3900\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3895\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3662 - val_loss: 0.3897\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3883\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.3874\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3883\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3865\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3620 - val_loss: 0.3860\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3612 - val_loss: 0.3850\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3841\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3833\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3586 - val_loss: 0.3820\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3810\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3574 - val_loss: 0.3817\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3567 - val_loss: 0.3816\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3555 - val_loss: 0.3823\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3553 - val_loss: 0.3797\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3789\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3782\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.3776\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3780\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3511 - val_loss: 0.3774\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.3760\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3502 - val_loss: 0.3758\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 0.3757\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3749\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.3481 - val_loss: 0.3746\n",
      "182/182 [==============================] - 0s 740us/step - loss: 0.3693\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 1.6899 - val_loss: 0.8669\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7471 - val_loss: 0.7679\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6937 - val_loss: 0.7254\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6645 - val_loss: 0.6969\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6422 - val_loss: 0.6737\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6233 - val_loss: 0.6544\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6072 - val_loss: 0.6372\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.6205\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.6048\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 0.5915\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5780\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5393 - val_loss: 0.5671\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5544\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5198 - val_loss: 0.5460\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5126 - val_loss: 0.5384\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.5305\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.5243\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4957 - val_loss: 0.5190\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4907 - val_loss: 0.5154\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4872 - val_loss: 0.5111\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4833 - val_loss: 0.5083\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4798 - val_loss: 0.5044\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4769 - val_loss: 0.5022\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4742 - val_loss: 0.4995\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4718 - val_loss: 0.4962\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.4934\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4669 - val_loss: 0.4912\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4649 - val_loss: 0.4905\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.4875\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4865\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4602 - val_loss: 0.4853\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4582 - val_loss: 0.4833\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4569 - val_loss: 0.4825\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4557 - val_loss: 0.4807\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4541 - val_loss: 0.4792\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4531 - val_loss: 0.4782\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4774\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4507 - val_loss: 0.4778\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4746\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4733\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4719\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4462 - val_loss: 0.4718\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4713\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4697\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4690\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4670\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4663\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4670\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4642\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4657\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4633\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4624\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4362 - val_loss: 0.4603\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4607\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4340 - val_loss: 0.4609\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4578\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4638\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4567\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4304 - val_loss: 0.4560\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4556\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4555\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4532\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4524\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4522\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.4511\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4497\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.4499\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.4491\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4465\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.4464\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.4467\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4445\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.4433\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4446\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4415\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.415 - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4402\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4400\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.4401\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4390\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4394\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4376\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.4369\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4354\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.4349\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.4358\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.4346\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4339\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4324\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4304\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4319\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4305\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4290\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4298\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.4281\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.4292\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.4277\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4262\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4241\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.4250\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.4254\n",
      "182/182 [==============================] - 0s 735us/step - loss: 0.4177\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 1s 2ms/step - loss: 2.3877 - val_loss: 1.1895\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 1.0366 - val_loss: 0.9540\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.8745 - val_loss: 0.8634\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7961 - val_loss: 0.7965\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7420 - val_loss: 0.7472\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.7042 - val_loss: 0.7111\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6760 - val_loss: 0.6832\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6554 - val_loss: 0.6645\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.6383 - val_loss: 0.6478\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6247 - val_loss: 0.6334\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6203\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.6013 - val_loss: 0.6105\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.6011\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5830 - val_loss: 0.5916\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5744 - val_loss: 0.5839\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5664 - val_loss: 0.5763\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5590 - val_loss: 0.5690\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5519 - val_loss: 0.5623\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.5569\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5390 - val_loss: 0.5498\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.5441\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5393\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.5340\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.5320\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5123 - val_loss: 0.5246\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5078 - val_loss: 0.5206\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5036 - val_loss: 0.5167\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.5000 - val_loss: 0.5137\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4957 - val_loss: 0.5111\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4926 - val_loss: 0.5060\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4892 - val_loss: 0.5035\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4861 - val_loss: 0.4998\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4827 - val_loss: 0.4981\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4947\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4920\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4747 - val_loss: 0.4897\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4726 - val_loss: 0.4889\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4702 - val_loss: 0.4852\n",
      "Epoch 39/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4679 - val_loss: 0.4833\n",
      "Epoch 40/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4661 - val_loss: 0.4811\n",
      "Epoch 41/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.4790\n",
      "Epoch 42/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4621 - val_loss: 0.4773\n",
      "Epoch 43/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4601 - val_loss: 0.4763\n",
      "Epoch 44/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4583 - val_loss: 0.4738\n",
      "Epoch 45/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4565 - val_loss: 0.4722\n",
      "Epoch 46/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4552 - val_loss: 0.4712\n",
      "Epoch 47/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4534 - val_loss: 0.4709\n",
      "Epoch 48/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4520 - val_loss: 0.4673\n",
      "Epoch 49/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4668\n",
      "Epoch 50/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4652\n",
      "Epoch 51/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4474 - val_loss: 0.4636\n",
      "Epoch 52/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4618\n",
      "Epoch 53/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4613\n",
      "Epoch 54/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4605\n",
      "Epoch 55/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4427 - val_loss: 0.4585\n",
      "Epoch 56/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4419 - val_loss: 0.4580\n",
      "Epoch 57/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4561\n",
      "Epoch 58/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4396 - val_loss: 0.4564\n",
      "Epoch 59/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4383 - val_loss: 0.4543\n",
      "Epoch 60/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4532\n",
      "Epoch 61/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4540\n",
      "Epoch 62/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4361 - val_loss: 0.4532\n",
      "Epoch 63/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4500\n",
      "Epoch 64/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4497\n",
      "Epoch 65/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4489\n",
      "Epoch 66/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4482\n",
      "Epoch 67/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4474\n",
      "Epoch 68/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4466\n",
      "Epoch 69/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4458\n",
      "Epoch 70/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4443\n",
      "Epoch 71/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4275 - val_loss: 0.4442\n",
      "Epoch 72/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.4428\n",
      "Epoch 73/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4261 - val_loss: 0.4425\n",
      "Epoch 74/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4248 - val_loss: 0.4403\n",
      "Epoch 75/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.4405\n",
      "Epoch 76/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.4386\n",
      "Epoch 77/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.4384\n",
      "Epoch 78/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4215 - val_loss: 0.4379\n",
      "Epoch 79/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.4371\n",
      "Epoch 80/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4377\n",
      "Epoch 81/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4362\n",
      "Epoch 82/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4185 - val_loss: 0.4352\n",
      "Epoch 83/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4339\n",
      "Epoch 84/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4164 - val_loss: 0.4345\n",
      "Epoch 85/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4345\n",
      "Epoch 86/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.4321\n",
      "Epoch 87/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4146 - val_loss: 0.4314\n",
      "Epoch 88/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.4300\n",
      "Epoch 89/100\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4131 - val_loss: 0.4307\n",
      "Epoch 90/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.4291\n",
      "Epoch 91/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4116 - val_loss: 0.4298\n",
      "Epoch 92/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.4280\n",
      "Epoch 93/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.4281\n",
      "Epoch 94/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4099 - val_loss: 0.4264\n",
      "Epoch 95/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.4264\n",
      "Epoch 96/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.4275\n",
      "Epoch 97/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4246\n",
      "Epoch 98/100\n",
      "182/182 [==============================] - ETA: 0s - loss: 0.406 - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4237\n",
      "Epoch 99/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4243\n",
      "Epoch 100/100\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.4235\n",
      "182/182 [==============================] - 0s 740us/step - loss: 0.4216\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8968 - val_loss: 0.5519\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5367 - val_loss: 0.4493\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4443\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4161\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.3928\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.3775\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.3634\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3536 - val_loss: 0.3531\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3537\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3407\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.3773\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.3318\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3347\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3309\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3151 - val_loss: 0.3397\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3195\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3065 - val_loss: 0.3261\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3075 - val_loss: 0.3414\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3054 - val_loss: 0.3088\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2988 - val_loss: 0.3394\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2985 - val_loss: 0.3156\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2969 - val_loss: 0.3150\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2933 - val_loss: 0.3259\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2915 - val_loss: 0.3049\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2899 - val_loss: 0.3341\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2864 - val_loss: 0.3162\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2857 - val_loss: 0.3133\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2856 - val_loss: 0.3011\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2842 - val_loss: 0.2999- ETA: 0s - loss: 0.285\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2842 - val_loss: 0.2957\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2802 - val_loss: 0.3154\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2782 - val_loss: 0.3137\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2815 - val_loss: 0.2985\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2779 - val_loss: 0.3064\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2776 - val_loss: 0.3098\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2783 - val_loss: 0.3052\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2764 - val_loss: 0.3011\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2739 - val_loss: 0.2926\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2722 - val_loss: 0.2981\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2711 - val_loss: 0.2943\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.2731 - val_loss: 0.3267\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2713 - val_loss: 0.3174\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.2701 - val_loss: 0.2971\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2680 - val_loss: 0.3078\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2675 - val_loss: 0.3046\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2686 - val_loss: 0.2863\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2647 - val_loss: 0.3031\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 975us/step - loss: 0.2647 - val_loss: 0.2891\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2630 - val_loss: 0.3126\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.3244\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2615 - val_loss: 0.3016\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2632 - val_loss: 0.2910\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2613 - val_loss: 0.3023\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2585 - val_loss: 0.2869\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2573 - val_loss: 0.3057\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2587 - val_loss: 0.2870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x0000014B2D79BEE0>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000014B2FBEA5B0>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2) # 범위 내의 연속적인 역수 반환\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RandomizedSearchCV는 k-fold cross validation을 사용하기 때문에 validation set이 필요없음. 여기서 사용하는 validation set은 조기 종료에 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.015394102216670189, 'n_hidden': 3, 'n_neurons': 74}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35982608795166016"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 저장된 최적의 모델은 테스트 세트로 평가한 뒤 상용 환경에 배포 가능.\n",
    "* 모델의 크기가 크면 하이퍼파라미터 탐색에 많은 시간이 소요되므로, 여러 효율적인 탐색 기법이 존재. 주로 탐색 지역이 좋다고 판명될 때 더 탐색을 수행하는 방식을 사용.\n",
    "  * 수동으로 탐색 영역을 좁히는 것 보다 훨씬 적은 시간에 더 나은 솔루션을 제공함.\n",
    "  * Hyperopt : 모든 종류의 복잡한 탐색 공간에 대해 최적화를 수행할 수 있는 라이브러리. https://github.com/hyperopt/hyperopt\n",
    "  * Hyperas, kopt, Talos : keras 모델을 위한 하이퍼파라미터 최적화 라이브러리. https://github.com/maxpumperla/hyperas, https://github.com/Avsecz/kopt, https://github.com/autonomio/talos\n",
    "  * Keras tuner : 구글에서 만든 케라스 하이퍼파라미터 최적화 라이브러리. 시각화와 분석을 포함한 클라우드 서비스 제공 예정.\n",
    "  * Scikit-Optimize(skopt) : 범용 최적화 라이브러리. <code>GridSearchCV</code>와 비슷한 인터페이스를 사용하는 <code>BayesSearchCV</code> (베이즈 최적화) 사용 가능.\n",
    "  * Spearmint : 베이즈 최적화 라이브러리.https://github.com/JasperSnoek/spearmint\n",
    "  * Hyperband : 빠른 하이퍼파라미터 튜닝 라이브러리. https://github.com/zygmuntz/hyperband\n",
    "  * Sklearn-Deap : GridSearchCV와 비슷한 인터페이스를 가진 진화 알고리즘 기반 하이퍼파라미터 최적화 라이브러리. https://github.com/rsteca/sklearn-deap\n",
    "* 구글의 AutoML(http://cloud.google.com/automl)과 같은 서비스는 하이퍼파라미터 탐색과 함께 문제에 최적인 신경망 구조를 찾기 위한 진화 전략을 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 개수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하나의 은닉층만으로도 많은 문제에서 괜찮은 결과를 얻을 수 있음.\n",
    "  * 이론상 은닉층이 하나인 다층 퍼셈트론도 뉴런 수가 충분하면 복잡한 함수도 모델링할 수 있음.\n",
    "* 단, 복잡한 문제에서는 심층 신경망이 파라미터 효율성(parameter efficiency)이 좋음. 즉, 심층 신경망이 복잡한 함수를 모델링하는 데 훨씬 적은 수의 뉴런을 사용하므로 동일한 양의 훈련 데이터에서 더 높은 성능을 낼 수 있음.\n",
    "* 아래쪽 은닉층은 저수준의 구조를 모델링하고, 중간 은닉층은 저수준의 구조를 연결해 중간 수준의 구조를 모델링하고, 위쪽 은닉층과 출력층은 중간 수준의 구조를 연결해 고수준의 구조를 모델링함.\n",
    "  * ex) 여러 방향이나 모양의 선 -> 사각형 또는 원 -> 얼굴\n",
    "* 계층 구조는 심층 신경망이 좋은 솔루션으로 빨리 수렴하게끔 도와주고, 새로운 데이터에 일반화하는 능력도 향상시킴.\n",
    "  * ex) 얼굴 인식 모델 훈련 후, 헤어스타일 인식 모델을 훈련하려면 얼굴 인식 모델의 하위 층을 재사용 할 수 있음.\n",
    "  * 이러한 경우 새로운 신경망(위의 경우 헤어스타일 인식 모델)에서 처음 몇개의 층의 가중치와 편향을 난수로 초기화하는 대신에 첫 번째 신경망(위의 경우 얼굴 인식 모델)에 있는 층의 가중치와 편향값으로 초기화할 수 있음. 이렇게 하면 저수준 구조를 다시 학습할 필요가 없게 되고, 이를 전이 학습(transfer learning)이라고 함.\n",
    "* MNIST데이터셋의 경우 몇백개의 뉴런을 가진 은닉층 1~2개로 97% 정도의 정확도를 얻을 수 있지만 대규모 이미지 분류나 음성 인식 등은 수십 개 층으로 이루어진 네트워크가 필요하고 훈련 데이터가 아주 많이 필요함. 하지만, 이런 경우 네트워크를 처음부터 훈련하기보다는 비슷한 작업에서 가장 뛰어난 성능을 낸 미리 훈련된 네트워크를 일부 재사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 은닉층 뉴런 개수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 입력층과 출력 층의 뉴런 개수는 작업에 필요한 입력과 출력에 따라 결정됨\n",
    "  * ex) MNIST 데이터셋의 경우 28*28=784개의 입력 뉴런과 10개의 출력 뉴런 필요\n",
    "* 은닉층 구성은 일반적으로 각 층의 뉴런을 점점 줄여서 깔때기 모양으로 구성.\n",
    "  * 저수준의 많은 특성이 고수준의 적은 특성으로 합쳐질 수 있기 때문.\n",
    "* 요즘은 대부분의 경우 모든 은닉층에 같은 크기를 사용해도 더 나은 성능이 나오기도 함.\n",
    "  * 데이터셋에 따라 다르지만 다른 은닉층보다 첫 번째 은닉층을 크게 하는 것이 도움됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 네트워크가 과대적합될때까지 뉴런 수를 점진적으로 늘릴 수 있음.\n",
    "* 실전에서는 필요한 층과 뉴런 수보다 더 많은 것을 가진 모델을 선택하고 과대적합되지 않도록 조기 종료가 규제를 사용하는 것이 효과적.\n",
    "  * 모델에서 문제를 일으키는 병목 층을 피할 수 있음.\n",
    "  * 한 층에 뉴런 수가 너무 적으면 입력에 있는 유용한 정보를 모두 유지하기 위한 표현 능력을 가지지 못할 수 있음.\n",
    "> * 일반적으로 층의 뉴런 수보다 층수를 늘리는 쪽이 이득인 경우가 많음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습률, 배치 크기, 다른 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습률 : 가장 중요한 하이퍼파라미터 중 하나. 최적의 학습률은 일반적으로 최대 학습률(훈련 알고리즘이 발산하는 학습률)의 절반 정도.\n",
    "  * 매우 낮은 학습률($10^{-5}$)에서 시작해 점점 매우 큰 학습률로 증가시키며 수백 번 반복하는 것이 하나의 방법. 반복마다 일정한 값을 학습률에 곱함.\n",
    "  * 학습률이 증가함에 따라 손실이 줄어들지만, 어느 순간 손실이 다시 늘어나게 되고, 손실이 다시 상승하는 지점보다 조금 아래에 있는 것이 최적의 학습률(일반적으로 상승점보다 10배정도 낮은 지점). 최적의 학습률을 찾으면 모델을 초기화하고 다시 훈련할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimizer : 미니배치 경사 하강법보다 더 좋은 optimizer를 사용하는 것이 도움이 될 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 배치 크기 : 모델 성능과 훈련 시간에 큰 영향을 줌. 배치 크기가 크면 GPU와 같은 하드웨어 가속기를 효율적으로 활용할 수 있음. 즉, 초당 더 많은 샘플을 처리할 수 있음.\n",
    "  * 보통 GPU 램에 맞는 가장 큰 배치 크기 사용이 권장됨.\n",
    "  * 단, 실전에서 큰 배치를 사용하면 특히 훈련 초기에 불안정하게 훈련될 수 있음. 결과 모델이 작은 배치로 훈련된 모델만큼 일반화 성능을 내지 못할 수 있음.\n",
    "* 큰 배치 크기는 일반화 성능에 영향을 끼치지 않고 훈련 시간을 단축함.\n",
    "  * 학습률 예열(warming up, 작은 학습률로 시작해 점점 키우는 것)을 사용해 큰 배치 크기를 시도해보는 것이 좋을 수 있음. 훈련이 불안정하거나 최종 성능이 좋지 못하면 작은 배치 크기 사용.\n",
    "  * <code>compile()</code>의 <code>steps_per_execution</code> 매개변수를 1이상으로 설정하면 한 계산 그래프를 실행하는 데 여러 배치를 처리할 수 있으므로 GPU를 최대로 활용하면서 배치 크기를 바꾸지 않고 훈련 속도를 높일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 활성화 함수 : 일반적으로 ReLU가 모든 은닉층에 좋은 기본값. 출력층의 활성화 함수는 작업에 따라 다름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 반복 횟수 : 일반적으로는 튜닝 필요 없음(조기 종료 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://arxiv.org/abs/1803.09820 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate=3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg.fit(X_train.reshape(60000, 28*28), y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = keras_reg.predict(X_test.reshape(10000,28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9425"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 1.5224\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4837\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3534\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2874\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2471\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2173\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1966\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1800\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1665\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1568\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1469\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1388\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1314\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1252\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1192\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1153\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1093\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1047\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1008\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0971\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0935\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0904\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0874\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0843\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0807\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0788\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0764\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0732\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0709\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001A6480A42E0>,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001A6480A4820>})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"learning_rate\": reciprocal(1e-5, 1e-2)\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, n_jobs=-1)\n",
    "search_cv.fit(X_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = search_cv.best_estimator_.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_pred, axis=1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0011742217529897762}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### callback 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/rickiepark/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
    "K = keras.backend\n",
    "class ExponentialLrCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.lr = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.lr.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "expon_lr = ExponentialLrCallback(1.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 6s 3ms/step - loss: nan - accuracy: 0.0987\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train/255, y_train, epochs=1,\n",
    "                    callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(expon_lr.lr, expon_lr.losses, 'r-')\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.lr), max(expon_lr.lr))\n",
    "plt.axis([min(expon_lr.lr), max(expon_lr.lr), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model_new.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=9e-1),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987A: 1s -\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987A: 0s - loss: nan - accuracy: 0.098\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987A: 0s - loss: nan - accuracy: 0.098\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987A: 0s - loss: nan - accu\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: nan - accuracy: 0.0987\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: nan - accuracy: 0.0987\n"
     ]
    }
   ],
   "source": [
    "history = model_new.fit(X_train/255, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: nan - accuracy: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.09799999743700027]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4211a37325b67dc4346d743008529599135417ca8cd1eae7178b10ad33111a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
